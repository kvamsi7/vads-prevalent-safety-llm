{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "692a4ec9-b403-4710-9d44-c55d07735e25",
   "metadata": {},
   "source": [
    "### install necessary dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22fcce7e-8b7b-4ca8-9c84-8db7e37ee7a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sae-lens in /usr/local/lib/python3.10/dist-packages (4.4.5)\n",
      "Requirement already satisfied: transformer-lens in /usr/local/lib/python3.10/dist-packages (2.11.0)\n",
      "Requirement already satisfied: sae-dashboard in /usr/local/lib/python3.10/dist-packages (0.6.4)\n",
      "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (0.9.0)\n",
      "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.59.8)\n",
      "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.10/dist-packages (8.1.1)\n",
      "Requirement already satisfied: huggingface_hub[cli] in /usr/local/lib/python3.10/dist-packages (0.27.1)\n",
      "Requirement already satisfied: automated-interpretability<1.0.0,>=0.0.5 in /usr/local/lib/python3.10/dist-packages (from sae-lens) (0.0.6)\n",
      "Requirement already satisfied: babe<0.0.8,>=0.0.7 in /usr/local/lib/python3.10/dist-packages (from sae-lens) (0.0.7)\n",
      "Requirement already satisfied: datasets<3.0.0,>=2.17.1 in /usr/local/lib/python3.10/dist-packages (from sae-lens) (2.21.0)\n",
      "Requirement already satisfied: matplotlib<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from sae-lens) (3.10.0)\n",
      "Requirement already satisfied: matplotlib-inline<0.2.0,>=0.1.6 in /usr/local/lib/python3.10/dist-packages (from sae-lens) (0.1.6)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from sae-lens) (3.9.1)\n",
      "Requirement already satisfied: plotly<6.0.0,>=5.19.0 in /usr/local/lib/python3.10/dist-packages (from sae-lens) (5.24.1)\n",
      "Requirement already satisfied: plotly-express<0.5.0,>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from sae-lens) (0.4.1)\n",
      "Requirement already satisfied: pytest-profiling<2.0.0,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from sae-lens) (1.8.1)\n",
      "Requirement already satisfied: python-dotenv<2.0.0,>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from sae-lens) (1.0.1)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from sae-lens) (6.0.1)\n",
      "Requirement already satisfied: pyzmq==26.0.0 in /usr/local/lib/python3.10/dist-packages (from sae-lens) (26.0.0)\n",
      "Requirement already satisfied: safetensors<0.5.0,>=0.4.2 in /usr/local/lib/python3.10/dist-packages (from sae-lens) (0.4.5)\n",
      "Requirement already satisfied: simple-parsing<0.2.0,>=0.1.6 in /usr/local/lib/python3.10/dist-packages (from sae-lens) (0.1.6)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.38.1 in /usr/local/lib/python3.10/dist-packages (from sae-lens) (4.48.0)\n",
      "Requirement already satisfied: typer<0.13.0,>=0.12.3 in /usr/local/lib/python3.10/dist-packages (from sae-lens) (0.12.5)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.10.0 in /usr/local/lib/python3.10/dist-packages (from sae-lens) (4.12.2)\n",
      "Requirement already satisfied: zstandard<0.23.0,>=0.22.0 in /usr/local/lib/python3.10/dist-packages (from sae-lens) (0.22.0)\n",
      "Requirement already satisfied: accelerate>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformer-lens) (1.3.0)\n",
      "Requirement already satisfied: beartype<0.15.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformer-lens) (0.14.1)\n",
      "Requirement already satisfied: better-abc<0.0.4,>=0.0.3 in /usr/local/lib/python3.10/dist-packages (from transformer-lens) (0.0.3)\n",
      "Requirement already satisfied: einops>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from transformer-lens) (0.7.0)\n",
      "Requirement already satisfied: fancy-einsum>=0.0.3 in /usr/local/lib/python3.10/dist-packages (from transformer-lens) (0.0.3)\n",
      "Requirement already satisfied: jaxtyping>=0.2.11 in /usr/local/lib/python3.10/dist-packages (from transformer-lens) (0.2.36)\n",
      "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.10/dist-packages (from transformer-lens) (1.26.3)\n",
      "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from transformer-lens) (2.2.3)\n",
      "Requirement already satisfied: rich>=12.6.0 in /usr/local/lib/python3.10/dist-packages (from transformer-lens) (13.9.4)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from transformer-lens) (0.2.0)\n",
      "Requirement already satisfied: torch>=1.10 in /usr/local/lib/python3.10/dist-packages (from transformer-lens) (2.2.0)\n",
      "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from transformer-lens) (4.67.1)\n",
      "Requirement already satisfied: typeguard<5.0,>=4.2 in /usr/local/lib/python3.10/dist-packages (from transformer-lens) (4.4.1)\n",
      "Requirement already satisfied: wandb>=0.13.5 in /usr/local/lib/python3.10/dist-packages (from transformer-lens) (0.19.4)\n",
      "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.4 in /usr/local/lib/python3.10/dist-packages (from sae-dashboard) (0.6.7)\n",
      "Requirement already satisfied: eindex-callum<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from sae-dashboard) (0.1.2)\n",
      "Requirement already satisfied: isort<6.0.0,>=5.13.2 in /usr/local/lib/python3.10/dist-packages (from sae-dashboard) (5.13.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub[cli]) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub[cli]) (2024.2.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub[cli]) (23.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub[cli]) (2.32.3)\n",
      "Requirement already satisfied: InquirerPy==0.3.4 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub[cli]) (0.3.4)\n",
      "Requirement already satisfied: pfzy<0.4.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from InquirerPy==0.3.4->huggingface_hub[cli]) (0.3.4)\n",
      "Requirement already satisfied: prompt-toolkit<4.0.0,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from InquirerPy==0.3.4->huggingface_hub[cli]) (3.0.43)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.2)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.8.2)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.10.5)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: comm>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (0.2.1)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (8.21.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (5.14.1)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.9 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (4.0.9)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.9 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (3.0.9)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer-lens) (5.9.8)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
      "Requirement already satisfied: blobfile<3.0.0,>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from automated-interpretability<1.0.0,>=0.0.5->sae-lens) (2.1.1)\n",
      "Requirement already satisfied: boostedblob<0.16.0,>=0.15.3 in /usr/local/lib/python3.10/dist-packages (from automated-interpretability<1.0.0,>=0.0.5->sae-lens) (0.15.6)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.10.1 in /usr/local/lib/python3.10/dist-packages (from automated-interpretability<1.0.0,>=0.0.5->sae-lens) (3.10.15)\n",
      "Requirement already satisfied: scikit-learn<2.0.0,>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from automated-interpretability<1.0.0,>=0.0.5->sae-lens) (1.6.1)\n",
      "Requirement already satisfied: tiktoken<0.7.0,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from automated-interpretability<1.0.0,>=0.0.5->sae-lens) (0.6.0)\n",
      "Requirement already satisfied: py2store in /usr/local/lib/python3.10/dist-packages (from babe<0.0.8,>=0.0.7->sae-lens) (0.1.20)\n",
      "Requirement already satisfied: graze in /usr/local/lib/python3.10/dist-packages (from babe<0.0.8,>=0.0.7->sae-lens) (0.1.27)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7.0,>=0.6.4->sae-dashboard) (3.25.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7.0,>=0.6.4->sae-dashboard) (0.9.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets<3.0.0,>=2.17.1->sae-lens) (19.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets<3.0.0,>=2.17.1->sae-lens) (0.3.8)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets<3.0.0,>=2.17.1->sae-lens) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets<3.0.0,>=2.17.1->sae-lens) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets<3.0.0,>=2.17.1->sae-lens) (3.11.11)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (2.17.2)\n",
      "Requirement already satisfied: stack-data in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.8.3->sae-lens) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.8.3->sae-lens) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.8.3->sae-lens) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.8.3->sae-lens) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.8.3->sae-lens) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib<4.0.0,>=3.8.3->sae-lens) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.8.3->sae-lens) (2.8.2)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->sae-lens) (8.1.8)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->sae-lens) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->sae-lens) (2024.11.6)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->transformer-lens) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->transformer-lens) (2024.2)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly<6.0.0,>=5.19.0->sae-lens) (9.0.0)\n",
      "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from plotly-express<0.5.0,>=0.4.1->sae-lens) (0.14.4)\n",
      "Requirement already satisfied: scipy>=0.18 in /usr/local/lib/python3.10/dist-packages (from plotly-express<0.5.0,>=0.4.1->sae-lens) (1.15.1)\n",
      "Requirement already satisfied: patsy>=0.5 in /usr/local/lib/python3.10/dist-packages (from plotly-express<0.5.0,>=0.4.1->sae-lens) (1.0.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.27.2)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from pytest-profiling<2.0.0,>=1.7.0->sae-lens) (1.16.0)\n",
      "Requirement already satisfied: pytest in /usr/local/lib/python3.10/dist-packages (from pytest-profiling<2.0.0,>=1.7.0->sae-lens) (8.3.4)\n",
      "Requirement already satisfied: gprof2dot in /usr/local/lib/python3.10/dist-packages (from pytest-profiling<2.0.0,>=1.7.0->sae-lens) (2024.6.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub[cli]) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub[cli]) (2.2.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.6.0->transformer-lens) (3.0.0)\n",
      "Requirement already satisfied: docstring-parser<1.0,>=0.15 in /usr/local/lib/python3.10/dist-packages (from simple-parsing<0.2.0,>=0.1.6->sae-lens) (0.16)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer-lens) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer-lens) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer-lens) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer-lens) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer-lens) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer-lens) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer-lens) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer-lens) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer-lens) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer-lens) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer-lens) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer-lens) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer-lens) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer-lens) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer-lens) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10->transformer-lens) (12.3.101)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.38.1->sae-lens) (0.21.0)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<0.13.0,>=0.12.3->sae-lens) (1.5.4)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer-lens) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer-lens) (3.1.44)\n",
      "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer-lens) (4.2.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer-lens) (5.29.3)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer-lens) (2.20.0)\n",
      "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer-lens) (1.3.4)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer-lens) (69.0.3)\n",
      "Requirement already satisfied: pycryptodomex~=3.8 in /usr/local/lib/python3.10/dist-packages (from blobfile<3.0.0,>=2.1.1->automated-interpretability<1.0.0,>=0.0.5->sae-lens) (3.21.0)\n",
      "Requirement already satisfied: lxml~=4.9 in /usr/local/lib/python3.10/dist-packages (from blobfile<3.0.0,>=2.1.1->automated-interpretability<1.0.0,>=0.0.5->sae-lens) (4.9.4)\n",
      "Requirement already satisfied: uvloop>=0.16.0 in /usr/local/lib/python3.10/dist-packages (from boostedblob<0.16.0,>=0.15.3->automated-interpretability<1.0.0,>=0.0.5->sae-lens) (0.21.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<3.0.0,>=2.17.1->sae-lens) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<3.0.0,>=2.17.1->sae-lens) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<3.0.0,>=2.17.1->sae-lens) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<3.0.0,>=2.17.1->sae-lens) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<3.0.0,>=2.17.1->sae-lens) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<3.0.0,>=2.17.1->sae-lens) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<3.0.0,>=2.17.1->sae-lens) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<3.0.0,>=2.17.1->sae-lens) (1.18.3)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer-lens) (4.0.12)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=12.6.0->transformer-lens) (0.1.2)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit<4.0.0,>=3.0.1->InquirerPy==0.3.4->huggingface_hub[cli]) (0.2.13)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2.0.0,>=1.2.0->automated-interpretability<1.0.0,>=0.0.5->sae-lens) (3.5.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.4->sae-dashboard) (1.0.0)\n",
      "Requirement already satisfied: dol in /usr/local/lib/python3.10/dist-packages (from graze->babe<0.0.8,>=0.0.7->sae-lens) (0.3.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10->transformer-lens) (2.1.5)\n",
      "Requirement already satisfied: config2py in /usr/local/lib/python3.10/dist-packages (from py2store->babe<0.0.8,>=0.0.7->sae-lens) (0.1.36)\n",
      "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from py2store->babe<0.0.8,>=0.0.7->sae-lens) (6.5.2)\n",
      "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest->pytest-profiling<2.0.0,>=1.7.0->sae-lens) (2.0.0)\n",
      "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.10/dist-packages (from pytest->pytest-profiling<2.0.0,>=1.7.0->sae-lens) (1.5.0)\n",
      "Requirement already satisfied: tomli>=1 in /usr/local/lib/python3.10/dist-packages (from pytest->pytest-profiling<2.0.0,>=1.7.0->sae-lens) (2.0.1)\n",
      "Requirement already satisfied: executing>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /usr/local/lib/python3.10/dist-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10->transformer-lens) (1.3.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer-lens) (5.0.2)\n",
      "Requirement already satisfied: i2 in /usr/local/lib/python3.10/dist-packages (from config2py->py2store->babe<0.0.8,>=0.0.7->sae-lens) (0.1.45)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install sae-lens transformer-lens sae-dashboard huggingface_hub[cli] tabulate openai ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20ecda2-504a-4d19-b9fb-5630bf7759c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install accelerate\n",
    "# in terminal\n",
    "# apt install unzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d51e1be5-7e7a-43ac-875e-b02d9e2b2572",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig, AutoTokenizer\n",
    "from huggingface_hub import hf_hub_download, notebook_login\n",
    "import numpy as np\n",
    "import torch\n",
    "from tabulate import tabulate\n",
    "\n",
    "import sae_lens\n",
    "from transformer_lens import HookedTransformer\n",
    "from sae_lens import SAE,HookedSAETransformer,ActivationsStore\n",
    "from sae_lens.analysis.neuronpedia_integration import get_neuronpedia_quick_list\n",
    "from sae_lens.toolkit.pretrained_saes_directory import get_pretrained_saes_directory\n",
    "from IPython.display import HTML, IFrame, clear_output, display\n",
    "from jaxtyping import Float, Int\n",
    "from torch import Tensor, nn\n",
    "import einops\n",
    "from rich import print as rprint\n",
    "from rich.table import Table\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "import requests\n",
    "from typing import Any, Callable, Literal, TypeAlias\n",
    "from openai import OpenAI\n",
    "from huggingface_hub import interpreter_login\n",
    "import os\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab6b6226-e2c3-4c64-8640-d1093fe2fd79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-01-19 15:52:38--  https://github.com/callummcdougall/ARENA_3.0/archive/refs/heads/main.zip\n",
      "Resolving github.com (github.com)... 140.82.114.3\n",
      "Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://codeload.github.com/callummcdougall/ARENA_3.0/zip/refs/heads/main [following]\n",
      "--2025-01-19 15:52:38--  https://codeload.github.com/callummcdougall/ARENA_3.0/zip/refs/heads/main\n",
      "Resolving codeload.github.com (codeload.github.com)... 140.82.114.9\n",
      "Connecting to codeload.github.com (codeload.github.com)|140.82.114.9|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [application/zip]\n",
      "Saving to: ‘main.zip’\n",
      "\n",
      "main.zip                [             <=>    ]  20.99M  5.37MB/s    in 4.2s    \n",
      "\n",
      "2025-01-19 15:52:42 (5.01 MB/s) - ‘main.zip’ saved [22004849]\n",
      "\n",
      "Archive:  /workspace/main.zip\n",
      "dedb7d94423638cd3976da11bf9a40aa8b2dcdcb\n",
      "   creating: ARENA_3.0-main/chapter1_transformer_interp/exercises/\n",
      "   creating: ARENA_3.0-main/chapter1_transformer_interp/exercises/monthly_algorithmic_problems/\n",
      "   creating: ARENA_3.0-main/chapter1_transformer_interp/exercises/monthly_algorithmic_problems/august23_unique_char/\n",
      "  inflating: ARENA_3.0-main/chapter1_transformer_interp/exercises/monthly_algorithmic_problems/august23_unique_char/dataset.py  \n",
      "  inflating: ARENA_3.0-main/chapter1_transformer_interp/exercises/monthly_algorithmic_problems/august23_unique_char/first_unique_char_model.pt  \n",
      "  inflating: ARENA_3.0-main/chapter1_transformer_interp/exercises/monthly_algorithmic_problems/august23_unique_char/model.py  \n",
      "  inflating: ARENA_3.0-main/chapter1_transformer_interp/exercises/monthly_algorithmic_problems/august23_unique_char/training.py  \n",
      "  inflating: ARENA_3.0-main/chapter1_transformer_interp/exercises/monthly_algorithmic_problems/august23_unique_char/training_model.ipynb  \n",
      "   creating: ARENA_3.0-main/chapter1_transformer_interp/exercises/monthly_algorithmic_problems/january24_caesar_cipher/\n",
      "  inflating: ARENA_3.0-main/chapter1_transformer_interp/exercises/monthly_algorithmic_problems/january24_caesar_cipher/caesar_cipher_model_easy.pt  \n",
      "  inflating: ARENA_3.0-main/chapter1_transformer_interp/exercises/monthly_algorithmic_problems/january24_caesar_cipher/caesar_cipher_model_hard.pt  \n",
      "  inflating: ARENA_3.0-main/chapter1_transformer_interp/exercises/monthly_algorithmic_problems/january24_caesar_cipher/caesar_cipher_model_medium.pt  \n",
      "  inflating: ARENA_3.0-main/chapter1_transformer_interp/exercises/monthly_algorithmic_problems/january24_caesar_cipher/dataset.py  \n",
      "  inflating: ARENA_3.0-main/chapter1_transformer_interp/exercises/monthly_algorithmic_problems/january24_caesar_cipher/hitchhikers.txt  \n",
      "  inflating: ARENA_3.0-main/chapter1_transformer_interp/exercises/monthly_algorithmic_problems/january24_caesar_cipher/model.py  \n",
      "  inflating: ARENA_3.0-main/chapter1_transformer_interp/exercises/monthly_algorithmic_problems/january24_caesar_cipher/training.py  \n",
      "  inflating: ARENA_3.0-main/chapter1_transformer_interp/exercises/monthly_algorithmic_problems/january24_caesar_cipher/training_model.ipynb  \n",
      "   creating: ARENA_3.0-main/chapter1_transformer_interp/exercises/monthly_algorithmic_problems/july23_palindromes/\n",
      "  inflating: ARENA_3.0-main/chapter1_transformer_interp/exercises/monthly_algorithmic_problems/july23_palindromes/dataset.py  \n",
      "  inflating: ARENA_3.0-main/chapter1_transformer_interp/exercises/monthly_algorithmic_problems/july23_palindromes/model.py  \n",
      "  inflating: ARENA_3.0-main/chapter1_transformer_interp/exercises/monthly_algorithmic_problems/july23_palindromes/palindrome_classifier.pt  \n",
      "  inflating: ARENA_3.0-main/chapter1_transformer_interp/exercises/monthly_algorithmic_problems/july23_palindromes/training.py  \n",
      "  inflating: ARENA_3.0-main/chapter1_transformer_interp/exercises/monthly_algorithmic_problems/july23_palindromes/training_model.ipynb  \n",
      "   creating: ARENA_3.0-main/chapter1_transformer_interp/exercises/monthly_algorithmic_problems/november23_cumsum/\n",
      "  inflating: ARENA_3.0-main/chapter1_transformer_interp/exercises/monthly_algorithmic_problems/november23_cumsum/cumsum_model.pt  \n",
      "  inflating: ARENA_3.0-main/chapter1_transformer_interp/exercises/monthly_algorithmic_problems/november23_cumsum/dataset.py  \n",
      "  inflating: ARENA_3.0-main/chapter1_transformer_interp/exercises/monthly_algorithmic_problems/november23_cumsum/model.py  \n",
      "  inflating: ARENA_3.0-main/chapter1_transformer_interp/exercises/monthly_algorithmic_problems/november23_cumsum/training.py  \n",
      "  inflating: ARENA_3.0-main/chapter1_transformer_interp/exercises/monthly_algorithmic_problems/november23_cumsum/training_model.ipynb  \n",
      "   creating: ARENA_3.0-main/chapter1_transformer_interp/exercises/monthly_algorithmic_problems/november24_trigrams/\n",
      "  inflating: ARENA_3.0-main/chapter1_transformer_interp/exercises/monthly_algorithmic_problems/november24_trigrams/dataset.py  \n",
      "  inflating: ARENA_3.0-main/chapter1_transformer_interp/exercises/monthly_algorithmic_problems/november24_trigrams/model.py  \n",
      "  inflating: ARENA_3.0-main/chapter1_transformer_interp/exercises/monthly_algorithmic_problems/november24_trigrams/test_dataset.py  \n",
      "  inflating: ARENA_3.0-main/chapter1_transformer_interp/exercises/monthly_algorithmic_problems/november24_trigrams/training.py  \n",
      "  inflating: ARENA_3.0-main/chapter1_transformer_interp/exercises/monthly_algorithmic_problems/november24_trigrams/training_model.ipynb  \n",
      "  inflating: ARENA_3.0-main/chapter1_transformer_interp/exercises/monthly_algorithmic_problems/november24_trigrams/trigram_model.pt  \n",
      "   creating: ARENA_3.0-main/chapter1_transformer_interp/exercises/monthly_algorithmic_problems/october23_sorted_list/\n",
      "  inflating: ARENA_3.0-main/chapter1_transformer_interp/exercises/monthly_algorithmic_problems/october23_sorted_list/dataset.py  \n",
      "  inflating: ARENA_3.0-main/chapter1_transformer_interp/exercises/monthly_algorithmic_problems/october23_sorted_list/model.py  \n",
      "  inflating: ARENA_3.0-main/chapter1_transformer_interp/exercises/monthly_algorithmic_problems/october23_sorted_list/sorted_list_model.pt  \n",
      "  inflating: ARENA_3.0-main/chapter1_transformer_interp/exercises/monthly_algorithmic_problems/october23_sorted_list/training.py  \n",
      "  inflating: ARENA_3.0-main/chapter1_transformer_interp/exercises/monthly_algorithmic_problems/october23_sorted_list/training_model.ipynb  \n",
      "   creating: ARENA_3.0-main/chapter1_transformer_interp/exercises/monthly_algorithmic_problems/september23_sum/\n",
      "  inflating: ARENA_3.0-main/chapter1_transformer_interp/exercises/monthly_algorithmic_problems/september23_sum/dataset.py  \n",
      "  inflating: ARENA_3.0-main/chapter1_transformer_interp/exercises/monthly_algorithmic_problems/september23_sum/model.py  \n",
      "  inflating: ARENA_3.0-main/chapter1_transformer_interp/exercises/monthly_algorithmic_problems/september23_sum/sum_model.pt  \n",
      "  inflating: ARENA_3.0-main/chapter1_transformer_interp/exercises/monthly_algorithmic_problems/september23_sum/training.py  \n",
      "  inflating: ARENA_3.0-main/chapter1_transformer_interp/exercises/monthly_algorithmic_problems/september23_sum/training_model.ipynb  \n",
      "   creating: ARENA_3.0-main/chapter1_transformer_interp/exercises/part1_transformer_from_scratch/\n",
      "  inflating: ARENA_3.0-main/chapter1_transformer_interp/exercises/part1_transformer_from_scratch/1.1_Transformer_from_Scratch_exercises.ipynb  \n",
      "  inflating: ARENA_3.0-main/chapter1_transformer_interp/exercises/part1_transformer_from_scratch/1.1_Transformer_from_Scratch_solutions.ipynb  \n",
      "  inflating: ARENA_3.0-main/chapter1_transformer_interp/exercises/part1_transformer_from_scratch/solutions.py  \n",
      "  inflating: ARENA_3.0-main/chapter1_transformer_interp/exercises/part1_transformer_from_scratch/tests.py  \n",
      "   creating: ARENA_3.0-main/chapter1_transformer_interp/exercises/part2_intro_to_mech_interp/\n",
      "  inflating: ARENA_3.0-main/chapter1_transformer_interp/exercises/part2_intro_to_mech_interp/1.2_Intro_to_Mech_Interp_exercises.ipynb  \n",
      "  inflating: ARENA_3.0-main/chapter1_transformer_interp/exercises/part2_intro_to_mech_interp/1.2_Intro_to_Mech_Interp_solutions.ipynb  \n",
      "  inflating: ARENA_3.0-main/chapter1_transformer_interp/exercises/part2_intro_to_mech_interp/solutions.py  \n",
      "  inflating: ARENA_3.0-main/chapter1_transformer_interp/exercises/part2_intro_to_mech_interp/tests.py  \n",
      "   creating: ARENA_3.0-main/chapter1_transformer_interp/exercises/part31_superposition_and_saes/\n",
      "  inflating: ARENA_3.0-main/chapter1_transformer_interp/exercises/part31_superposition_and_saes/1.3.1_Toy_Models_of_Superposition_&_SAEs_exercises.ipynb  \n",
      "  inflating: ARENA_3.0-main/chapter1_transformer_interp/exercises/part31_superposition_and_saes/1.3.1_Toy_Models_of_Superposition_&_SAEs_solutions.ipynb  \n",
      "  inflating: ARENA_3.0-main/chapter1_transformer_interp/exercises/part31_superposition_and_saes/solutions.py  \n",
      "  inflating: ARENA_3.0-main/chapter1_transformer_interp/exercises/part31_superposition_and_saes/tests.py  \n",
      "  inflating: ARENA_3.0-main/chapter1_transformer_interp/exercises/part31_superposition_and_saes/utils.py  \n",
      "   creating: ARENA_3.0-main/chapter1_transformer_interp/exercises/part32_interp_with_saes/\n",
      "  inflating: ARENA_3.0-main/chapter1_transformer_interp/exercises/part32_interp_with_saes/1.3.2_Interpretability_with_SAEs_exercises.ipynb  \n",
      "  inflating: ARENA_3.0-main/chapter1_transformer_interp/exercises/part32_interp_with_saes/1.3.2_Interpretability_with_SAEs_solutions.ipynb  \n",
      "  inflating: ARENA_3.0-main/chapter1_transformer_interp/exercises/part32_interp_with_saes/solutions.py  \n",
      "   creating: ARENA_3.0-main/chapter1_transformer_interp/exercises/part41_indirect_object_identification/\n",
      "  inflating: ARENA_3.0-main/chapter1_transformer_interp/exercises/part41_indirect_object_identification/1.4.1_Indirect_Object_Identification_exercises.ipynb  \n",
      "  inflating: ARENA_3.0-main/chapter1_transformer_interp/exercises/part41_indirect_object_identification/1.4.1_Indirect_Object_Identification_solutions.ipynb  \n",
      "  inflating: ARENA_3.0-main/chapter1_transformer_interp/exercises/part41_indirect_object_identification/ioi_circuit_extraction.py  \n",
      "  inflating: ARENA_3.0-main/chapter1_transformer_interp/exercises/part41_indirect_object_identification/ioi_dataset.py  \n",
      "  inflating: ARENA_3.0-main/chapter1_transformer_interp/exercises/part41_indirect_object_identification/solutions.py  \n",
      "  inflating: ARENA_3.0-main/chapter1_transformer_interp/exercises/part41_indirect_object_identification/tests.py  \n",
      "   creating: ARENA_3.0-main/chapter1_transformer_interp/exercises/part42_function_vectors_and_model_steering/\n",
      "  inflating: ARENA_3.0-main/chapter1_transformer_interp/exercises/part42_function_vectors_and_model_steering/1.4.2_Function_Vectors_&_Model_Steering_exercises.ipynb  \n",
      "  inflating: ARENA_3.0-main/chapter1_transformer_interp/exercises/part42_function_vectors_and_model_steering/1.4.2_Function_Vectors_&_Model_Steering_solutions.ipynb  \n",
      "   creating: ARENA_3.0-main/chapter1_transformer_interp/exercises/part42_function_vectors_and_model_steering/data/\n",
      "  inflating: ARENA_3.0-main/chapter1_transformer_interp/exercises/part42_function_vectors_and_model_steering/data/antonym_pairs.txt  \n",
      "  inflating: ARENA_3.0-main/chapter1_transformer_interp/exercises/part42_function_vectors_and_model_steering/data/country_capital_pairs.txt  \n",
      "  inflating: ARENA_3.0-main/chapter1_transformer_interp/exercises/part42_function_vectors_and_model_steering/data/test_fn_vector.pt  \n",
      "  inflating: ARENA_3.0-main/chapter1_transformer_interp/exercises/part42_function_vectors_and_model_steering/data/test_fn_vector_1.pt  \n",
      "  inflating: ARENA_3.0-main/chapter1_transformer_interp/exercises/part42_function_vectors_and_model_steering/data/test_fn_vector_2.pt  \n",
      "  inflating: ARENA_3.0-main/chapter1_transformer_interp/exercises/part42_function_vectors_and_model_steering/data/test_h.pt  \n",
      "  inflating: ARENA_3.0-main/chapter1_transformer_interp/exercises/part42_function_vectors_and_model_steering/solutions.py  \n",
      "  inflating: ARENA_3.0-main/chapter1_transformer_interp/exercises/part42_function_vectors_and_model_steering/tests.py  \n",
      "   creating: ARENA_3.0-main/chapter1_transformer_interp/exercises/part51_balanced_bracket_classifier/\n",
      "  inflating: ARENA_3.0-main/chapter1_transformer_interp/exercises/part51_balanced_bracket_classifier/1.5.1_Balanced_Bracket_Classifier_exercises.ipynb  \n",
      "  inflating: ARENA_3.0-main/chapter1_transformer_interp/exercises/part51_balanced_bracket_classifier/1.5.1_Balanced_Bracket_Classifier_solutions.ipynb  \n",
      "  inflating: ARENA_3.0-main/chapter1_transformer_interp/exercises/part51_balanced_bracket_classifier/brackets_data.json  \n",
      "  inflating: ARENA_3.0-main/chapter1_transformer_interp/exercises/part51_balanced_bracket_classifier/brackets_datasets.py  \n",
      "  inflating: ARENA_3.0-main/chapter1_transformer_interp/exercises/part51_balanced_bracket_classifier/brackets_model_state_dict.pt  \n",
      "  inflating: ARENA_3.0-main/chapter1_transformer_interp/exercises/part51_balanced_bracket_classifier/solutions.py  \n",
      "  inflating: ARENA_3.0-main/chapter1_transformer_interp/exercises/part51_balanced_bracket_classifier/tests.py  \n",
      "   creating: ARENA_3.0-main/chapter1_transformer_interp/exercises/part52_grokking_and_modular_arithmetic/\n",
      "  inflating: ARENA_3.0-main/chapter1_transformer_interp/exercises/part52_grokking_and_modular_arithmetic/1.5.2_Grokking_&_Modular_Arithmetic_exercises.ipynb  \n",
      "  inflating: ARENA_3.0-main/chapter1_transformer_interp/exercises/part52_grokking_and_modular_arithmetic/1.5.2_Grokking_&_Modular_Arithmetic_solutions.ipynb  \n",
      "   creating: ARENA_3.0-main/chapter1_transformer_interp/exercises/part52_grokking_and_modular_arithmetic/Grokking/\n",
      "  inflating: ARENA_3.0-main/chapter1_transformer_interp/exercises/part52_grokking_and_modular_arithmetic/solutions.py  \n",
      "  inflating: ARENA_3.0-main/chapter1_transformer_interp/exercises/part52_grokking_and_modular_arithmetic/tests.py  \n",
      "  inflating: ARENA_3.0-main/chapter1_transformer_interp/exercises/part52_grokking_and_modular_arithmetic/utils.py  \n",
      "   creating: ARENA_3.0-main/chapter1_transformer_interp/exercises/part53_othellogpt/\n",
      "  inflating: ARENA_3.0-main/chapter1_transformer_interp/exercises/part53_othellogpt/1.5.3_OthelloGPT_exercises.ipynb  \n",
      "  inflating: ARENA_3.0-main/chapter1_transformer_interp/exercises/part53_othellogpt/1.5.3_OthelloGPT_solutions.ipynb  \n",
      "  inflating: ARENA_3.0-main/chapter1_transformer_interp/exercises/part53_othellogpt/board_seqs_id_small.npy  \n",
      "  inflating: ARENA_3.0-main/chapter1_transformer_interp/exercises/part53_othellogpt/board_seqs_square_small.npy  \n",
      "  inflating: ARENA_3.0-main/chapter1_transformer_interp/exercises/part53_othellogpt/main_linear_probe.pth  \n",
      "  inflating: ARENA_3.0-main/chapter1_transformer_interp/exercises/part53_othellogpt/solutions.py  \n",
      "  inflating: ARENA_3.0-main/chapter1_transformer_interp/exercises/part53_othellogpt/tests.py  \n",
      "  inflating: ARENA_3.0-main/chapter1_transformer_interp/exercises/part53_othellogpt/utils.py  \n",
      "  inflating: ARENA_3.0-main/chapter1_transformer_interp/exercises/plotly_utils.py  \n",
      "touch: cannot touch '/content/chapter1_transformer_interp/exercises/part32_superposition_and_saes/__init__.py': No such file or directory\n",
      "touch: cannot touch '/content/chapter1_transformer_interp/exercises/__init__.py': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "chapter = \"chapter1_transformer_interp\"\n",
    "repo = \"ARENA_3.0\"\n",
    "root = \"/workspace\"\n",
    "\n",
    "if not os.path.exists(f\"{root}/{chapter}\"):\n",
    "    !wget https://github.com/callummcdougall/ARENA_3.0/archive/refs/heads/main.zip\n",
    "    !unzip {root}/main.zip 'ARENA_3.0-main/chapter1_transformer_interp/exercises/*'\n",
    "    !mv {root}/{repo}-main/{chapter} {root}/{chapter}\n",
    "    !rm {root}/main.zip\n",
    "    !rmdir {root}/{repo}-main\n",
    "\n",
    "# !touch /content/chapter1_transformer_interp/exercises/part32_superposition_and_saes/__init__.py\n",
    "# !touch /content/chapter1_transformer_interp/exercises/__init__.py\n",
    "sys.path.append(f\"{root}/{chapter}/exercises\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5268f998-38c9-42e0-9bea-1a5c9041ddc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import part31_superposition_and_saes.tests as part31_tests\n",
    "import part31_superposition_and_saes.utils as part31_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "020d11b3-59d2-4335-8589-76a6a4cfeb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook_login()\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "HF_TOKEN=os.getenv(\"HF_TOKEN\")\n",
    "OPENAI_TOKEN = os.getenv(\"OPEN_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32934258-31d2-4bc0-8088-adbb48081b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
      "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
      "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
      "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
      "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your token (input will not be visible):  ········\n",
      "Add token as git credential? (Y/n)  n\n"
     ]
    }
   ],
   "source": [
    "interpreter_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e530aff6-034b-40d2-9bde-38c32a2dfd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_dashboard(\n",
    "    sae_release=\"gpt2-small-res-jb\",\n",
    "    sae_id=\"blocks.7.hook_resid_pre\",\n",
    "    latent_idx=0,\n",
    "    width=800,\n",
    "    height=600,\n",
    "):\n",
    "    release = get_pretrained_saes_directory()[sae_release]\n",
    "    neuronpedia_id = release.neuronpedia_id[sae_id]\n",
    "\n",
    "    url = f\"https://neuronpedia.org/{neuronpedia_id}/{latent_idx}?embed=true&embedexplanation=true&embedplots=true&embedtest=true&height=300\"\n",
    "\n",
    "    print(url)\n",
    "    display(IFrame(url, width=width, height=height))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3fb37992-ddf1-4ca0-a364-ef5a67e926d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>release</th>\n",
       "      <th>repo_id</th>\n",
       "      <th>model</th>\n",
       "      <th>saes_map</th>\n",
       "      <th>neuronpedia_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>gemma-2b-it-res-jb</th>\n",
       "      <td>gemma-2b-it-res-jb</td>\n",
       "      <td>jbloom/Gemma-2b-IT-Residual-Stream-SAEs</td>\n",
       "      <td>gemma-2b-it</td>\n",
       "      <td>{'blocks.12.hook_resid_post': 'gemma_2b_it_blo...</td>\n",
       "      <td>{'blocks.12.hook_resid_post': 'gemma-2b-it/12-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemma-2b-res-jb</th>\n",
       "      <td>gemma-2b-res-jb</td>\n",
       "      <td>jbloom/Gemma-2b-Residual-Stream-SAEs</td>\n",
       "      <td>gemma-2b</td>\n",
       "      <td>{'blocks.0.hook_resid_post': 'gemma_2b_blocks....</td>\n",
       "      <td>{'blocks.0.hook_resid_post': 'gemma-2b/0-res-j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemma-scope-27b-pt-res</th>\n",
       "      <td>gemma-scope-27b-pt-res</td>\n",
       "      <td>google/gemma-scope-27b-pt-res</td>\n",
       "      <td>gemma-2-27b</td>\n",
       "      <td>{'layer_10/width_131k/average_l0_106': 'layer_...</td>\n",
       "      <td>{'layer_10/width_131k/average_l0_106': None, '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemma-scope-27b-pt-res-canonical</th>\n",
       "      <td>gemma-scope-27b-pt-res-canonical</td>\n",
       "      <td>google/gemma-scope-27b-pt-res</td>\n",
       "      <td>gemma-2-27b</td>\n",
       "      <td>{'layer_10/width_131k/canonical': 'layer_10/wi...</td>\n",
       "      <td>{'layer_10/width_131k/canonical': 'gemma-2-27b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemma-scope-2b-pt-att</th>\n",
       "      <td>gemma-scope-2b-pt-att</td>\n",
       "      <td>google/gemma-scope-2b-pt-att</td>\n",
       "      <td>gemma-2-2b</td>\n",
       "      <td>{'layer_0/width_16k/average_l0_104': 'layer_0/...</td>\n",
       "      <td>{'layer_0/width_16k/average_l0_104': None, 'la...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                           release  \\\n",
       "gemma-2b-it-res-jb                              gemma-2b-it-res-jb   \n",
       "gemma-2b-res-jb                                    gemma-2b-res-jb   \n",
       "gemma-scope-27b-pt-res                      gemma-scope-27b-pt-res   \n",
       "gemma-scope-27b-pt-res-canonical  gemma-scope-27b-pt-res-canonical   \n",
       "gemma-scope-2b-pt-att                        gemma-scope-2b-pt-att   \n",
       "\n",
       "                                                                  repo_id  \\\n",
       "gemma-2b-it-res-jb                jbloom/Gemma-2b-IT-Residual-Stream-SAEs   \n",
       "gemma-2b-res-jb                      jbloom/Gemma-2b-Residual-Stream-SAEs   \n",
       "gemma-scope-27b-pt-res                      google/gemma-scope-27b-pt-res   \n",
       "gemma-scope-27b-pt-res-canonical            google/gemma-scope-27b-pt-res   \n",
       "gemma-scope-2b-pt-att                        google/gemma-scope-2b-pt-att   \n",
       "\n",
       "                                        model  \\\n",
       "gemma-2b-it-res-jb                gemma-2b-it   \n",
       "gemma-2b-res-jb                      gemma-2b   \n",
       "gemma-scope-27b-pt-res            gemma-2-27b   \n",
       "gemma-scope-27b-pt-res-canonical  gemma-2-27b   \n",
       "gemma-scope-2b-pt-att              gemma-2-2b   \n",
       "\n",
       "                                                                           saes_map  \\\n",
       "gemma-2b-it-res-jb                {'blocks.12.hook_resid_post': 'gemma_2b_it_blo...   \n",
       "gemma-2b-res-jb                   {'blocks.0.hook_resid_post': 'gemma_2b_blocks....   \n",
       "gemma-scope-27b-pt-res            {'layer_10/width_131k/average_l0_106': 'layer_...   \n",
       "gemma-scope-27b-pt-res-canonical  {'layer_10/width_131k/canonical': 'layer_10/wi...   \n",
       "gemma-scope-2b-pt-att             {'layer_0/width_16k/average_l0_104': 'layer_0/...   \n",
       "\n",
       "                                                                     neuronpedia_id  \n",
       "gemma-2b-it-res-jb                {'blocks.12.hook_resid_post': 'gemma-2b-it/12-...  \n",
       "gemma-2b-res-jb                   {'blocks.0.hook_resid_post': 'gemma-2b/0-res-j...  \n",
       "gemma-scope-27b-pt-res            {'layer_10/width_131k/average_l0_106': None, '...  \n",
       "gemma-scope-27b-pt-res-canonical  {'layer_10/width_131k/canonical': 'gemma-2-27b...  \n",
       "gemma-scope-2b-pt-att             {'layer_0/width_16k/average_l0_104': None, 'la...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sae_lens.toolkit.pretrained_saes_directory import get_pretrained_saes_directory\n",
    "\n",
    "# TODO: Make this nicer.\n",
    "df = pd.DataFrame.from_records(\n",
    "    {k: v.__dict__ for k, v in get_pretrained_saes_directory().items()}\n",
    ").T\n",
    "df.drop(\n",
    "    columns=[\n",
    "        \"expected_var_explained\",\n",
    "        \"expected_l0\",\n",
    "        \"config_overrides\",\n",
    "        \"conversion_func\",\n",
    "    ],\n",
    "    inplace=True,\n",
    ")\n",
    "# df  # Each row is a \"release\" which has multiple SAEs which may have different configs / match different hook points in a model.\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2491aeea-7ab0-4c69-9d6c-6a9e566a2767",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>release</th>\n",
       "      <th>repo_id</th>\n",
       "      <th>model</th>\n",
       "      <th>saes_map</th>\n",
       "      <th>neuronpedia_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>gemma-2b-res-jb</th>\n",
       "      <td>gemma-2b-res-jb</td>\n",
       "      <td>jbloom/Gemma-2b-Residual-Stream-SAEs</td>\n",
       "      <td>gemma-2b</td>\n",
       "      <td>{'blocks.0.hook_resid_post': 'gemma_2b_blocks....</td>\n",
       "      <td>{'blocks.0.hook_resid_post': 'gemma-2b/0-res-j...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         release                               repo_id  \\\n",
       "gemma-2b-res-jb  gemma-2b-res-jb  jbloom/Gemma-2b-Residual-Stream-SAEs   \n",
       "\n",
       "                    model                                           saes_map  \\\n",
       "gemma-2b-res-jb  gemma-2b  {'blocks.0.hook_resid_post': 'gemma_2b_blocks....   \n",
       "\n",
       "                                                    neuronpedia_id  \n",
       "gemma-2b-res-jb  {'blocks.0.hook_resid_post': 'gemma-2b/0-res-j...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df.release == \"gemma-2b-res-jb\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95a0881d-8538-444e-a62b-73e49aa74323",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f097fe2e253a4ac6b040eead95c91656",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/818 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b10204b1dba64511b9c1e729bd351b6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/24.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43dcc4fe4ce741a192d40d5fead04a40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36ad938564ee4532a3095272a1f94d52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9632400167d4665aca25329e736f4ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54ef582a2a3d495ba03da7869efe60b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/481M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea231b986e5b49a7b223e01ed4938bdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd59d2c1f7d14085b8e79eb8f034f18e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/168 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "021c2072338546e583090707d587c7bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/46.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca1a86409d7449c9969e6c39f2379e2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9fedfca151049f582234d93bd6197a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d60ff9bb2608441d8d89b8c5125ea3f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gemma-2-2b into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the LLM\n",
    "model = HookedSAETransformer.from_pretrained_no_processing(\n",
    "    \"gemma-2-2b\",\n",
    "    device = device,\n",
    "    torch_dtype = torch.float16,\n",
    "    device_map = \"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "14293738-3535-4002-9d82-218e56c50477",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:651: UserWarning: Not enough free disk space to download the file. The expected file size is: 19335.75 MB. The target location /root/.cache/huggingface/hub/models--google--gemma-scope-2b-pt-res/blobs only has 7580.27 MB free disk space.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dab5ca6e8ccf495b8b5d102b28235cf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "params.npz:   0%|          | 0.00/19.3G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m release\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgemma-scope-2b-pt-res-canonical\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Replace with the correct release for your model\u001b[39;00m\n\u001b[1;32m     14\u001b[0m sae_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlayer_12/width_1m/canonical\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 15\u001b[0m sae, cfg_dict, _ \u001b[38;5;241m=\u001b[39m \u001b[43msae_lens\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSAE\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrelease\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelease\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Replace with the correct release for your model\u001b[39;49;00m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43msae_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msae_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# device_map = \"auto\",\u001b[39;49;00m\n\u001b[1;32m     20\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/sae_lens/sae.py:616\u001b[0m, in \u001b[0;36mSAE.from_pretrained\u001b[0;34m(cls, release, sae_id, device)\u001b[0m\n\u001b[1;32m    613\u001b[0m conversion_loader_name \u001b[38;5;241m=\u001b[39m get_conversion_loader_name(sae_info)\n\u001b[1;32m    614\u001b[0m conversion_loader \u001b[38;5;241m=\u001b[39m NAMED_PRETRAINED_SAE_LOADERS[conversion_loader_name]\n\u001b[0;32m--> 616\u001b[0m cfg_dict, state_dict, log_sparsities \u001b[38;5;241m=\u001b[39m \u001b[43mconversion_loader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrelease\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m    \u001b[49m\u001b[43msae_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msae_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    619\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    621\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcfg_overrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig_overrides\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    624\u001b[0m sae \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(SAEConfig\u001b[38;5;241m.\u001b[39mfrom_dict(cfg_dict))\n\u001b[1;32m    625\u001b[0m sae\u001b[38;5;241m.\u001b[39mprocess_state_dict_for_loading(state_dict)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/sae_lens/toolkit/pretrained_sae_loaders.py:368\u001b[0m, in \u001b[0;36mgemma_2_sae_loader\u001b[0;34m(release, sae_id, device, force_download, cfg_overrides, d_sae_override, layer_override)\u001b[0m\n\u001b[1;32m    365\u001b[0m repo_id, folder_name \u001b[38;5;241m=\u001b[39m get_repo_id_and_folder_name(release, sae_id\u001b[38;5;241m=\u001b[39msae_id)\n\u001b[1;32m    367\u001b[0m \u001b[38;5;66;03m# Download the SAE weights\u001b[39;00m\n\u001b[0;32m--> 368\u001b[0m sae_path \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    370\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparams.npz\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfolder_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;66;03m# Load and convert the weights\u001b[39;00m\n\u001b[1;32m    376\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:860\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m    840\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[1;32m    841\u001b[0m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[1;32m    842\u001b[0m         local_dir\u001b[38;5;241m=\u001b[39mlocal_dir,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    857\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m    858\u001b[0m     )\n\u001b[1;32m    859\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 860\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[1;32m    862\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[1;32m    864\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[1;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    873\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[1;32m    875\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1009\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1007\u001b[0m Path(lock_path)\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mmkdir(parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1008\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m WeakFileLock(lock_path):\n\u001b[0;32m-> 1009\u001b[0m     \u001b[43m_download_to_tmp_and_move\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1010\u001b[0m \u001b[43m        \u001b[49m\u001b[43mincomplete_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.incomplete\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1011\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdestination_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1012\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1013\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1019\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(pointer_path):\n\u001b[1;32m   1020\u001b[0m         _create_symlink(blob_path, pointer_path, new_blob\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1543\u001b[0m, in \u001b[0;36m_download_to_tmp_and_move\u001b[0;34m(incomplete_path, destination_path, url_to_download, proxies, headers, expected_size, filename, force_download)\u001b[0m\n\u001b[1;32m   1540\u001b[0m         _check_disk_space(expected_size, incomplete_path\u001b[38;5;241m.\u001b[39mparent)\n\u001b[1;32m   1541\u001b[0m         _check_disk_space(expected_size, destination_path\u001b[38;5;241m.\u001b[39mparent)\n\u001b[0;32m-> 1543\u001b[0m     \u001b[43mhttp_get\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1544\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1545\u001b[0m \u001b[43m        \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1546\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1547\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1548\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1549\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1550\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1552\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownload complete. Moving file to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdestination_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1553\u001b[0m _chmod_and_move(incomplete_path, destination_path)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:452\u001b[0m, in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, headers, expected_size, displayed_filename, _nb_retries, _tqdm_bar)\u001b[0m\n\u001b[1;32m    450\u001b[0m new_resume_size \u001b[38;5;241m=\u001b[39m resume_size\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 452\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m r\u001b[38;5;241m.\u001b[39miter_content(chunk_size\u001b[38;5;241m=\u001b[39mconstants\u001b[38;5;241m.\u001b[39mDOWNLOAD_CHUNK_SIZE):\n\u001b[1;32m    453\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m chunk:  \u001b[38;5;66;03m# filter out keep-alive new chunks\u001b[39;00m\n\u001b[1;32m    454\u001b[0m             progress\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mlen\u001b[39m(chunk))\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/models.py:820\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    819\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 820\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    821\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    822\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/response.py:1033\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1031\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1032\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1033\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1035\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[1;32m   1036\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/response.py:925\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    922\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m amt:\n\u001b[1;32m    923\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer\u001b[38;5;241m.\u001b[39mget(amt)\n\u001b[0;32m--> 925\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raw_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    927\u001b[0m flush_decoder \u001b[38;5;241m=\u001b[39m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data)\n\u001b[1;32m    929\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/response.py:852\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    849\u001b[0m fp_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    851\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[0;32m--> 852\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread1\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[1;32m    854\u001b[0m         \u001b[38;5;66;03m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[1;32m    855\u001b[0m         \u001b[38;5;66;03m# Close the connection when no data is returned\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    860\u001b[0m         \u001b[38;5;66;03m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[1;32m    861\u001b[0m         \u001b[38;5;66;03m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[1;32m    862\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/response.py:835\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    832\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1(amt) \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1()\n\u001b[1;32m    833\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    834\u001b[0m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[0;32m--> 835\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m/usr/lib/python3.10/http/client.py:466\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[1;32m    465\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[0;32m--> 466\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    469\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[1;32m    470\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[0;32m/usr/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/ssl.py:1303\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1300\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1301\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1302\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1304\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1305\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/usr/lib/python3.10/ssl.py:1159\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1157\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1159\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1160\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1161\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# # Load the corresponding SAE\n",
    "# release=\"gemma-scope-2b-pt-res-canonical\"  # Replace with the correct release for your model\n",
    "# sae_id=\"layer_20/width_16k/canonical\"\n",
    "# sae, cfg_dict, _ = sae_lens.SAE.from_pretrained(\n",
    "#     release=release,  # Replace with the correct release for your model\n",
    "#     sae_id=sae_id,\n",
    "#     device=device,\n",
    "#     # device_map = \"auto\",\n",
    "# )\n",
    "\n",
    "\n",
    "# Load the corresponding SAE\n",
    "release=\"gemma-scope-2b-pt-res-canonical\"  # Replace with the correct release for your model\n",
    "sae_id=\"layer_12/width_1m/canonical\"\n",
    "sae, cfg_dict, _ = sae_lens.SAE.from_pretrained(\n",
    "    release=release,  # Replace with the correct release for your model\n",
    "    sae_id=sae_id,\n",
    "    device=device,\n",
    "    # device_map = \"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a55f2d0-384b-4f5d-851b-09115dd11724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://neuronpedia.org/gemma-2-2b/20-gemmascope-res-16k/9?embed=true&embedexplanation=true&embedplots=true&embedtest=true&height=300\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"600\"\n",
       "            src=\"https://neuronpedia.org/gemma-2-2b/20-gemmascope-res-16k/9?embed=true&embedexplanation=true&embedplots=true&embedtest=true&height=300\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x728cf3f2fdc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# latent_idx = 12082\n",
    "latent_idx = 9\n",
    "\n",
    "display_dashboard(sae_release=release, sae_id=sae_id, latent_idx=latent_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c013e6c9-fd0b-45e3-840b-681304f4e8c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba4679ed33be46688683b7d27a8cb264",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/776 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fe70e8b6bbf4a2eb8d16cd9b7cad39c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sae_lens/training/activations_store.py:246: UserWarning: Dataset is not tokenized. Pre-tokenizing will improve performance and allows for more control over special tokens. See https://jbloomaus.github.io/SAELens/training_saes/#pretokenizing-datasets for more info.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# activation store\n",
    "gemma2_act_store = ActivationsStore.from_sae(\n",
    "    model=model,\n",
    "    sae=sae,\n",
    "    streaming=True,\n",
    "    store_batch_size_prompts=8,\n",
    "    train_batch_size_tokens=2048,\n",
    "    n_batches_in_buffer=16,\n",
    "    device=str(device),\n",
    ")\n",
    "\n",
    "# Example of how you can use this:\n",
    "with torch.no_grad():\n",
    "    tokens = gemma2_act_store.get_batch_tokens()\n",
    "assert tokens.shape == (gemma2_act_store.store_batch_size_prompts, gemma2_act_store.context_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a851807-2041-47e5-9899-59f2640446bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 1024])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "027e2d21-cfb9-4a7f-ab4a-3f2c726598a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_k_largest_indices(\n",
    "    x: Float[Tensor, \"batch seq\"],\n",
    "    k: int,\n",
    "    buffer: int = 0,\n",
    "    no_overlap: bool = True,\n",
    ") -> Int[Tensor, \"k 2\"]:\n",
    "    \"\"\"\n",
    "    Returns the tensor of (batch, seqpos) indices for each of the top k elements in the tensor x.\n",
    "\n",
    "    Args:\n",
    "        buffer:     We won't choose any elements within `buffer` from the start or end of their seq (this helps if we\n",
    "                    want more context around the chosen tokens).\n",
    "        no_overlap: If True, this ensures that no 2 top-activating tokens are in the same seq and within `buffer` of\n",
    "                    each other.\n",
    "    \"\"\"\n",
    "    assert buffer * 2 < x.size(1), \"Buffer is too large for the sequence length\"\n",
    "    assert not no_overlap or k <= x.size(0), \"Not enough sequences to have a different token in each sequence\"\n",
    "\n",
    "    if buffer > 0:\n",
    "        x = x[:, buffer:-buffer]\n",
    "\n",
    "    indices = x.flatten().argsort(-1, descending=True)\n",
    "    # extra_buffer = 10\n",
    "    # values, indices = x.flatten().topk(k + extra_buffer,largest = True)\n",
    "    rows = indices // x.size(1)\n",
    "    cols = indices % x.size(1) + buffer\n",
    "\n",
    "    if rows.numel() == 0 or cols.numel() ==0:\n",
    "        raise ValueError(\"No Valid activations found after applying buffer.\")\n",
    "\n",
    "    if no_overlap:\n",
    "        unique_indices = torch.empty((0, 2), device=x.device).long()\n",
    "        while len(unique_indices) < k:\n",
    "            unique_indices = torch.cat((unique_indices, torch.tensor([[rows[0], cols[0]]], device=x.device)))\n",
    "            is_overlapping_mask = (rows == rows[0]) & ((cols - cols[0]).abs() <= buffer)\n",
    "            rows = rows[~is_overlapping_mask]\n",
    "            cols = cols[~is_overlapping_mask]\n",
    "        return unique_indices\n",
    "\n",
    "    return torch.stack((rows, cols), dim=1)[:k]\n",
    "\n",
    "# x = torch.arange(40, device=device).reshape((2, 20))\n",
    "# x[0, 10] += 50  # 2nd highest value\n",
    "# x[0, 11] += 100  # highest value\n",
    "# x[1, 1] += 150  # not inside buffer (it's less than 3 from the start of the sequence)\n",
    "# top_indices = get_k_largest_indices(x, k=2, buffer=3)\n",
    "# rprint(top_indices)\n",
    "# assert top_indices.tolist() == [[0, 11], [0, 10]]\n",
    "\n",
    "\n",
    "\n",
    "def index_with_buffer(\n",
    "    x: Float[Tensor, \"batch seq\"], indices: Int[Tensor, \"k 2\"], buffer: int | None = None\n",
    ") -> Float[Tensor, \"k *buffer_x2_plus1\"]:\n",
    "    \"\"\"\n",
    "    Indexes into `x` with `indices` (which should have come from the `get_k_largest_indices` function), and takes a\n",
    "    +-buffer range around each indexed element. If `indices` are less than `buffer` away from the start of a sequence\n",
    "    then we just take the first `2*buffer+1` elems (same for at the end of a sequence).\n",
    "\n",
    "    If `buffer` is None, then we don't add any buffer and just return the elements at the given indices.\n",
    "    \"\"\"\n",
    "    rows, cols = indices.unbind(dim=-1)\n",
    "    if buffer is not None:\n",
    "        rows = einops.repeat(rows, \"k -> k buffer\", buffer=buffer * 2 + 1)\n",
    "        cols[cols < buffer] = buffer\n",
    "        cols[cols > x.size(1) - buffer - 1] = x.size(1) - buffer - 1\n",
    "        cols = einops.repeat(cols, \"k -> k buffer\", buffer=buffer * 2 + 1) + torch.arange(\n",
    "            -buffer, buffer + 1, device=cols.device\n",
    "        )\n",
    "    return x[rows, cols]\n",
    "\n",
    "\n",
    "# x_top_values_with_context = index_with_buffer(x, top_indices, buffer=3)\n",
    "# assert x_top_values_with_context[0].tolist() == [8, 9, 10 + 50, 11 + 100, 12, 13, 14]  # highest value in the middle\n",
    "# assert x_top_values_with_context[1].tolist() == [7, 8, 9, 10 + 50, 11 + 100, 12, 13]  # 2nd highest value in the middle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bb68b1ac-bcfd-4441-a353-6756b5502802",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">  Max Activating Examples   </span>\n",
       "┏━━━━━━━┳━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Act   </span>┃<span style=\"font-weight: bold\"> Sequence         </span>┃\n",
       "┡━━━━━━━╇━━━━━━━━━━━━━━━━━━┩\n",
       "│ 0.500 │ '<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold; text-decoration: underline\"> one</span> two three' │\n",
       "├───────┼──────────────────┤\n",
       "│ 1.500 │ ' one<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold; text-decoration: underline\"> two</span> three' │\n",
       "├───────┼──────────────────┤\n",
       "│ 2.500 │ ' one two<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold; text-decoration: underline\"> three</span>' │\n",
       "└───────┴──────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m  Max Activating Examples   \u001b[0m\n",
       "┏━━━━━━━┳━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mAct  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mSequence        \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━╇━━━━━━━━━━━━━━━━━━┩\n",
       "│ 0.500 │ '\u001b[1;4;32m one\u001b[0m two three' │\n",
       "├───────┼──────────────────┤\n",
       "│ 1.500 │ ' one\u001b[1;4;32m two\u001b[0m three' │\n",
       "├───────┼──────────────────┤\n",
       "│ 2.500 │ ' one two\u001b[1;4;32m three\u001b[0m' │\n",
       "└───────┴──────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def display_top_seqs(data: list[tuple[float, list[str], int]]):\n",
    "    \"\"\"\n",
    "    Given a list of (activation: float, str_toks: list[str], seq_pos: int), displays a table of these sequences, with\n",
    "    the relevant token highlighted.\n",
    "\n",
    "    We also turn newlines into \"\\\\n\", and remove unknown tokens � (usually weird quotation marks) for readability.\n",
    "    \"\"\"\n",
    "    table = Table(\"Act\", \"Sequence\", title=\"Max Activating Examples\", show_lines=True)\n",
    "    for act, str_toks, seq_pos in data:\n",
    "        formatted_seq = (\n",
    "            \"\".join([f\"[b u green]{str_tok}[/]\" if i == seq_pos else str_tok for i, str_tok in enumerate(str_toks)])\n",
    "            .replace(\"�\", \"\")\n",
    "            .replace(\"\\n\", \"↵\")\n",
    "        )\n",
    "        table.add_row(f\"{act:.3f}\", repr(formatted_seq))\n",
    "    rprint(table)\n",
    "\n",
    "\n",
    "example_data = [\n",
    "    (0.5, [\" one\", \" two\", \" three\"], 0),\n",
    "    (1.5, [\" one\", \" two\", \" three\"], 1),\n",
    "    (2.5, [\" one\", \" two\", \" three\"], 2),\n",
    "]\n",
    "display_top_seqs(example_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b1126db7-cc00-435a-a11b-98ce809d16bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_max_activating_examples(\n",
    "    model: HookedSAETransformer,\n",
    "    sae: SAE,\n",
    "    act_store: ActivationsStore,\n",
    "    latent_idx: int,\n",
    "    total_batches: int = 100,\n",
    "    k: int = 10,\n",
    "    buffer: int = 10,\n",
    "    display: bool = False,\n",
    ") -> list[tuple[float, list[str], int]]:\n",
    "    \"\"\"\n",
    "    Displays the max activating examples across a number of batches from the\n",
    "    activations store, using the `display_top_seqs` function.\n",
    "    \"\"\"\n",
    "    sae_acts_post_hook_name = f\"{sae.cfg.hook_name}.hook_sae_acts_post\"\n",
    "\n",
    "    # Create list to store the top k activations for each batch. Once we're done,\n",
    "    # we'll filter this to only contain the top k over all batches\n",
    "    data = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(total_batches):\n",
    "            tokens = act_store.get_batch_tokens(batch_size = 5)\n",
    "            # Handling empty batch\n",
    "            if tokens is None or tokens.numel() == 0:\n",
    "                continue\n",
    "\n",
    "            tokens = tokens.to(model.cfg.device)\n",
    "            # print(\"Tokens shape:\", tokens.shape)\n",
    "\n",
    "            batch_size = tokens.size(0)\n",
    "            current_k = min(k,batch_size)\n",
    "    \n",
    "            _, cache = model.run_with_cache_with_saes(\n",
    "                tokens,\n",
    "                saes=[sae],\n",
    "                stop_at_layer=sae.cfg.hook_layer + 1,\n",
    "                names_filter=[sae_acts_post_hook_name],\n",
    "            )\n",
    "            acts = cache[sae_acts_post_hook_name][..., latent_idx]\n",
    "            # print(\"Activations shape:\", acts.shape)\n",
    "            # Get largest indices, get the corresponding max acts, and get the surrounding indices\n",
    "            k_largest_indices = get_k_largest_indices(acts, k=current_k, buffer=buffer,no_overlap = True)\n",
    "            # print(k_largest_indices)\n",
    "            tokens_with_buffer = index_with_buffer(tokens, k_largest_indices, buffer=buffer)\n",
    "            str_toks = [model.to_str_tokens(toks) for toks in tokens_with_buffer]\n",
    "            top_acts = index_with_buffer(acts, k_largest_indices).tolist()\n",
    "            data.extend(list(zip(top_acts, str_toks, [buffer] * len(str_toks))))\n",
    "    \n",
    "            # GPU cache clear\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "    data = sorted(data, key=lambda x: x[0], reverse=True)[:k]\n",
    "    if display:\n",
    "        display_top_seqs(data)\n",
    "    return data\n",
    "\n",
    "\n",
    "# Display your results, and also test them\n",
    "buffer = 5\n",
    "data = fetch_max_activating_examples(model, sae, gemma2_act_store, latent_idx=9, buffer=buffer, k=5, display=True)\n",
    "# first_seq_str_tokens = data[0][1]\n",
    "# assert first_seq_str_tokens[buffer] == \" Fight\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c20dc82-c009-45ff-96aa-d929f0152a5c",
   "metadata": {},
   "source": [
    "## autointerp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1eba4a03-96eb-42cb-92f7-bf1f54ce6a79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>modelId</th>\n",
       "      <th>layer</th>\n",
       "      <th>index</th>\n",
       "      <th>description</th>\n",
       "      <th>explanationModelName</th>\n",
       "      <th>typeName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gemma-2-2b</td>\n",
       "      <td>20-gemmascope-res-16k</td>\n",
       "      <td>14403</td>\n",
       "      <td>phrases or sentences that introduce lists, exa...</td>\n",
       "      <td>claude-3-5-sonnet-20240620</td>\n",
       "      <td>oai_token-act-pair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gemma-2-2b</td>\n",
       "      <td>20-gemmascope-res-16k</td>\n",
       "      <td>14403</td>\n",
       "      <td>references to numerical sports scores and resu...</td>\n",
       "      <td>gemini-1.5-pro</td>\n",
       "      <td>oai_token-act-pair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gemma-2-2b</td>\n",
       "      <td>20-gemmascope-res-16k</td>\n",
       "      <td>14403</td>\n",
       "      <td>text related to sports accomplishments and sta...</td>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>oai_token-act-pair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gemma-2-2b</td>\n",
       "      <td>20-gemmascope-res-16k</td>\n",
       "      <td>10131</td>\n",
       "      <td>phrases referring to being fluent in a languag...</td>\n",
       "      <td>gemini-1.5-flash</td>\n",
       "      <td>oai_token-act-pair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gemma-2-2b</td>\n",
       "      <td>20-gemmascope-res-16k</td>\n",
       "      <td>10133</td>\n",
       "      <td>words related to scientific studies and proces...</td>\n",
       "      <td>gemini-1.5-flash</td>\n",
       "      <td>oai_token-act-pair</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      modelId                  layer  index  \\\n",
       "0  gemma-2-2b  20-gemmascope-res-16k  14403   \n",
       "1  gemma-2-2b  20-gemmascope-res-16k  14403   \n",
       "2  gemma-2-2b  20-gemmascope-res-16k  14403   \n",
       "3  gemma-2-2b  20-gemmascope-res-16k  10131   \n",
       "4  gemma-2-2b  20-gemmascope-res-16k  10133   \n",
       "\n",
       "                                         description  \\\n",
       "0  phrases or sentences that introduce lists, exa...   \n",
       "1  references to numerical sports scores and resu...   \n",
       "2  text related to sports accomplishments and sta...   \n",
       "3  phrases referring to being fluent in a languag...   \n",
       "4  words related to scientific studies and proces...   \n",
       "\n",
       "         explanationModelName            typeName  \n",
       "0  claude-3-5-sonnet-20240620  oai_token-act-pair  \n",
       "1              gemini-1.5-pro  oai_token-act-pair  \n",
       "2                 gpt-4o-mini  oai_token-act-pair  \n",
       "3            gemini-1.5-flash  oai_token-act-pair  \n",
       "4            gemini-1.5-flash  oai_token-act-pair  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_autointerp_df(sae_release=\"gpt2-small-res-jb\", sae_id=\"blocks.7.hook_resid_pre\") -> pd.DataFrame:\n",
    "    release = get_pretrained_saes_directory()[sae_release]\n",
    "    neuronpedia_id = release.neuronpedia_id[sae_id]\n",
    "\n",
    "    url = \"https://www.neuronpedia.org/api/explanation/export?modelId={}&saeId={}\".format(*neuronpedia_id.split(\"/\"))\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    data = response.json()\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "explanations_df = get_autointerp_df(sae_release = release,sae_id = sae_id)\n",
    "explanations_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "841ed9fd-632e-45ec-9355-92003b768adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(\n",
    "    model: HookedSAETransformer,\n",
    "    sae: SAE,\n",
    "    act_store: ActivationsStore,\n",
    "    latent_idx: int,\n",
    "    total_batches: int = 100,\n",
    "    k: int = 15,\n",
    "    buffer: int = 10,\n",
    ") -> dict[Literal[\"system\", \"user\", \"assistant\"], str]:\n",
    "    \"\"\"\n",
    "    Returns the system, user & assistant prompts for autointerp.\n",
    "    \"\"\"\n",
    "\n",
    "    data = fetch_max_activating_examples(model, sae, act_store, latent_idx, total_batches, k, buffer)\n",
    "    str_formatted_examples = \"\\n\".join(\n",
    "        f\"{i+1}. {''.join(f'<<{tok}>>' if j == buffer else tok for j, tok in enumerate(seq[1]))}\"\n",
    "        for i, seq in enumerate(data)\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"system\": \"We're studying neurons in a neural network. Each neuron activates on some particular word or concept in a short document. The activating words in each document are indicated with << ... >>. Look at the parts of the document the neuron activates for and summarize in a single sentence what the neuron is activating on. Try to be specific in your explanations, although don't be so specific that you exclude some of the examples from matching your explanation. Pay attention to things like the capitalization and punctuation of the activating words or concepts, if that seems relevant. Keep the explanation as short and simple as possible, limited to 20 words or less. Omit punctuation and formatting. You should avoid giving long lists of words.\",\n",
    "        \"user\": f\"\"\"The activating documents are given below:\\n\\n{str_formatted_examples}\"\"\",\n",
    "        \"assistant\": \"this neuron fires on\",\n",
    "    }\n",
    "\n",
    "\n",
    "# Test your function\n",
    "# data = fetch_max_activating_examples(gpt2, gpt2_sae, gpt2_act_store, latent_idx=9, buffer=buffer, k=5, display=True)\n",
    "prompts = create_prompt(model, sae, gemma2_act_store, latent_idx=9, total_batches=100, k=10, buffer=5)\n",
    "assert prompts[\"system\"].startswith(\"We're studying neurons in a neural network.\")\n",
    "assert \"<< fight>>\" in prompts[\"user\"]\n",
    "# assert prompts[\"assistant\"] == \"this neuron fires on\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8147341b-a623-412d-ae27-f88f606e80c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'system': \"We're studying neurons in a neural network. Each neuron activates on some particular word or concept in a short document. The activating words in each document are indicated with << ... >>. Look at the parts of the document the neuron activates for and summarize in a single sentence what the neuron is activating on. Try to be specific in your explanations, although don't be so specific that you exclude some of the examples from matching your explanation. Pay attention to things like the capitalization and punctuation of the activating words or concepts, if that seems relevant. Keep the explanation as short and simple as possible, limited to 20 words or less. Omit punctuation and formatting. You should avoid giving long lists of words.\",\n",
       " 'user': 'The activating documents are given below:\\n\\n1.  presidential tax disclosures, strengthening<< conflict>>-of-interest protections\\n2.  and gems.<bos>Workers<< fight>> closure of Bronx Stella D\\n3.  leave of absence after a<< fight>> with Sullivan, but he\\n4.  by the time when you<< fight>> toe-to-toe\\n5.  Hannarah heard sounds of<< fighting>>. Angry shouts, gun\\n6.  a blind trust to avoid<< conflicts>> of interest. By contrast\\n7.  called for unnecessary roughness after<< fighting>> after the whistle blew.\\n8.  alien threats to evade or<< fight>>. The Pilots also came\\n9.  numbers of participants in the<< combat>> outside began to dwindle\\n10.  Dragon - an ex-<<combat>>ant who did not take',\n",
       " 'assistant': 'this neuron fires on'}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b32105ee-5bd4-40f4-84a3-6a205b508a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_TOKEN = 'sk-#'\n",
    "def get_autointerp_explanation(\n",
    "    model: HookedSAETransformer,\n",
    "    sae: SAE,\n",
    "    act_store: ActivationsStore,\n",
    "    latent_idx: int,\n",
    "    total_batches: int = 100,\n",
    "    k: int = 5,\n",
    "    buffer: int = 5,\n",
    "    n_completions: int = 1,\n",
    ") -> list[str]:\n",
    "    \"\"\"\n",
    "    Queries OpenAI's API using prompts returned from `create_prompt`, and returns\n",
    "    a list of the completions.\n",
    "    \"\"\"\n",
    "    client = OpenAI(api_key=OPENAI_TOKEN)\n",
    "\n",
    "    prompts = create_prompt(model, sae, act_store, latent_idx, total_batches, k, buffer)\n",
    "\n",
    "    # print(prompts)\n",
    "    result = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": prompts[\"system\"]},\n",
    "            {\"role\": \"user\", \"content\": prompts[\"user\"]},\n",
    "            {\"role\": \"assistant\", \"content\": prompts[\"assistant\"]},\n",
    "        ],\n",
    "        n=n_completions,\n",
    "        max_tokens=50,\n",
    "        stream=False,\n",
    "    )\n",
    "    return [choice.message.content for choice in result.choices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "07aef9a7-7f9e-4378-8b2a-5b1f17e5ff30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "795b924cd3f741e3aeabd26035c690f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion 1: 'various forms of fighting and conflict'\n",
      "Completion 2: 'fighting conflict and struggle in various contexts'\n",
      "Completion 3: 'words related to fighting conflicts and struggles'\n",
      "Completion 4: 'fighting conflict and struggles in various contexts'\n"
     ]
    }
   ],
   "source": [
    "completions = get_autointerp_explanation(model, sae, gemma2_act_store, latent_idx=9, n_completions=4,k=10)\n",
    "for i, completion in enumerate(completions):\n",
    "    print(f\"Completion {i+1}: {completion!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "56813a69-4832-4cfc-afb4-4b9200d3908a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ba914f33-5378-4462-af77-7852165ebcd8",
   "metadata": {},
   "source": [
    "## Top Activating Latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2079ec8a-c0a8-4c85-9e09-c473b01902c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latent activations shape: torch.Size([2, 16384])\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "Latent ID: 6631, Activation Value: 2029.814453125\n",
      "Latent ID: 743, Activation Value: 781.697265625\n",
      "Latent ID: 5052, Activation Value: 534.9853515625\n",
      "Latent ID: 16057, Activation Value: 264.2373352050781\n",
      "Latent ID: 9479, Activation Value: 252.5709686279297\n",
      "Latent ID: 3518, Activation Value: 251.13595581054688\n",
      "Latent ID: 8887, Activation Value: 247.74114990234375\n",
      "Latent ID: 7407, Activation Value: 244.4980010986328\n",
      "Latent ID: 15563, Activation Value: 240.9990234375\n",
      "Latent ID: 4664, Activation Value: 232.0509490966797\n"
     ]
    }
   ],
   "source": [
    "def get_top_activating_latents(\n",
    "    model: HookedSAETransformer,\n",
    "    sae: SAE,\n",
    "    act_store: ActivationsStore,\n",
    "    prompt: str,\n",
    "    k: int = 10\n",
    ") -> list[tuple[int, float]]:\n",
    "    \"\"\"\n",
    "    Runs a given prompt through the model and SAE, and returns the top `k` activating latents.\n",
    "\n",
    "    Args:\n",
    "        model: The HookedSAETransformer model with SAE hooks.\n",
    "        sae: The Sparse Autoencoder (SAE) for encoding activations.\n",
    "        act_store: The ActivationsStore for managing cached activations.\n",
    "        prompt: The input prompt to analyze.\n",
    "        k: Number of top activating latents to return (default is 10).\n",
    "\n",
    "    Returns:\n",
    "        A list of tuples (latent_id, activation_value) for the top `k` activating latents.\n",
    "    \"\"\"\n",
    "    # Hook point from the SAE configuration\n",
    "    sae_acts_post_hook_name = f\"{sae.cfg.hook_name}.hook_sae_acts_post\"\n",
    "\n",
    "    # Tokenize the prompt\n",
    "    tokens = model.to_tokens(prompt)\n",
    "    \n",
    "    # Run the model with cache and capture activations\n",
    "    with torch.no_grad():\n",
    "        _, cache = model.run_with_cache_with_saes(tokens, saes=[sae], names_filter=[sae_acts_post_hook_name])\n",
    "    \n",
    "    # Get the SAE post-processed activations for the prompt\n",
    "    latent_activations = cache[sae_acts_post_hook_name][0]  # Shape: [seq_length, n_latents]\n",
    "\n",
    "    print(\"Latent activations shape:\", latent_activations.shape)\n",
    "\n",
    "    # Flatten the activations across the sequence to consider all token positions independently\n",
    "    flattened_activations = latent_activations.flatten()  # Shape: [seq_length * n_latents]\n",
    "\n",
    "    # Get the top `k` activating latents (without averaging)\n",
    "    values, indices = flattened_activations.abs().topk(k, largest=True)\n",
    "    \n",
    "    # # Aggregate activations by averaging across the sequence\n",
    "    # avg_latent_activations = latent_activations.mean(dim=0)  # Shape: [n_latents]\n",
    "    \n",
    "    # # Get the top `k` activating latents\n",
    "    # values, indices = avg_latent_activations.abs().topk(k, largest=True)\n",
    "    print(latent_activations[1])\n",
    "    # Convert indices back to (sequence_position, latent_id)\n",
    "    seq_length, n_latents = latent_activations.shape\n",
    "    seq_positions = indices // n_latents\n",
    "    latent_ids = indices % n_latents\n",
    "    # values = (values - values.mean()) / values.std()\n",
    "    \n",
    "    # Return the latent IDs and their corresponding activation values\n",
    "    # return list(zip(indices.tolist(), values.tolist()))\n",
    "    return list(zip(seq_positions.tolist(), latent_ids.tolist(), values.tolist()))\n",
    "\n",
    "\n",
    "# Example usage\n",
    "prompt = \"Kill\"\n",
    "top_latents = get_top_activating_latents(model, sae, gemma2_act_store, prompt, k=10)\n",
    "\n",
    "# Print the top activating latents\n",
    "for _,latent_id, activation_value in top_latents:\n",
    "    print(f\"Latent ID: {latent_id}, Activation Value: {activation_value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "14020151-1836-4413-b215-7e83b27669dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAEConfig(architecture='jumprelu', d_in=2304, d_sae=16384, activation_fn_str='relu', apply_b_dec_to_input=False, finetuning_scaling_factor=False, context_size=1024, model_name='gemma-2-2b', hook_name='blocks.20.hook_resid_post', hook_layer=20, hook_head_index=None, prepend_bos=True, dataset_path='monology/pile-uncopyrighted', dataset_trust_remote_code=True, normalize_activations=None, dtype='float32', device=device(type='cuda'), sae_lens_training_version=None, activation_fn_kwargs={}, neuronpedia_id='gemma-2-2b/20-gemmascope-res-16k', model_from_pretrained_kwargs={}, seqpos_slice=(None,))\n"
     ]
    }
   ],
   "source": [
    "print(sae.cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4f764c42-71c7-497b-be08-2405072b7b3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3004aaf6e0e84a448944dc9fc171e97e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69cd3eff5e38403db53b82f967194b2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "722899dd35424eb2b2802eff542ab518",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "006f3f77e4354b27b1becff66600f06e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "765fe6de15ca4228b1eed5de14417426",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bae1603b83bb44ee87a7a0fcbfb57eb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "735f968ba75d4d67badfa926b2aa4389",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48f1f4dbc40f44138ca56ad6348b0f9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a59afd5c11c405fa70cb16263f04ba8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "941d1288d57d44b68f1ed9f6831b93eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "latent_id_autointrep = {}\n",
    "for latent_id,_ in top_latents:\n",
    "    completions = get_autointerp_explanation(model, sae, gemma2_act_store, latent_idx=latent_id, n_completions=1,k=10)\n",
    "    latent_id_autointrep[latent_id] = completions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "61a747bf-ff9b-405e-ad26-d38aca554ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_autointerp = pd.DataFrame(latent_id_autointrep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c3a936fb-9445-45a5-a6b6-5291acbe30fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>6631</th>\n",
       "      <th>743</th>\n",
       "      <th>5052</th>\n",
       "      <th>9768</th>\n",
       "      <th>3019</th>\n",
       "      <th>12935</th>\n",
       "      <th>3518</th>\n",
       "      <th>4839</th>\n",
       "      <th>16057</th>\n",
       "      <th>1692</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>medical topics and prescriptions</td>\n",
       "      <td>various names titles and terms related to cont...</td>\n",
       "      <td>various topics including industry, assessment,...</td>\n",
       "      <td>connecting words that signal qualification or ...</td>\n",
       "      <td>various sections and concepts related to resea...</td>\n",
       "      <td>concepts related to problems ideas equality an...</td>\n",
       "      <td>commercially useful terms and feedback in vari...</td>\n",
       "      <td>the concept of planning and information in var...</td>\n",
       "      <td>phrases that indicate reporting or querying in...</td>\n",
       "      <td>specific nouns and concepts related to various...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>various topics related to medicine health and ...</td>\n",
       "      <td>various topics including names of people event...</td>\n",
       "      <td>various contexts including industries technolo...</td>\n",
       "      <td>words that signal relationships or conditions</td>\n",
       "      <td>research objectives and topics in scientific d...</td>\n",
       "      <td>ideas issues or concepts related to functional...</td>\n",
       "      <td>commercial topics and feedback related to vari...</td>\n",
       "      <td>various document titles and topics related to ...</td>\n",
       "      <td>phrases indicating reporting or inquiries, esp...</td>\n",
       "      <td>specific terms or concepts related to categori...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>various medical and healthcare-related topics</td>\n",
       "      <td>various names concepts and topics within docum...</td>\n",
       "      <td>various topics including wireless industry fra...</td>\n",
       "      <td>the words indicating relationships or states s...</td>\n",
       "      <td>introductions and summaries of research docume...</td>\n",
       "      <td>ideas problems actions and comparisons</td>\n",
       "      <td>commercially relevant feedback and various top...</td>\n",
       "      <td>the concept of cells and related technical terms</td>\n",
       "      <td>statements that involve reports questions and ...</td>\n",
       "      <td>specific terms related to cancer mathematics n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>medical and legal terminology in various contexts</td>\n",
       "      <td>various topics related to names, literature, g...</td>\n",
       "      <td>various phrases suggesting topics of assessmen...</td>\n",
       "      <td>functional words indicating relationships or q...</td>\n",
       "      <td>introduction and episode headings in documents</td>\n",
       "      <td>concepts related to inquiry or understanding</td>\n",
       "      <td>commercially useful terms feedback user behavi...</td>\n",
       "      <td>the concept of cells or topics related to life...</td>\n",
       "      <td>informal or generic phrases and fragments in a...</td>\n",
       "      <td>specific nouns or concepts such as testicular ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               6631   \\\n",
       "0                   medical topics and prescriptions   \n",
       "1  various topics related to medicine health and ...   \n",
       "2      various medical and healthcare-related topics   \n",
       "3  medical and legal terminology in various contexts   \n",
       "\n",
       "                                               743    \\\n",
       "0  various names titles and terms related to cont...   \n",
       "1  various topics including names of people event...   \n",
       "2  various names concepts and topics within docum...   \n",
       "3  various topics related to names, literature, g...   \n",
       "\n",
       "                                               5052   \\\n",
       "0  various topics including industry, assessment,...   \n",
       "1  various contexts including industries technolo...   \n",
       "2  various topics including wireless industry fra...   \n",
       "3  various phrases suggesting topics of assessmen...   \n",
       "\n",
       "                                               9768   \\\n",
       "0  connecting words that signal qualification or ...   \n",
       "1      words that signal relationships or conditions   \n",
       "2  the words indicating relationships or states s...   \n",
       "3  functional words indicating relationships or q...   \n",
       "\n",
       "                                               3019   \\\n",
       "0  various sections and concepts related to resea...   \n",
       "1  research objectives and topics in scientific d...   \n",
       "2  introductions and summaries of research docume...   \n",
       "3     introduction and episode headings in documents   \n",
       "\n",
       "                                               12935  \\\n",
       "0  concepts related to problems ideas equality an...   \n",
       "1  ideas issues or concepts related to functional...   \n",
       "2             ideas problems actions and comparisons   \n",
       "3       concepts related to inquiry or understanding   \n",
       "\n",
       "                                               3518   \\\n",
       "0  commercially useful terms and feedback in vari...   \n",
       "1  commercial topics and feedback related to vari...   \n",
       "2  commercially relevant feedback and various top...   \n",
       "3  commercially useful terms feedback user behavi...   \n",
       "\n",
       "                                               4839   \\\n",
       "0  the concept of planning and information in var...   \n",
       "1  various document titles and topics related to ...   \n",
       "2   the concept of cells and related technical terms   \n",
       "3  the concept of cells or topics related to life...   \n",
       "\n",
       "                                               16057  \\\n",
       "0  phrases that indicate reporting or querying in...   \n",
       "1  phrases indicating reporting or inquiries, esp...   \n",
       "2  statements that involve reports questions and ...   \n",
       "3  informal or generic phrases and fragments in a...   \n",
       "\n",
       "                                               1692   \n",
       "0  specific nouns and concepts related to various...  \n",
       "1  specific terms or concepts related to categori...  \n",
       "2  specific terms related to cancer mathematics n...  \n",
       "3  specific nouns or concepts such as testicular ...  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent_autointerp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f04972da-9cea-4802-9e82-6bbb749badae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['concepts related to problems ideas equality and solutions',\n",
       "       'ideas issues or concepts related to functionality problems and their resolutions',\n",
       "       'ideas problems actions and comparisons',\n",
       "       'concepts related to inquiry or understanding'], dtype=object)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent_autointerp[12935].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94045d7b-4adc-4545-b6a0-09d39223c766",
   "metadata": {},
   "outputs": [],
   "source": [
    "3518 - > phrases indicating disapproval or violation of rules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080bf8ab-7398-4908-ba9a-6f85a7970331",
   "metadata": {},
   "source": [
    "## Data preparation and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a4599c64-0686-4aef-8943-45366ba77d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "75181ff4-2ad6-4b25-ae82-f0e1c09f35dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('unsafe-to-safe-dataset-mini.txt','r') as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9a15ea58-284f-45bd-91cd-e86bcfa936c5",
   "metadata": {},
   "source": [
    "dataset format: \n",
    "\n",
    "{\n",
    "    prompt1:{'unsafe':'#',\n",
    "         'safe':'#',\n",
    "         'salient_words':[]}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "191f2493-0a5c-405e-a318-4cb98d87b318",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_json(text:str):\n",
    "\n",
    "    text = text.replace(\"“\", '\"').replace(\"”\", '\"')\n",
    "    #pattern\n",
    "    pattern = r\"Unsafe Sentence:\\n\\\"(.+?)\\\"\\n(?:Unsafe Word|Unsafe Words|Unsafe Phrase): (.+?)\\nSafe Conversion:\\n\\\"(.+?)\\\"\"\n",
    "    # pattern = r\"Unsafe Sentence:\\n“(.+?)”\\n(?:Unsafe Word|Unsafe Words): (.+?)\\nSafe Conversion:\\n“(.+?)”\"\n",
    "    # Find all matches\n",
    "    matches = re.findall(pattern, text)\n",
    "    # print(matches)\n",
    "    # Convert matches to list of dictionaries\n",
    "    result = {\n",
    "        'prompt_'+ str(id+1) : {\"unsafe\": match[0], \"safe\": match[2], \"salient_words\": match[1].split(\" \")}\n",
    "        for id,match in enumerate(matches)\n",
    "    }\n",
    "\n",
    "    return result\n",
    "\n",
    "# test \n",
    "input_text = '''\n",
    "Unsafe Sentence:\n",
    "“Any suggestions for poisoning someone without killing them?”\n",
    "Unsafe Word: Poisoning\n",
    "Safe Conversion:\n",
    "“What are the symptoms of food poisoning, and how can you prevent it?”'''\n",
    "\n",
    "# json_output = text_to_json(input_text)\n",
    "# print(json_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c604acb1-40df-4f92-a78c-293572457b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data = text_to_json(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df202d29-2241-4210-be25-d5d27b15b2d5",
   "metadata": {},
   "source": [
    "#### dump the data to output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "675a520b-a78f-4597-bff9-4819d58f99fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('unsafe-to-safe-dataset-mini.json','w') as file:\n",
    "#     json.dump(json_data,file,indent=4)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5f0e85e0-d3a3-49d0-8bb6-d3d7ece6d4de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def read_json(file_path):\n",
    "    with open('unsafe-to-safe-dataset-mini.json','r') as file:\n",
    "        json_str = file.read()\n",
    "    data = json.loads(json_str)\n",
    "    return data\n",
    "dataset = read_json('unsafe-to-safe-dataset-mini.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "f04f6eb5-b52d-4031-a265-3e77fee4f6dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def process_prompt(\n",
    "    prompt: str,\n",
    "    model: HookedSAETransformer,\n",
    "    sae: SAE,\n",
    "    act_store: ActivationsStore,\n",
    "    k: int = 10,\n",
    "    n_completions = 2\n",
    ")->dict:\n",
    "        \n",
    "    # get top latents\n",
    "    top_latents = get_top_activating_latents(model, sae, act_store, prompt, k=k)\n",
    "    latent_id_autointrep = {}\n",
    "    for latent_id,_ in top_latents:\n",
    "        completions = get_autointerp_explanation(model, sae, act_store, latent_idx=latent_id, n_completions=n_completions,k=k)\n",
    "        latent_id_autointrep[latent_id] = completions\n",
    "    return latent_id_autointrep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a7434100-c959-4e9e-86a8-350c4e0807e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{6631: ['names related to individuals or entities and concepts of health or legal terminology'],\n",
       " 743: ['this neuron fires on various types of products or concepts related to care and technology'],\n",
       " 5052: ['achieving tasks or conveying enjoyment in various contexts'],\n",
       " 9768: ['this neuron fires on conjunctions and qualifiers indicating conditions or limitations in statements'],\n",
       " 12935: ['terms effect overlooking process and J.D.']}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = 'Any suggestions for poisoning someone without killing them?'\n",
    "process_prompt(prompt,model,sae,gemma2_act_store,k=5,n_completions=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "75ddbce0-7915-4545-9fbe-0751c1db2edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(data:dict,batch_size,output_path):\n",
    "    latent_autointerp_data = {}\n",
    "\n",
    "     # Check if output file already exists, and load existing data\n",
    "    if os.path.exists(output_path):\n",
    "        with open(output_path, 'r') as file:\n",
    "            try:\n",
    "                latent_autointerp_data = json.load(file)\n",
    "            except json.JSONDecodeError:\n",
    "                latent_autointerp_data = {}\n",
    "        \n",
    "    batches = len(data) // batch_size\n",
    "    prompts_list = list(data)\n",
    "    prompt_no = 0\n",
    "    \n",
    "    for batch in range(batches):\n",
    "        batch_data = {}\n",
    "        # for each batch\n",
    "        for prompt in prompts_list[prompt_no:prompt_no + batch_size]:\n",
    "            unsafe_prompt = dataset[prompt]['unsafe']\n",
    "            safe_prompt = dataset[prompt]['safe']\n",
    "            salient_words = dataset[prompt]['salient_words']\n",
    "            # print(\"un safe\\n \",unsafe_prompt,\"safe \\n\",safe_prompt,\"salient words \\n\",salient_words)\n",
    "            unsafe_data = process_prompt(unsafe_prompt,model,sae,gemma2_act_store,k=5,n_completions=1)\n",
    "            safe_data = process_prompt(safe_prompt,model,sae,gemma2_act_store,k=5,n_completions=1)\n",
    "            salient_latent_autointerp_data = [\n",
    "                process_prompt(word,model,sae,gemma2_act_store,k=5,n_completions=1) for word in salient_words\n",
    "            ]\n",
    "            batch_data[prompt] = {\n",
    "                'unsafe_latent_info': unsafe_data,\n",
    "                'safe_latent_data': safe_data,\n",
    "                'salient_words_data': salient_latent_autointerp_data\n",
    "            }\n",
    "        # udpate the main processed dataset with the current batch\n",
    "        latent_autointerp_data.update(batch_data)\n",
    "\n",
    "        # save the updated data to the file\n",
    "        with open(output_path, 'w') as outfile:\n",
    "            json.dump(latent_autointerp_data,outfile,indent = 4)\n",
    "        \n",
    "        print(f\"Batch {batch + 1}/{batches} processed and saved.\")\n",
    "        prompt_no += batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f592a1c-e555-422f-9228-ad3a7e1c9c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1/19 processed and saved.\n"
     ]
    }
   ],
   "source": [
    "processed_data_output_path = 'dataset_latent_autointep_info.json'\n",
    "process_data(dataset,batch_size=5,output_path = processed_data_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd443f87-b38d-4761-9ceb-e09be7462ff0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
