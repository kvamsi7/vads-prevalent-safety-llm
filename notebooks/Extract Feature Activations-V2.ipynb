{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "692a4ec9-b403-4710-9d44-c55d07735e25",
   "metadata": {},
   "source": [
    "### install necessary dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22fcce7e-8b7b-4ca8-9c84-8db7e37ee7a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytest (from pytest-profiling<2.0.0,>=1.7.0->sae-lens)\n",
      "  Downloading pytest-8.3.4-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting gprof2dot (from pytest-profiling<2.0.0,>=1.7.0->sae-lens)\n",
      "  Downloading gprof2dot-2024.6.6-py2.py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub[cli]) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub[cli]) (2.2.0)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=12.6.0->transformer-lens)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting docstring-parser<1.0,>=0.15 (from simple-parsing<0.2.0,>=0.1.6->sae-lens)\n",
      "  Downloading docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer-lens) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer-lens) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer-lens) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer-lens) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer-lens) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer-lens) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer-lens) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer-lens) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer-lens) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer-lens) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer-lens) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer-lens) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer-lens) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer-lens) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer-lens) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10->transformer-lens) (12.3.101)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers<5.0.0,>=4.38.1->sae-lens)\n",
      "  Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting shellingham>=1.3.0 (from typer<0.13.0,>=0.12.3->sae-lens)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting docker-pycreds>=0.4.0 (from wandb>=0.13.5->transformer-lens)\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb>=0.13.5->transformer-lens)\n",
      "  Downloading GitPython-3.1.44-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer-lens) (4.2.0)\n",
      "Collecting protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 (from wandb>=0.13.5->transformer-lens)\n",
      "  Downloading protobuf-5.29.3-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Collecting sentry-sdk>=2.0.0 (from wandb>=0.13.5->transformer-lens)\n",
      "  Downloading sentry_sdk-2.20.0-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Collecting setproctitle (from wandb>=0.13.5->transformer-lens)\n",
      "  Downloading setproctitle-1.3.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer-lens) (69.0.3)\n",
      "Collecting pycryptodomex~=3.8 (from blobfile<3.0.0,>=2.1.1->automated-interpretability<1.0.0,>=0.0.5->sae-lens)\n",
      "  Downloading pycryptodomex-3.21.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
      "Collecting lxml~=4.9 (from blobfile<3.0.0,>=2.1.1->automated-interpretability<1.0.0,>=0.0.5->sae-lens)\n",
      "  Downloading lxml-4.9.4-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.7 kB)\n",
      "Collecting uvloop>=0.16.0 (from boostedblob<0.16.0,>=0.15.3->automated-interpretability<1.0.0,>=0.0.5->sae-lens)\n",
      "  Downloading uvloop-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets<3.0.0,>=2.17.1->sae-lens)\n",
      "  Downloading aiohappyeyeballs-2.4.4-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets<3.0.0,>=2.17.1->sae-lens)\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting async-timeout<6.0,>=4.0 (from aiohttp->datasets<3.0.0,>=2.17.1->sae-lens)\n",
      "  Downloading async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<3.0.0,>=2.17.1->sae-lens) (23.2.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets<3.0.0,>=2.17.1->sae-lens)\n",
      "  Downloading frozenlist-1.5.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets<3.0.0,>=2.17.1->sae-lens)\n",
      "  Downloading multidict-6.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->datasets<3.0.0,>=2.17.1->sae-lens)\n",
      "  Downloading propcache-0.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.2 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->datasets<3.0.0,>=2.17.1->sae-lens)\n",
      "  Downloading yarl-1.18.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (69 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.2/69.2 kB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer-lens)\n",
      "  Downloading gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=12.6.0->transformer-lens)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit<4.0.0,>=3.0.1->InquirerPy==0.3.4->huggingface_hub[cli]) (0.2.13)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn<2.0.0,>=1.2.0->automated-interpretability<1.0.0,>=0.0.5->sae-lens)\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.4->sae-dashboard)\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting dol (from graze->babe<0.0.8,>=0.0.7->sae-lens)\n",
      "  Downloading dol-0.3.7-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10->transformer-lens) (2.1.5)\n",
      "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting multiprocess (from datasets<3.0.0,>=2.17.1->sae-lens)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Collecting config2py (from py2store->babe<0.0.8,>=0.0.7->sae-lens)\n",
      "  Downloading config2py-0.1.36-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting importlib_resources (from py2store->babe<0.0.8,>=0.0.7->sae-lens)\n",
      "  Downloading importlib_resources-6.5.2-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting iniconfig (from pytest->pytest-profiling<2.0.0,>=1.7.0->sae-lens)\n",
      "  Downloading iniconfig-2.0.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting pluggy<2,>=1.5 (from pytest->pytest-profiling<2.0.0,>=1.7.0->sae-lens)\n",
      "  Downloading pluggy-1.5.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: tomli>=1 in /usr/local/lib/python3.10/dist-packages (from pytest->pytest-profiling<2.0.0,>=1.7.0->sae-lens) (2.0.1)\n",
      "Requirement already satisfied: executing>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /usr/local/lib/python3.10/dist-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10->transformer-lens) (1.3.0)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer-lens)\n",
      "  Downloading smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting i2 (from config2py->py2store->babe<0.0.8,>=0.0.7->sae-lens)\n",
      "  Downloading i2-0.1.46-py3-none-any.whl.metadata (2.1 kB)\n",
      "Downloading pyzmq-26.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (920 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m920.0/920.0 kB\u001b[0m \u001b[31m81.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading transformer_lens-2.11.0-py3-none-any.whl (177 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.6/177.6 kB\u001b[0m \u001b[31m80.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sae_dashboard-0.6.4-py3-none-any.whl (98 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.9/98.9 kB\u001b[0m \u001b[31m52.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sae_lens-4.4.5-py3-none-any.whl (141 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m67.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading InquirerPy-0.3.4-py3-none-any.whl (67 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.7/67.7 kB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Downloading openai-1.60.2-py3-none-any.whl (456 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m456.1/456.1 kB\u001b[0m \u001b[31m120.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-1.3.0-py3-none-any.whl (336 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m336.6/336.6 kB\u001b[0m \u001b[31m109.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading automated_interpretability-0.0.6-py3-none-any.whl (57 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading babe-0.0.7-py3-none-any.whl (6.9 kB)\n",
      "Downloading beartype-0.14.1-py3-none-any.whl (739 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m739.7/739.7 kB\u001b[0m \u001b[31m110.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading better_abc-0.0.3-py3-none-any.whl (3.5 kB)\n",
      "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Downloading datasets-2.21.0-py3-none-any.whl (527 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m527.3/527.3 kB\u001b[0m \u001b[31m125.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading eindex_callum-0.1.2-py3-none-any.whl (8.3 kB)\n",
      "Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fancy_einsum-0.0.3-py3-none-any.whl (6.2 kB)\n",
      "Downloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.28.0-py3-none-any.whl (464 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m464.1/464.1 kB\u001b[0m \u001b[31m131.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading isort-5.13.2-py3-none-any.whl (92 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.3/92.3 kB\u001b[0m \u001b[31m51.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jaxtyping-0.2.37-py3-none-any.whl (56 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 kB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jiter-0.8.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (345 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.0/345.0 kB\u001b[0m \u001b[31m116.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading matplotlib-3.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m172.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m139.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m169.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading plotly-5.24.1-py3-none-any.whl (19.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.1/19.1 MB\u001b[0m \u001b[31m143.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading plotly_express-0.4.1-py2.py3-none-any.whl (2.9 kB)\n",
      "Downloading pydantic-2.10.6-py3-none-any.whl (431 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.7/431.7 kB\u001b[0m \u001b[31m125.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_core-2.27.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m163.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytest_profiling-1.8.1-py3-none-any.whl (9.9 kB)\n",
      "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.4/242.4 kB\u001b[0m \u001b[31m92.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (435 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m435.0/435.0 kB\u001b[0m \u001b[31m126.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading simple_parsing-0.1.7-py3-none-any.whl (112 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.8/112.8 kB\u001b[0m \u001b[31m60.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.48.1-py3-none-any.whl (9.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m159.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading typeguard-4.4.1-py3-none-any.whl (35 kB)\n",
      "Downloading typer-0.12.5-py3-none-any.whl (47 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.3/47.3 kB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Downloading wandb-0.19.5-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.3/20.3 MB\u001b[0m \u001b[31m158.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading zstandard-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m172.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m144.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading blobfile-2.1.1-py3-none-any.whl (73 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.7/73.7 kB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading boostedblob-0.15.6-py3-none-any.whl (59 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.2/59.2 kB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.11.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m140.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading click-8.1.8-py3-none-any.whl (98 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.2/98.2 kB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (324 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.0/325.0 kB\u001b[0m \u001b[31m114.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m60.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Downloading docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
      "Downloading fonttools-4.55.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m167.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading GitPython-3.1.44-py3-none-any.whl (207 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.6/207.6 kB\u001b[0m \u001b[31m81.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.8-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m144.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading marshmallow-3.26.0-py3-none-any.whl (50 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading orjson-3.10.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.3/130.3 kB\u001b[0m \u001b[31m55.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading patsy-1.0.1-py2.py3-none-any.whl (232 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.9/232.9 kB\u001b[0m \u001b[31m78.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pfzy-0.3.4-py3-none-any.whl (8.5 kB)\n",
      "Downloading protobuf-5.29.3-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m74.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-19.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (42.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.1/42.1 MB\u001b[0m \u001b[31m99.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m508.0/508.0 kB\u001b[0m \u001b[31m126.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.7/781.7 kB\u001b[0m \u001b[31m135.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m159.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m100.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (40.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.6/40.6 MB\u001b[0m \u001b[31m94.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading sentry_sdk-2.20.0-py2.py3-none-any.whl (322 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.6/322.6 kB\u001b[0m \u001b[31m95.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading statsmodels-0.14.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m143.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading tenacity-9.0.0-py3-none-any.whl (28 kB)\n",
      "Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m163.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m158.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading tzdata-2025.1-py2.py3-none-any.whl (346 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m346.8/346.8 kB\u001b[0m \u001b[31m110.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading wadler_lindig-0.1.3-py3-none-any.whl (20 kB)\n",
      "Downloading gprof2dot-2024.6.6-py2.py3-none-any.whl (34 kB)\n",
      "Downloading graze-0.1.27-py3-none-any.whl (19 kB)\n",
      "Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m56.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytest-8.3.4-py3-none-any.whl (343 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m343.1/343.1 kB\u001b[0m \u001b[31m82.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading setproctitle-1.3.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
      "Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m89.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohappyeyeballs-2.4.4-py3-none-any.whl (14 kB)\n",
      "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Downloading frozenlist-1.5.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (241 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.9/241.9 kB\u001b[0m \u001b[31m98.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading lxml-4.9.4-cp310-cp310-manylinux_2_28_x86_64.whl (7.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m166.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Downloading multidict-6.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.6/124.6 kB\u001b[0m \u001b[31m54.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Downloading pluggy-1.5.0-py3-none-any.whl (20 kB)\n",
      "Downloading propcache-0.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (205 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m205.1/205.1 kB\u001b[0m \u001b[31m87.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pycryptodomex-3.21.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m152.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Downloading uvloop-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m157.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading yarl-1.18.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (319 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m94.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading config2py-0.1.36-py3-none-any.whl (32 kB)\n",
      "Downloading dol-0.3.7-py3-none-any.whl (247 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.7/247.7 kB\u001b[0m \u001b[31m84.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading importlib_resources-6.5.2-py3-none-any.whl (37 kB)\n",
      "Downloading iniconfig-2.0.0-py3-none-any.whl (5.9 kB)\n",
      "Downloading smmap-5.0.2-py3-none-any.whl (24 kB)\n",
      "Downloading i2-0.1.46-py3-none-any.whl (202 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m202.8/202.8 kB\u001b[0m \u001b[31m87.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: py2store\n",
      "  Building wheel for py2store (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for py2store: filename=py2store-0.1.20-py3-none-any.whl size=118410 sha256=5d23a0694e6d1b5e5652c811ca1bd05bad05066b3e020c949b8638521c71e6d1\n",
      "  Stored in directory: /root/.cache/pip/wheels/ff/40/40/fa84c63029cbb45f4f3824be4be62c6838436ad4cb264b5585\n",
      "Successfully built py2store\n",
      "Installing collected packages: sentencepiece, pytz, i2, dol, better-abc, zstandard, xxhash, wadler-lindig, uvloop, tzdata, typing-extensions, tqdm, threadpoolctl, tenacity, tabulate, smmap, shellingham, setproctitle, sentry-sdk, scipy, safetensors, requests, regex, pyzmq, python-dotenv, pycryptodomex, pyarrow, protobuf, propcache, pluggy, pfzy, patsy, orjson, mypy-extensions, mdurl, marshmallow, lxml, kiwisolver, joblib, jiter, isort, iniconfig, importlib_resources, gprof2dot, frozenlist, fonttools, fancy-einsum, einops, docstring-parser, docker-pycreds, dill, cycler, contourpy, config2py, click, beartype, async-timeout, annotated-types, aiohappyeyeballs, typing-inspect, typeguard, tiktoken, simple-parsing, scikit-learn, pytest, pydantic-core, py2store, plotly, pandas, nltk, multiprocess, multidict, matplotlib, markdown-it-py, jaxtyping, InquirerPy, huggingface_hub, graze, gitdb, blobfile, aiosignal, yarl, tokenizers, statsmodels, rich, pytest-profiling, pydantic, httpx, gitpython, dataclasses-json, babe, wandb, typer, transformers, plotly-express, openai, eindex-callum, aiohttp, accelerate, boostedblob, datasets, automated-interpretability, transformer-lens, sae-lens, sae-dashboard\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.9.0\n",
      "    Uninstalling typing_extensions-4.9.0:\n",
      "      Successfully uninstalled typing_extensions-4.9.0\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.31.0\n",
      "    Uninstalling requests-2.31.0:\n",
      "      Successfully uninstalled requests-2.31.0\n",
      "  Attempting uninstall: pyzmq\n",
      "    Found existing installation: pyzmq 24.0.1\n",
      "    Uninstalling pyzmq-24.0.1:\n",
      "      Successfully uninstalled pyzmq-24.0.1\n",
      "  Attempting uninstall: lxml\n",
      "    Found existing installation: lxml 5.1.0\n",
      "    Uninstalling lxml-5.1.0:\n",
      "      Successfully uninstalled lxml-5.1.0\n",
      "  Attempting uninstall: httpx\n",
      "    Found existing installation: httpx 0.26.0\n",
      "    Uninstalling httpx-0.26.0:\n",
      "      Successfully uninstalled httpx-0.26.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "notebook 6.5.5 requires pyzmq<25,>=17, but you have pyzmq 26.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed InquirerPy-0.3.4 accelerate-1.3.0 aiohappyeyeballs-2.4.4 aiohttp-3.11.11 aiosignal-1.3.2 annotated-types-0.7.0 async-timeout-5.0.1 automated-interpretability-0.0.6 babe-0.0.7 beartype-0.14.1 better-abc-0.0.3 blobfile-2.1.1 boostedblob-0.15.6 click-8.1.8 config2py-0.1.36 contourpy-1.3.1 cycler-0.12.1 dataclasses-json-0.6.7 datasets-2.21.0 dill-0.3.8 docker-pycreds-0.4.0 docstring-parser-0.16 dol-0.3.7 eindex-callum-0.1.2 einops-0.7.0 fancy-einsum-0.0.3 fonttools-4.55.8 frozenlist-1.5.0 gitdb-4.0.12 gitpython-3.1.44 gprof2dot-2024.6.6 graze-0.1.27 httpx-0.27.2 huggingface_hub-0.28.0 i2-0.1.46 importlib_resources-6.5.2 iniconfig-2.0.0 isort-5.13.2 jaxtyping-0.2.37 jiter-0.8.2 joblib-1.4.2 kiwisolver-1.4.8 lxml-4.9.4 markdown-it-py-3.0.0 marshmallow-3.26.0 matplotlib-3.10.0 mdurl-0.1.2 multidict-6.1.0 multiprocess-0.70.16 mypy-extensions-1.0.0 nltk-3.9.1 openai-1.60.2 orjson-3.10.15 pandas-2.2.3 patsy-1.0.1 pfzy-0.3.4 plotly-5.24.1 plotly-express-0.4.1 pluggy-1.5.0 propcache-0.2.1 protobuf-5.29.3 py2store-0.1.20 pyarrow-19.0.0 pycryptodomex-3.21.0 pydantic-2.10.6 pydantic-core-2.27.2 pytest-8.3.4 pytest-profiling-1.8.1 python-dotenv-1.0.1 pytz-2024.2 pyzmq-26.0.0 regex-2024.11.6 requests-2.32.3 rich-13.9.4 sae-dashboard-0.6.4 sae-lens-4.4.5 safetensors-0.4.5 scikit-learn-1.6.1 scipy-1.15.1 sentencepiece-0.2.0 sentry-sdk-2.20.0 setproctitle-1.3.4 shellingham-1.5.4 simple-parsing-0.1.7 smmap-5.0.2 statsmodels-0.14.4 tabulate-0.9.0 tenacity-9.0.0 threadpoolctl-3.5.0 tiktoken-0.6.0 tokenizers-0.21.0 tqdm-4.67.1 transformer-lens-2.11.0 transformers-4.48.1 typeguard-4.4.1 typer-0.12.5 typing-extensions-4.12.2 typing-inspect-0.9.0 tzdata-2025.1 uvloop-0.21.0 wadler-lindig-0.1.3 wandb-0.19.5 xxhash-3.5.0 yarl-1.18.3 zstandard-0.22.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install sae-lens transformer-lens sae-dashboard huggingface_hub[cli] tabulate openai ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20ecda2-504a-4d19-b9fb-5630bf7759c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install accelerate\n",
    "# in terminal\n",
    "# apt install unzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d51e1be5-7e7a-43ac-875e-b02d9e2b2572",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig, AutoTokenizer\n",
    "from huggingface_hub import hf_hub_download, notebook_login\n",
    "import numpy as np\n",
    "import torch\n",
    "from tabulate import tabulate\n",
    "\n",
    "import sae_lens\n",
    "from transformer_lens import HookedTransformer\n",
    "from sae_lens import SAE,HookedSAETransformer,ActivationsStore\n",
    "from sae_lens.analysis.neuronpedia_integration import get_neuronpedia_quick_list\n",
    "from sae_lens.toolkit.pretrained_saes_directory import get_pretrained_saes_directory\n",
    "from IPython.display import HTML, IFrame, clear_output, display\n",
    "from jaxtyping import Float, Int\n",
    "from torch import Tensor, nn\n",
    "import einops\n",
    "from rich import print as rprint\n",
    "from rich.table import Table\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "import requests\n",
    "from typing import Any, Callable, Literal, TypeAlias\n",
    "from openai import OpenAI\n",
    "from huggingface_hub import interpreter_login\n",
    "import os\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab6b6226-e2c3-4c64-8640-d1093fe2fd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "chapter = \"chapter1_transformer_interp\"\n",
    "repo = \"ARENA_3.0\"\n",
    "root = \"/workspace/vads-prevalent-safety-llm/notebooks\"\n",
    "\n",
    "if not os.path.exists(f\"{root}/{chapter}\"):\n",
    "    !wget https://github.com/callummcdougall/ARENA_3.0/archive/refs/heads/main.zip\n",
    "    !unzip {root}/main.zip 'ARENA_3.0-main/chapter1_transformer_interp/exercises/*'\n",
    "    !mv {root}/{repo}-main/{chapter} {root}/{chapter}\n",
    "    !rm {root}/main.zip\n",
    "    !rmdir {root}/{repo}-main\n",
    "\n",
    "# !touch {root}/chapter1_transformer_interp/exercises/part32_superposition_and_saes/__init__.py\n",
    "# !touch {root}/chapter1_transformer_interp/exercises/__init__.py\n",
    "\n",
    "# !touch //chapter1_transformer_interp/exercises/part32_superposition_and_saes/__init__.py\n",
    "# !touch /content/chapter1_transformer_interp/exercises/__init__.py\n",
    "sys.path.append(f\"{root}/{chapter}/exercises\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5268f998-38c9-42e0-9bea-1a5c9041ddc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import part31_superposition_and_saes.tests as part31_tests\n",
    "import part31_superposition_and_saes.utils as part31_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "020d11b3-59d2-4335-8589-76a6a4cfeb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook_login()\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "HF_TOKEN=os.getenv(\"HF_TOKEN\")\n",
    "OPENAI_TOKEN = os.getenv(\"OPEN_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32934258-31d2-4bc0-8088-adbb48081b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
      "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
      "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
      "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
      "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your token (input will not be visible):  ········\n",
      "Add token as git credential? (Y/n)  n\n"
     ]
    }
   ],
   "source": [
    "interpreter_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e530aff6-034b-40d2-9bde-38c32a2dfd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_dashboard(\n",
    "    sae_release=\"gpt2-small-res-jb\",\n",
    "    sae_id=\"blocks.7.hook_resid_pre\",\n",
    "    latent_idx=0,\n",
    "    width=800,\n",
    "    height=600,\n",
    "):\n",
    "    release = get_pretrained_saes_directory()[sae_release]\n",
    "    neuronpedia_id = release.neuronpedia_id[sae_id]\n",
    "\n",
    "    url = f\"https://neuronpedia.org/{neuronpedia_id}/{latent_idx}?embed=true&embedexplanation=true&embedplots=true&embedtest=true&height=300\"\n",
    "\n",
    "    print(url)\n",
    "    display(IFrame(url, width=width, height=height))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3fb37992-ddf1-4ca0-a364-ef5a67e926d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>release</th>\n",
       "      <th>repo_id</th>\n",
       "      <th>model</th>\n",
       "      <th>saes_map</th>\n",
       "      <th>neuronpedia_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>gemma-2b-it-res-jb</th>\n",
       "      <td>gemma-2b-it-res-jb</td>\n",
       "      <td>jbloom/Gemma-2b-IT-Residual-Stream-SAEs</td>\n",
       "      <td>gemma-2b-it</td>\n",
       "      <td>{'blocks.12.hook_resid_post': 'gemma_2b_it_blo...</td>\n",
       "      <td>{'blocks.12.hook_resid_post': 'gemma-2b-it/12-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemma-2b-res-jb</th>\n",
       "      <td>gemma-2b-res-jb</td>\n",
       "      <td>jbloom/Gemma-2b-Residual-Stream-SAEs</td>\n",
       "      <td>gemma-2b</td>\n",
       "      <td>{'blocks.0.hook_resid_post': 'gemma_2b_blocks....</td>\n",
       "      <td>{'blocks.0.hook_resid_post': 'gemma-2b/0-res-j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemma-scope-27b-pt-res</th>\n",
       "      <td>gemma-scope-27b-pt-res</td>\n",
       "      <td>google/gemma-scope-27b-pt-res</td>\n",
       "      <td>gemma-2-27b</td>\n",
       "      <td>{'layer_10/width_131k/average_l0_106': 'layer_...</td>\n",
       "      <td>{'layer_10/width_131k/average_l0_106': None, '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemma-scope-27b-pt-res-canonical</th>\n",
       "      <td>gemma-scope-27b-pt-res-canonical</td>\n",
       "      <td>google/gemma-scope-27b-pt-res</td>\n",
       "      <td>gemma-2-27b</td>\n",
       "      <td>{'layer_10/width_131k/canonical': 'layer_10/wi...</td>\n",
       "      <td>{'layer_10/width_131k/canonical': 'gemma-2-27b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemma-scope-2b-pt-att</th>\n",
       "      <td>gemma-scope-2b-pt-att</td>\n",
       "      <td>google/gemma-scope-2b-pt-att</td>\n",
       "      <td>gemma-2-2b</td>\n",
       "      <td>{'layer_0/width_16k/average_l0_104': 'layer_0/...</td>\n",
       "      <td>{'layer_0/width_16k/average_l0_104': None, 'la...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                           release  \\\n",
       "gemma-2b-it-res-jb                              gemma-2b-it-res-jb   \n",
       "gemma-2b-res-jb                                    gemma-2b-res-jb   \n",
       "gemma-scope-27b-pt-res                      gemma-scope-27b-pt-res   \n",
       "gemma-scope-27b-pt-res-canonical  gemma-scope-27b-pt-res-canonical   \n",
       "gemma-scope-2b-pt-att                        gemma-scope-2b-pt-att   \n",
       "\n",
       "                                                                  repo_id  \\\n",
       "gemma-2b-it-res-jb                jbloom/Gemma-2b-IT-Residual-Stream-SAEs   \n",
       "gemma-2b-res-jb                      jbloom/Gemma-2b-Residual-Stream-SAEs   \n",
       "gemma-scope-27b-pt-res                      google/gemma-scope-27b-pt-res   \n",
       "gemma-scope-27b-pt-res-canonical            google/gemma-scope-27b-pt-res   \n",
       "gemma-scope-2b-pt-att                        google/gemma-scope-2b-pt-att   \n",
       "\n",
       "                                        model  \\\n",
       "gemma-2b-it-res-jb                gemma-2b-it   \n",
       "gemma-2b-res-jb                      gemma-2b   \n",
       "gemma-scope-27b-pt-res            gemma-2-27b   \n",
       "gemma-scope-27b-pt-res-canonical  gemma-2-27b   \n",
       "gemma-scope-2b-pt-att              gemma-2-2b   \n",
       "\n",
       "                                                                           saes_map  \\\n",
       "gemma-2b-it-res-jb                {'blocks.12.hook_resid_post': 'gemma_2b_it_blo...   \n",
       "gemma-2b-res-jb                   {'blocks.0.hook_resid_post': 'gemma_2b_blocks....   \n",
       "gemma-scope-27b-pt-res            {'layer_10/width_131k/average_l0_106': 'layer_...   \n",
       "gemma-scope-27b-pt-res-canonical  {'layer_10/width_131k/canonical': 'layer_10/wi...   \n",
       "gemma-scope-2b-pt-att             {'layer_0/width_16k/average_l0_104': 'layer_0/...   \n",
       "\n",
       "                                                                     neuronpedia_id  \n",
       "gemma-2b-it-res-jb                {'blocks.12.hook_resid_post': 'gemma-2b-it/12-...  \n",
       "gemma-2b-res-jb                   {'blocks.0.hook_resid_post': 'gemma-2b/0-res-j...  \n",
       "gemma-scope-27b-pt-res            {'layer_10/width_131k/average_l0_106': None, '...  \n",
       "gemma-scope-27b-pt-res-canonical  {'layer_10/width_131k/canonical': 'gemma-2-27b...  \n",
       "gemma-scope-2b-pt-att             {'layer_0/width_16k/average_l0_104': None, 'la...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sae_lens.toolkit.pretrained_saes_directory import get_pretrained_saes_directory\n",
    "\n",
    "# TODO: Make this nicer.\n",
    "df = pd.DataFrame.from_records(\n",
    "    {k: v.__dict__ for k, v in get_pretrained_saes_directory().items()}\n",
    ").T\n",
    "df.drop(\n",
    "    columns=[\n",
    "        \"expected_var_explained\",\n",
    "        \"expected_l0\",\n",
    "        \"config_overrides\",\n",
    "        \"conversion_func\",\n",
    "    ],\n",
    "    inplace=True,\n",
    ")\n",
    "# df  # Each row is a \"release\" which has multiple SAEs which may have different configs / match different hook points in a model.\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2491aeea-7ab0-4c69-9d6c-6a9e566a2767",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>release</th>\n",
       "      <th>repo_id</th>\n",
       "      <th>model</th>\n",
       "      <th>saes_map</th>\n",
       "      <th>neuronpedia_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>gemma-2b-res-jb</th>\n",
       "      <td>gemma-2b-res-jb</td>\n",
       "      <td>jbloom/Gemma-2b-Residual-Stream-SAEs</td>\n",
       "      <td>gemma-2b</td>\n",
       "      <td>{'blocks.0.hook_resid_post': 'gemma_2b_blocks....</td>\n",
       "      <td>{'blocks.0.hook_resid_post': 'gemma-2b/0-res-j...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         release                               repo_id  \\\n",
       "gemma-2b-res-jb  gemma-2b-res-jb  jbloom/Gemma-2b-Residual-Stream-SAEs   \n",
       "\n",
       "                    model                                           saes_map  \\\n",
       "gemma-2b-res-jb  gemma-2b  {'blocks.0.hook_resid_post': 'gemma_2b_blocks....   \n",
       "\n",
       "                                                    neuronpedia_id  \n",
       "gemma-2b-res-jb  {'blocks.0.hook_resid_post': 'gemma-2b/0-res-j...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df.release == \"gemma-2b-res-jb\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da072c4d-6eb9-454b-8859-0b8c5ca07923",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ff2368-f4a6-4451-964b-4a5ba7b23803",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c515a9d3-703f-4f45-85fe-70c7f5c9bd91",
   "metadata": {},
   "source": [
    "## Load the Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95a0881d-8538-444e-a62b-73e49aa74323",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad29c89fdf5c43a19ad630d79c0debba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/818 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a96af08d6e354d5194ff3d3e13625416",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/24.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d84f4ca7a5f84c1c9fbdfa208def398a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22b8c634628f4cafbf3421356acb85e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c83b1f309894874be62491156a37361",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b0184aba1734892bc0b68fc68815073",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/481M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99a4406c50e84882ac1a30ec0673fa5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36792144914e441696387959d0a48511",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/168 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2ae2001b88740fc996472347c90967c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/46.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88893b30bd9040408c57d32087c65e29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdcb9981e4ae421597bd18a90391b969",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb74f25b40d44524b759ae1f87789d5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gemma-2-2b into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the LLM\n",
    "model = HookedSAETransformer.from_pretrained_no_processing(\n",
    "    \"gemma-2-2b\",\n",
    "    device = device,\n",
    "    torch_dtype = torch.float16,\n",
    "    device_map = \"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d74bc5-43a2-44b0-8a09-cd46c9e3aa3b",
   "metadata": {},
   "source": [
    "## Load SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14293738-3535-4002-9d82-218e56c50477",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0a9908c184d4387b487e24bea25760d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "params.npz:   0%|          | 0.00/302M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the corresponding SAE\n",
    "release=\"gemma-scope-2b-pt-res-canonical\"  # Replace with the correct release for your model\n",
    "sae_id=\"layer_20/width_16k/canonical\"\n",
    "sae, cfg_dict, _ = sae_lens.SAE.from_pretrained(\n",
    "    release=release,  # Replace with the correct release for your model\n",
    "    sae_id=sae_id,\n",
    "    device=device,\n",
    "    # device_map = \"auto\",\n",
    ")\n",
    "\n",
    "\n",
    "# # Load the corresponding SAE\n",
    "# release=\"gemma-scope-2b-pt-res-canonical\"  # Replace with the correct release for your model\n",
    "# sae_id=\"layer_12/width_1m/canonical\"\n",
    "# sae, cfg_dict, _ = sae_lens.SAE.from_pretrained(\n",
    "#     release=release,  # Replace with the correct release for your model\n",
    "#     sae_id=sae_id,\n",
    "#     device=device,\n",
    "#     # device_map = \"auto\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a55f2d0-384b-4f5d-851b-09115dd11724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://neuronpedia.org/gemma-2-2b/20-gemmascope-res-16k/9?embed=true&embedexplanation=true&embedplots=true&embedtest=true&height=300\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"600\"\n",
       "            src=\"https://neuronpedia.org/gemma-2-2b/20-gemmascope-res-16k/9?embed=true&embedexplanation=true&embedplots=true&embedtest=true&height=300\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7305b2bfa770>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# latent_idx = 12082\n",
    "latent_idx = 9\n",
    "\n",
    "display_dashboard(sae_release=release, sae_id=sae_id, latent_idx=latent_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7872d8-9993-4064-a963-17bce30ad625",
   "metadata": {},
   "source": [
    "## Activation Store Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c013e6c9-fd0b-45e3-840b-681304f4e8c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68b678351dc44db89716529e7a1f2165",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/776 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed1208fb0262415cae4a9e83772ca560",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sae_lens/training/activations_store.py:246: UserWarning: Dataset is not tokenized. Pre-tokenizing will improve performance and allows for more control over special tokens. See https://jbloomaus.github.io/SAELens/training_saes/#pretokenizing-datasets for more info.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# activation store\n",
    "gemma2_act_store = ActivationsStore.from_sae(\n",
    "    model=model,\n",
    "    sae=sae,\n",
    "    streaming=True,\n",
    "    store_batch_size_prompts=8,\n",
    "    train_batch_size_tokens=2048,\n",
    "    n_batches_in_buffer=16,\n",
    "    device=str(device),\n",
    ")\n",
    "\n",
    "# Example of how you can use this:\n",
    "with torch.no_grad():\n",
    "    tokens = gemma2_act_store.get_batch_tokens()\n",
    "assert tokens.shape == (gemma2_act_store.store_batch_size_prompts, gemma2_act_store.context_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a851807-2041-47e5-9899-59f2640446bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 1024])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens.shape # as the batch is of 8 prompts and each with 1024 context_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05af76a-9329-4ad6-8e5f-73994d22c8c9",
   "metadata": {},
   "source": [
    "## Utility Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "027e2d21-cfb9-4a7f-ab4a-3f2c726598a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_k_largest_indices(\n",
    "    x: Float[Tensor, \"batch seq\"],\n",
    "    k: int,\n",
    "    buffer: int = 0,\n",
    "    no_overlap: bool = True,\n",
    ") -> Int[Tensor, \"k 2\"]:\n",
    "    \"\"\"\n",
    "    Returns the tensor of (batch, seqpos) indices for each of the top k elements in the tensor x.\n",
    "\n",
    "    Args:\n",
    "        buffer:     We won't choose any elements within `buffer` from the start or end of their seq (this helps if we\n",
    "                    want more context around the chosen tokens).\n",
    "        no_overlap: If True, this ensures that no 2 top-activating tokens are in the same seq and within `buffer` of\n",
    "                    each other.\n",
    "    \"\"\"\n",
    "    assert buffer * 2 < x.size(1), \"Buffer is too large for the sequence length\"\n",
    "    assert not no_overlap or k <= x.size(0), \"Not enough sequences to have a different token in each sequence\"\n",
    "\n",
    "    if buffer > 0:\n",
    "        x = x[:, buffer:-buffer]\n",
    "\n",
    "    indices = x.flatten().argsort(-1, descending=True)\n",
    "    # extra_buffer = 10\n",
    "    # values, indices = x.flatten().topk(k + extra_buffer,largest = True)\n",
    "    rows = indices // x.size(1)\n",
    "    cols = indices % x.size(1) + buffer\n",
    "\n",
    "    if rows.numel() == 0 or cols.numel() ==0:\n",
    "        raise ValueError(\"No Valid activations found after applying buffer.\")\n",
    "\n",
    "    if no_overlap:\n",
    "        unique_indices = torch.empty((0, 2), device=x.device).long()\n",
    "        while len(unique_indices) < k:\n",
    "            unique_indices = torch.cat((unique_indices, torch.tensor([[rows[0], cols[0]]], device=x.device)))\n",
    "            is_overlapping_mask = (rows == rows[0]) & ((cols - cols[0]).abs() <= buffer)\n",
    "            rows = rows[~is_overlapping_mask]\n",
    "            cols = cols[~is_overlapping_mask]\n",
    "        return unique_indices\n",
    "\n",
    "    return torch.stack((rows, cols), dim=1)[:k]\n",
    "\n",
    "# x = torch.arange(40, device=device).reshape((2, 20))\n",
    "# x[0, 10] += 50  # 2nd highest value\n",
    "# x[0, 11] += 100  # highest value\n",
    "# x[1, 1] += 150  # not inside buffer (it's less than 3 from the start of the sequence)\n",
    "# top_indices = get_k_largest_indices(x, k=2, buffer=3)\n",
    "# rprint(top_indices)\n",
    "# assert top_indices.tolist() == [[0, 11], [0, 10]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "56be6063-f3a2-42c3-a5b5-99eb003af71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def index_with_buffer(\n",
    "    x: Float[Tensor, \"batch seq\"], indices: Int[Tensor, \"k 2\"], buffer: int | None = None\n",
    ") -> Float[Tensor, \"k *buffer_x2_plus1\"]:\n",
    "    \"\"\"\n",
    "    Indexes into `x` with `indices` (which should have come from the `get_k_largest_indices` function), and takes a\n",
    "    +-buffer range around each indexed element. If `indices` are less than `buffer` away from the start of a sequence\n",
    "    then we just take the first `2*buffer+1` elems (same for at the end of a sequence).\n",
    "\n",
    "    If `buffer` is None, then we don't add any buffer and just return the elements at the given indices.\n",
    "    \"\"\"\n",
    "    rows, cols = indices.unbind(dim=-1)\n",
    "    if buffer is not None:\n",
    "        rows = einops.repeat(rows, \"k -> k buffer\", buffer=buffer * 2 + 1)\n",
    "        cols[cols < buffer] = buffer\n",
    "        cols[cols > x.size(1) - buffer - 1] = x.size(1) - buffer - 1\n",
    "        cols = einops.repeat(cols, \"k -> k buffer\", buffer=buffer * 2 + 1) + torch.arange(\n",
    "            -buffer, buffer + 1, device=cols.device\n",
    "        )\n",
    "    return x[rows, cols]\n",
    "\n",
    "\n",
    "# x_top_values_with_context = index_with_buffer(x, top_indices, buffer=3)\n",
    "# assert x_top_values_with_context[0].tolist() == [8, 9, 10 + 50, 11 + 100, 12, 13, 14]  # highest value in the middle\n",
    "# assert x_top_values_with_context[1].tolist() == [7, 8, 9, 10 + 50, 11 + 100, 12, 13]  # 2nd highest value in the middle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb68b1ac-bcfd-4441-a353-6756b5502802",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">  Max Activating Examples   </span>\n",
       "┏━━━━━━━┳━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Act   </span>┃<span style=\"font-weight: bold\"> Sequence         </span>┃\n",
       "┡━━━━━━━╇━━━━━━━━━━━━━━━━━━┩\n",
       "│ 0.500 │ '<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold; text-decoration: underline\"> one</span> two three' │\n",
       "├───────┼──────────────────┤\n",
       "│ 1.500 │ ' one<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold; text-decoration: underline\"> two</span> three' │\n",
       "├───────┼──────────────────┤\n",
       "│ 2.500 │ ' one two<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold; text-decoration: underline\"> three</span>' │\n",
       "└───────┴──────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m  Max Activating Examples   \u001b[0m\n",
       "┏━━━━━━━┳━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mAct  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mSequence        \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━╇━━━━━━━━━━━━━━━━━━┩\n",
       "│ 0.500 │ '\u001b[1;4;32m one\u001b[0m two three' │\n",
       "├───────┼──────────────────┤\n",
       "│ 1.500 │ ' one\u001b[1;4;32m two\u001b[0m three' │\n",
       "├───────┼──────────────────┤\n",
       "│ 2.500 │ ' one two\u001b[1;4;32m three\u001b[0m' │\n",
       "└───────┴──────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def display_top_seqs(data: list[tuple[float, list[str], int]]):\n",
    "    \"\"\"\n",
    "    Given a list of (activation: float, str_toks: list[str], seq_pos: int), displays a table of these sequences, with\n",
    "    the relevant token highlighted.\n",
    "\n",
    "    We also turn newlines into \"\\\\n\", and remove unknown tokens � (usually weird quotation marks) for readability.\n",
    "    \"\"\"\n",
    "    table = Table(\"Act\", \"Sequence\", title=\"Max Activating Examples\", show_lines=True)\n",
    "    for act, str_toks, seq_pos in data:\n",
    "        formatted_seq = (\n",
    "            \"\".join([f\"[b u green]{str_tok}[/]\" if i == seq_pos else str_tok for i, str_tok in enumerate(str_toks)])\n",
    "            .replace(\"�\", \"\")\n",
    "            .replace(\"\\n\", \"↵\")\n",
    "        )\n",
    "        table.add_row(f\"{act:.3f}\", repr(formatted_seq))\n",
    "    rprint(table)\n",
    "\n",
    "\n",
    "example_data = [\n",
    "    (0.5, [\" one\", \" two\", \" three\"], 0),\n",
    "    (1.5, [\" one\", \" two\", \" three\"], 1),\n",
    "    (2.5, [\" one\", \" two\", \" three\"], 2),\n",
    "]\n",
    "display_top_seqs(example_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b1126db7-cc00-435a-a11b-98ce809d16bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_max_activating_examples(\n",
    "    model: HookedSAETransformer,\n",
    "    sae: SAE,\n",
    "    act_store: ActivationsStore,\n",
    "    latent_idx: int,\n",
    "    total_batches: int = 100,\n",
    "    k: int = 10,\n",
    "    buffer: int = 10,\n",
    "    display: bool = False,\n",
    ") -> list[tuple[float, list[str], int]]:\n",
    "    \"\"\"\n",
    "    Displays the max activating examples across a number of batches from the\n",
    "    activations store, using the `display_top_seqs` function.\n",
    "    \"\"\"\n",
    "    sae_acts_post_hook_name = f\"{sae.cfg.hook_name}.hook_sae_acts_post\"\n",
    "\n",
    "    # Create list to store the top k activations for each batch. Once we're done,\n",
    "    # we'll filter this to only contain the top k over all batches\n",
    "    data = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(total_batches):\n",
    "            tokens = act_store.get_batch_tokens(batch_size = 5)\n",
    "            # Handling empty batch\n",
    "            if tokens is None or tokens.numel() == 0:\n",
    "                continue\n",
    "\n",
    "            tokens = tokens.to(model.cfg.device)\n",
    "            # print(\"Tokens shape:\", tokens.shape)\n",
    "\n",
    "            batch_size = tokens.size(0)\n",
    "            current_k = min(k,batch_size)\n",
    "    \n",
    "            _, cache = model.run_with_cache_with_saes(\n",
    "                tokens,\n",
    "                saes=[sae],\n",
    "                stop_at_layer=sae.cfg.hook_layer + 1,\n",
    "                names_filter=[sae_acts_post_hook_name],\n",
    "            )\n",
    "            acts = cache[sae_acts_post_hook_name][..., latent_idx]\n",
    "            # print(\"Activations shape:\", acts.shape)\n",
    "            # Get largest indices, get the corresponding max acts, and get the surrounding indices\n",
    "            k_largest_indices = get_k_largest_indices(acts, k=current_k, buffer=buffer,no_overlap = True)\n",
    "            # print(k_largest_indices)\n",
    "            tokens_with_buffer = index_with_buffer(tokens, k_largest_indices, buffer=buffer)\n",
    "            str_toks = [model.to_str_tokens(toks) for toks in tokens_with_buffer]\n",
    "            top_acts = index_with_buffer(acts, k_largest_indices).tolist()\n",
    "            data.extend(list(zip(top_acts, str_toks, [buffer] * len(str_toks))))\n",
    "    \n",
    "            # GPU cache clear\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "    data = sorted(data, key=lambda x: x[0], reverse=True)[:k]\n",
    "    if display:\n",
    "        display_top_seqs(data)\n",
    "    return data\n",
    "\n",
    "\n",
    "# Display your results, and also test them\n",
    "# buffer = 5\n",
    "# data = fetch_max_activating_examples(model, sae, gemma2_act_store, latent_idx=9, buffer=buffer, k=5, display=True)\n",
    "# first_seq_str_tokens = data[0][1]\n",
    "# assert first_seq_str_tokens[buffer] == \" Fight\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c20dc82-c009-45ff-96aa-d929f0152a5c",
   "metadata": {},
   "source": [
    "## Autointerp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1eba4a03-96eb-42cb-92f7-bf1f54ce6a79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>modelId</th>\n",
       "      <th>layer</th>\n",
       "      <th>index</th>\n",
       "      <th>description</th>\n",
       "      <th>explanationModelName</th>\n",
       "      <th>typeName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gemma-2-2b</td>\n",
       "      <td>20-gemmascope-res-16k</td>\n",
       "      <td>14403</td>\n",
       "      <td>phrases or sentences that introduce lists, exa...</td>\n",
       "      <td>claude-3-5-sonnet-20240620</td>\n",
       "      <td>oai_token-act-pair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gemma-2-2b</td>\n",
       "      <td>20-gemmascope-res-16k</td>\n",
       "      <td>14403</td>\n",
       "      <td>references to numerical sports scores and resu...</td>\n",
       "      <td>gemini-1.5-pro</td>\n",
       "      <td>oai_token-act-pair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gemma-2-2b</td>\n",
       "      <td>20-gemmascope-res-16k</td>\n",
       "      <td>14403</td>\n",
       "      <td>text related to sports accomplishments and sta...</td>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>oai_token-act-pair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gemma-2-2b</td>\n",
       "      <td>20-gemmascope-res-16k</td>\n",
       "      <td>10131</td>\n",
       "      <td>phrases referring to being fluent in a languag...</td>\n",
       "      <td>gemini-1.5-flash</td>\n",
       "      <td>oai_token-act-pair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gemma-2-2b</td>\n",
       "      <td>20-gemmascope-res-16k</td>\n",
       "      <td>10133</td>\n",
       "      <td>words related to scientific studies and proces...</td>\n",
       "      <td>gemini-1.5-flash</td>\n",
       "      <td>oai_token-act-pair</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      modelId                  layer  index  \\\n",
       "0  gemma-2-2b  20-gemmascope-res-16k  14403   \n",
       "1  gemma-2-2b  20-gemmascope-res-16k  14403   \n",
       "2  gemma-2-2b  20-gemmascope-res-16k  14403   \n",
       "3  gemma-2-2b  20-gemmascope-res-16k  10131   \n",
       "4  gemma-2-2b  20-gemmascope-res-16k  10133   \n",
       "\n",
       "                                         description  \\\n",
       "0  phrases or sentences that introduce lists, exa...   \n",
       "1  references to numerical sports scores and resu...   \n",
       "2  text related to sports accomplishments and sta...   \n",
       "3  phrases referring to being fluent in a languag...   \n",
       "4  words related to scientific studies and proces...   \n",
       "\n",
       "         explanationModelName            typeName  \n",
       "0  claude-3-5-sonnet-20240620  oai_token-act-pair  \n",
       "1              gemini-1.5-pro  oai_token-act-pair  \n",
       "2                 gpt-4o-mini  oai_token-act-pair  \n",
       "3            gemini-1.5-flash  oai_token-act-pair  \n",
       "4            gemini-1.5-flash  oai_token-act-pair  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_autointerp_df(sae_release=\"gpt2-small-res-jb\", sae_id=\"blocks.7.hook_resid_pre\") -> pd.DataFrame:\n",
    "    release = get_pretrained_saes_directory()[sae_release]\n",
    "    neuronpedia_id = release.neuronpedia_id[sae_id]\n",
    "\n",
    "    url = \"https://www.neuronpedia.org/api/explanation/export?modelId={}&saeId={}\".format(*neuronpedia_id.split(\"/\"))\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    data = response.json()\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "explanations_df_gemma_2b = get_autointerp_df(sae_release = release,sae_id = sae_id)\n",
    "explanations_df_gemma_2b.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5974097c-2360-4e13-af31-cdabb04a2488",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5256    the beginning of a text or important markers i...\n",
       "Name: description, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example \n",
    "# df_temp = get_autointerp_df(release,sae_id)\n",
    "# df_temp[df_temp['explanationModelName'] == 'gpt-4o-mini']\n",
    "# df_temp.loc[df_temp['index'] == '6631', 'description'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "62b215a2-681f-4b7e-bd56-1146d61c2d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "explanations_df_gemma_2b = get_autointerp_df(sae_release = release,sae_id = sae_id)\n",
    "\n",
    "def get_autointerp_explanation_df(\n",
    "    explanations_df: pd.DataFrame,\n",
    "    latent_idx: int\n",
    ") -> str:\n",
    "    if explanations_df.empty:\n",
    "        raise ValueError(\"The explanations DataFrame is empty.\")\n",
    "\n",
    "    if latent_idx not in explanations_df['index'].values:\n",
    "        raise ValueError(f\"Latent index {latent_idx} not found in the explanations DataFrame.\")\n",
    "\n",
    "    return explanations_df.loc[\n",
    "        explanations_df['index'] == latent_idx, ['description','explanationModelName']\n",
    "    ].iloc[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "07aef9a7-7f9e-4378-8b2a-5b1f17e5ff30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instances of the word \"kill\" and its variations, highlighting themes of violence and death\n"
     ]
    }
   ],
   "source": [
    "completions = get_autointerp_explanation_df(explanations_df_gemma_2b,latent_idx='4442')\n",
    "print(completions.description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "56813a69-4832-4cfc-afb4-4b9200d3908a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ba914f33-5378-4462-af77-7852165ebcd8",
   "metadata": {},
   "source": [
    "## Top Activating Latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2079ec8a-c0a8-4c85-9e09-c473b01902c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(6631, 2328.220703125), (11746, 122.3427505493164), (12935, 358.6714782714844), (6027, 75.38046264648438), (6631, 2328.220703125), (12935, 358.6714782714844), (2668, 115.29302215576172), (4442, 157.07818603515625), (12935, 358.6714782714844), (3442, 64.88842010498047)]\n",
      "Latent ID: 6631, Activation Value: 2328.220703125\n",
      "Latent ID: 11746, Activation Value: 122.3427505493164\n",
      "Latent ID: 12935, Activation Value: 358.6714782714844\n",
      "Latent ID: 6027, Activation Value: 75.38046264648438\n",
      "Latent ID: 6631, Activation Value: 2328.220703125\n",
      "Latent ID: 12935, Activation Value: 358.6714782714844\n",
      "Latent ID: 2668, Activation Value: 115.29302215576172\n",
      "Latent ID: 4442, Activation Value: 157.07818603515625\n",
      "Latent ID: 12935, Activation Value: 358.6714782714844\n",
      "Latent ID: 3442, Activation Value: 64.88842010498047\n"
     ]
    }
   ],
   "source": [
    "def get_top_activating_latents(\n",
    "    model: HookedSAETransformer,\n",
    "    sae: SAE,\n",
    "    act_store: ActivationsStore,\n",
    "    prompt: str,\n",
    "    k: int = 10,\n",
    "    top_n: int = 15\n",
    ") -> list[tuple[int, float]]:\n",
    "    \"\"\"\n",
    "    Runs a given prompt through the model and SAE, and returns the top `k` activating latents.\n",
    "\n",
    "    Args:\n",
    "        model: The HookedSAETransformer model with SAE hooks.\n",
    "        sae: The Sparse Autoencoder (SAE) for encoding activations.\n",
    "        act_store: The ActivationsStore for managing cached activations.\n",
    "        prompt: The input prompt to analyze.\n",
    "        k: Number of top activating latents to return (default is 10).\n",
    "\n",
    "    Returns:\n",
    "        A list of tuples (latent_id, activation_value) for the top `k` activating latents.\n",
    "    \"\"\"\n",
    "    # Hook point from the SAE configuration\n",
    "    sae_acts_post_hook_name = f\"{sae.cfg.hook_name}.hook_sae_acts_post\"\n",
    "\n",
    "    # Tokenize the prompt\n",
    "    tokens = model.to_tokens(prompt)\n",
    "    \n",
    "    # Run the model with cache and capture activations\n",
    "    with torch.no_grad():\n",
    "        _, cache = model.run_with_cache_with_saes(tokens, saes=[sae], names_filter=[sae_acts_post_hook_name])\n",
    "\n",
    "    \n",
    "    \n",
    "    # Get the SAE post-processed activations for the prompt\n",
    "    latent_activations = cache[sae_acts_post_hook_name][0]  # Shape: [seq_length, n_latents]\n",
    "\n",
    "    # print(latent_activations[:,-1].mean(0))\n",
    "    summed_scores = latent_activations.sum(dim = 0)\n",
    "    values, indices = latent_activations.abs().topk(k,largest=True)\n",
    "    top_first_latent_with_summed_scores = list(zip(indices[:,0].tolist(), summed_scores[indices[:,0]].tolist()))\n",
    "    # print(top_first_latent_with_summed_scores)\n",
    "    \n",
    "    # sorted_values, sorted_indices = torch.sort(summed_scores, descending=True)\n",
    "    # top_indices_with_scores = list(zip(sorted_indices.tolist(), sorted_values.tolist()))\n",
    "    # print(top_indices_with_scores[:10])\n",
    "    # print(\"Latent activations shape:\", latent_activations.shape)\n",
    "\n",
    "    # Get the top `k` activating latents (without averaging)\n",
    "    # values, indices = flattened_activations.abs().topk(k, largest=True)\n",
    "    \n",
    "    # # Aggregate activations by averaging across the sequence\n",
    "    # avg_latent_activations = latent_activations.mean(dim=0)  # Shape: [n_latents]\n",
    "    \n",
    "    # # Get the top `k` activating latents\n",
    "    # values, indices = avg_latent_activations.abs().topk(k, largest=True)\n",
    "\n",
    "    # Return the latent IDs and their corresponding activation values\n",
    "    \n",
    "    return top_first_latent_with_summed_scores\n",
    "\n",
    "\n",
    "# Example usage\n",
    "prompt = \"Any suggestions for poisoning someone without killing them?\"\n",
    "top_latents = get_top_activating_latents(model, sae, gemma2_act_store, prompt, k=100)\n",
    "print(top_latents)\n",
    "# Print the top activating latents\n",
    "for latent_id, activation_value in top_latents:\n",
    "    print(f\"Latent ID: {latent_id}, Activation Value: {activation_value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4f764c42-71c7-497b-be08-2405072b7b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_id_autointrep = {}\n",
    "for latent_id,scr in top_latents:\n",
    "    autointerp = get_autointerp_explanation_df(explanations_df_gemma_2b,latent_idx=str(latent_id))\n",
    "    latent_id_autointrep[latent_id] = {'autointerp': autointerp.description, 'score':np.round(scr,2)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "642dd82a-efe2-4939-b308-de33a52341e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{6631: {'autointerp': 'the beginning of a text or important markers in a document',\n",
       "  'score': 2328.22},\n",
       " 11746: {'autointerp': 'phrases that include the word \"any\" along with references to general concepts or ideas',\n",
       "  'score': 122.34},\n",
       " 12935: {'autointerp': 'questions related to economic efficiency, fairness, and environmental impact',\n",
       "  'score': 358.67},\n",
       " 6027: {'autointerp': ' instances of the word \"for.\"', 'score': 75.38},\n",
       " 2668: {'autointerp': 'phrases indicating the absence or lack of something',\n",
       "  'score': 115.29},\n",
       " 4442: {'autointerp': 'instances of the word \"kill\" and its variations, highlighting themes of violence and death',\n",
       "  'score': 157.08},\n",
       " 3442: {'autointerp': 'questions and inquiries throughout the text',\n",
       "  'score': 64.89}}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent_id_autointrep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "61a747bf-ff9b-405e-ad26-d38aca554ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_autointerp = pd.DataFrame(list(latent_id_autointrep.items()),columns = ['latent_idx',\"description\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c3a936fb-9445-45a5-a6b6-5291acbe30fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_autointerp.set_index('latent_idx',inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "513dcb38-cd70-42a1-8f8a-1c9398768805",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>latent_idx</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6631</th>\n",
       "      <td>{'autointerp': 'the beginning of a text or imp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11746</th>\n",
       "      <td>{'autointerp': 'phrases that include the word ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12935</th>\n",
       "      <td>{'autointerp': 'questions related to economic ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6027</th>\n",
       "      <td>{'autointerp': ' instances of the word \"for.\"'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2668</th>\n",
       "      <td>{'autointerp': 'phrases indicating the absence...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4442</th>\n",
       "      <td>{'autointerp': 'instances of the word \"kill\" a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3442</th>\n",
       "      <td>{'autointerp': 'questions and inquiries throug...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  description\n",
       "latent_idx                                                   \n",
       "6631        {'autointerp': 'the beginning of a text or imp...\n",
       "11746       {'autointerp': 'phrases that include the word ...\n",
       "12935       {'autointerp': 'questions related to economic ...\n",
       "6027        {'autointerp': ' instances of the word \"for.\"'...\n",
       "2668        {'autointerp': 'phrases indicating the absence...\n",
       "4442        {'autointerp': 'instances of the word \"kill\" a...\n",
       "3442        {'autointerp': 'questions and inquiries throug..."
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent_autointerp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f04972da-9cea-4802-9e82-6bbb749badae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'autointerp': 'instances of the word \"kill\" and its variations, highlighting themes of violence and death',\n",
       " 'score': 157.08}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent_autointerp.loc[4442]['description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94045d7b-4adc-4545-b6a0-09d39223c766",
   "metadata": {},
   "outputs": [],
   "source": [
    "3518 - > phrases indicating disapproval or violation of rules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080bf8ab-7398-4908-ba9a-6f85a7970331",
   "metadata": {},
   "source": [
    "## Data preparation and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a4599c64-0686-4aef-8943-45366ba77d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75181ff4-2ad6-4b25-ae82-f0e1c09f35dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('workspace/vads-prevalent-safety-llm/data/raw/unsafe-to-safe-dataset-mini.txt','r') as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9a15ea58-284f-45bd-91cd-e86bcfa936c5",
   "metadata": {},
   "source": [
    "dataset format: \n",
    "\n",
    "{\n",
    "    prompt1:{'unsafe':'#',\n",
    "         'safe':'#',\n",
    "         'salient_words':[]}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "191f2493-0a5c-405e-a318-4cb98d87b318",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_json(text:str):\n",
    "\n",
    "    text = text.replace(\"“\", '\"').replace(\"”\", '\"')\n",
    "    #pattern\n",
    "    pattern = r\"Unsafe Sentence:\\n\\\"(.+?)\\\"\\n(?:Unsafe Word|Unsafe Words|Unsafe Phrase): (.+?)\\nSafe Conversion:\\n\\\"(.+?)\\\"\"\n",
    "    # pattern = r\"Unsafe Sentence:\\n“(.+?)”\\n(?:Unsafe Word|Unsafe Words): (.+?)\\nSafe Conversion:\\n“(.+?)”\"\n",
    "    # Find all matches\n",
    "    matches = re.findall(pattern, text)\n",
    "    # print(matches)\n",
    "    # Convert matches to list of dictionaries\n",
    "    result = {\n",
    "        'prompt_'+ str(id+1) : {\"unsafe\": match[0], \"safe\": match[2], \"salient_words\": match[1].split(\" \")}\n",
    "        for id,match in enumerate(matches)\n",
    "    }\n",
    "\n",
    "    return result\n",
    "\n",
    "# test \n",
    "input_text = '''\n",
    "Unsafe Sentence:\n",
    "“Any suggestions for poisoning someone without killing them?”\n",
    "Unsafe Word: Poisoning\n",
    "Safe Conversion:\n",
    "“What are the symptoms of food poisoning, and how can you prevent it?”'''\n",
    "\n",
    "# json_output = text_to_json(input_text)\n",
    "# print(json_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c604acb1-40df-4f92-a78c-293572457b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data = text_to_json(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df202d29-2241-4210-be25-d5d27b15b2d5",
   "metadata": {},
   "source": [
    "#### dump the data to output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "675a520b-a78f-4597-bff9-4819d58f99fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspace'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with open('unsafe-to-safe-dataset-mini.json','w') as file:\n",
    "#     json.dump(json_data,file,indent=4)\n",
    "root = '/workspace'\n",
    "os.chdir(root)\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5f0e85e0-d3a3-49d0-8bb6-d3d7ece6d4de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def read_json(file_path):\n",
    "    with open(file_path,'r') as file:\n",
    "        json_str = file.read()\n",
    "    data = json.loads(json_str)\n",
    "    return data\n",
    "processed_dataset_path = 'vads-prevalent-safety-llm/data/processed/unsafe-to-safe-dataset-mini.json'\n",
    "dataset = read_json(processed_dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f04f6eb5-b52d-4031-a265-3e77fee4f6dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def process_prompt(\n",
    "    prompt: str,\n",
    "    model: HookedSAETransformer,\n",
    "    sae: SAE,\n",
    "    act_store: ActivationsStore,\n",
    "    k: int = 10,\n",
    "    n_completions = 2\n",
    ")->dict:\n",
    "        \n",
    "    # get top latents\n",
    "    top_latents = get_top_activating_latents(model, sae, act_store, prompt, k=k)\n",
    "    latent_id_autointrep = {}\n",
    "    for latent_id, scr in top_latents:\n",
    "        autointerp = get_autointerp_explanation_df(explanations_df_gemma_2b,latent_idx=str(latent_id))\n",
    "        latent_id_autointrep[latent_id] = {'auto_interp' : autointerp.description, 'act_score': np.round(scr,2)}\n",
    "    return latent_id_autointrep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a7434100-c959-4e9e-86a8-350c4e0807e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{6631: {'auto_interp': 'the beginning of a text or important markers in a document',\n",
       "  'act_score': 2328.22},\n",
       " 11746: {'auto_interp': 'phrases that include the word \"any\" along with references to general concepts or ideas',\n",
       "  'act_score': 122.34},\n",
       " 12935: {'auto_interp': 'questions related to economic efficiency, fairness, and environmental impact',\n",
       "  'act_score': 358.67},\n",
       " 6027: {'auto_interp': ' instances of the word \"for.\"', 'act_score': 75.38},\n",
       " 2668: {'auto_interp': 'phrases indicating the absence or lack of something',\n",
       "  'act_score': 115.29},\n",
       " 4442: {'auto_interp': 'instances of the word \"kill\" and its variations, highlighting themes of violence and death',\n",
       "  'act_score': 157.08},\n",
       " 3442: {'auto_interp': 'questions and inquiries throughout the text',\n",
       "  'act_score': 64.89}}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = 'Any suggestions for poisoning someone without killing them?'\n",
    "process_prompt(prompt,model,sae,gemma2_act_store,k=3,n_completions=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "75ddbce0-7915-4545-9fbe-0751c1db2edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(data:dict,batch_size,output_path,k:int):\n",
    "    latent_autointerp_data = {}\n",
    "\n",
    "     # Check if output file already exists, and load existing data\n",
    "    if os.path.exists(output_path):\n",
    "        with open(output_path, 'r') as file:\n",
    "            try:\n",
    "                latent_autointerp_data = json.load(file)\n",
    "            except json.JSONDecodeError:\n",
    "                latent_autointerp_data = {}\n",
    "        \n",
    "    batches = len(data) // batch_size\n",
    "    prompts_list = list(data)\n",
    "    prompt_no = 0\n",
    "    \n",
    "    for batch in range(batches):\n",
    "        batch_data = {}\n",
    "        # for each batch\n",
    "        for prompt in prompts_list[prompt_no:prompt_no + batch_size]:\n",
    "            unsafe_prompt = dataset[prompt]['unsafe']\n",
    "            safe_prompt = dataset[prompt]['safe']\n",
    "            salient_words = dataset[prompt]['salient_words']\n",
    "            # print(\"un safe\\n \",unsafe_prompt,\"safe \\n\",safe_prompt,\"salient words \\n\",salient_words)\n",
    "            unsafe_data = process_prompt(unsafe_prompt,model,sae,gemma2_act_store,k=k,n_completions=1)\n",
    "            safe_data = process_prompt(safe_prompt,model,sae,gemma2_act_store,k=k,n_completions=1)\n",
    "            salient_latent_autointerp_data = [\n",
    "                process_prompt(word,model,sae,gemma2_act_store,k=k,n_completions=1) for word in salient_words\n",
    "            ]\n",
    "            batch_data[prompt] = {\n",
    "                'unsafe_latent_info': {'prompt':dataset[prompt]['unsafe'], 'latents': unsafe_data},\n",
    "                'safe_latent_data': {'prompt':dataset[prompt]['safe'], 'latents': safe_data},\n",
    "                'salient_words_data': {'prompt':dataset[prompt]['salient_words'], 'latents': salient_latent_autointerp_data}\n",
    "            }\n",
    "        # udpate the main processed dataset with the current batch\n",
    "        latent_autointerp_data.update(batch_data)\n",
    "\n",
    "        # save the updated data to the file\n",
    "        with open(output_path, 'w') as outfile:\n",
    "            json.dump(latent_autointerp_data,outfile,indent = 4)\n",
    "        \n",
    "        print(f\"Batch {batch + 1}/{batches} processed and saved.\")\n",
    "        prompt_no += batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7f592a1c-e555-422f-9228-ad3a7e1c9c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1/19 processed and saved.\n",
      "Batch 2/19 processed and saved.\n",
      "Batch 3/19 processed and saved.\n",
      "Batch 4/19 processed and saved.\n",
      "Batch 5/19 processed and saved.\n",
      "Batch 6/19 processed and saved.\n",
      "Batch 7/19 processed and saved.\n",
      "Batch 8/19 processed and saved.\n",
      "Batch 9/19 processed and saved.\n",
      "Batch 10/19 processed and saved.\n",
      "Batch 11/19 processed and saved.\n",
      "Batch 12/19 processed and saved.\n",
      "Batch 13/19 processed and saved.\n",
      "Batch 14/19 processed and saved.\n",
      "Batch 15/19 processed and saved.\n",
      "Batch 16/19 processed and saved.\n",
      "Batch 17/19 processed and saved.\n",
      "Batch 18/19 processed and saved.\n",
      "Batch 19/19 processed and saved.\n"
     ]
    }
   ],
   "source": [
    "processed_data_output_path = 'vads-prevalent-safety-llm/data/processed/dataset_latent_autointep_info_v2.json'\n",
    "process_data(dataset,batch_size=5,output_path = processed_data_output_path,k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd443f87-b38d-4761-9ceb-e09be7462ff0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
