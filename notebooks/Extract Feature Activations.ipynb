{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "692a4ec9-b403-4710-9d44-c55d07735e25",
   "metadata": {},
   "source": [
    "### install necessary dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22fcce7e-8b7b-4ca8-9c84-8db7e37ee7a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sae-lens\n",
      "  Downloading sae_lens-5.3.3-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting transformer-lens\n",
      "  Downloading transformer_lens-2.11.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting sae-dashboard\n",
      "  Downloading sae_dashboard-0.6.4-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting tabulate\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Collecting openai\n",
      "  Downloading openai-1.60.2-py3-none-any.whl.metadata (27 kB)\n",
      "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.10/dist-packages (8.1.1)\n",
      "Collecting huggingface_hub[cli]\n",
      "  Downloading huggingface_hub-0.27.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting automated-interpretability<1.0.0,>=0.0.5 (from sae-lens)\n",
      "  Downloading automated_interpretability-0.0.6-py3-none-any.whl.metadata (778 bytes)\n",
      "Collecting babe<0.0.8,>=0.0.7 (from sae-lens)\n",
      "  Downloading babe-0.0.7-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting datasets<3.0.0,>=2.17.1 (from sae-lens)\n",
      "  Downloading datasets-2.21.0-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting matplotlib<4.0.0,>=3.8.3 (from sae-lens)\n",
      "  Downloading matplotlib-3.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: matplotlib-inline<0.2.0,>=0.1.6 in /usr/local/lib/python3.10/dist-packages (from sae-lens) (0.1.6)\n",
      "Collecting nltk<4.0.0,>=3.8.1 (from sae-lens)\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting plotly<6.0.0,>=5.19.0 (from sae-lens)\n",
      "  Downloading plotly-5.24.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting plotly-express<0.5.0,>=0.4.1 (from sae-lens)\n",
      "  Downloading plotly_express-0.4.1-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting pytest-profiling<2.0.0,>=1.7.0 (from sae-lens)\n",
      "  Downloading pytest_profiling-1.8.1-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting python-dotenv<2.0.0,>=1.0.1 (from sae-lens)\n",
      "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from sae-lens) (6.0.1)\n",
      "Collecting pyzmq==26.0.0 (from sae-lens)\n",
      "  Downloading pyzmq-26.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (6.2 kB)\n",
      "Collecting safetensors<0.5.0,>=0.4.2 (from sae-lens)\n",
      "  Downloading safetensors-0.4.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting simple-parsing<0.2.0,>=0.1.6 (from sae-lens)\n",
      "  Downloading simple_parsing-0.1.7-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting transformers<5.0.0,>=4.38.1 (from sae-lens)\n",
      "  Downloading transformers-4.48.1-py3-none-any.whl.metadata (44 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting typer<0.13.0,>=0.12.3 (from sae-lens)\n",
      "  Downloading typer-0.12.5-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting typing-extensions<5.0.0,>=4.10.0 (from sae-lens)\n",
      "  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting zstandard<0.23.0,>=0.22.0 (from sae-lens)\n",
      "  Downloading zstandard-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.9 kB)\n",
      "Collecting accelerate>=0.23.0 (from transformer-lens)\n",
      "  Downloading accelerate-1.3.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting beartype<0.15.0,>=0.14.1 (from transformer-lens)\n",
      "  Downloading beartype-0.14.1-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting better-abc<0.0.4,>=0.0.3 (from transformer-lens)\n",
      "  Downloading better_abc-0.0.3-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting einops>=0.6.0 (from transformer-lens)\n",
      "  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting fancy-einsum>=0.0.3 (from transformer-lens)\n",
      "  Downloading fancy_einsum-0.0.3-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting jaxtyping>=0.2.11 (from transformer-lens)\n",
      "  Downloading jaxtyping-0.2.37-py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.10/dist-packages (from transformer-lens) (1.26.3)\n",
      "Collecting pandas>=1.1.5 (from transformer-lens)\n",
      "  Downloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m44.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting rich>=12.6.0 (from transformer-lens)\n",
      "  Downloading rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting sentencepiece (from transformer-lens)\n",
      "  Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: torch>=1.10 in /usr/local/lib/python3.10/dist-packages (from transformer-lens) (2.2.0)\n",
      "Collecting tqdm>=4.64.1 (from transformer-lens)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting typeguard<5.0,>=4.2 (from transformer-lens)\n",
      "  Downloading typeguard-4.4.1-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting wandb>=0.13.5 (from transformer-lens)\n",
      "  Downloading wandb-0.19.4-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting dataclasses-json<0.7.0,>=0.6.4 (from sae-dashboard)\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting eindex-callum<0.2.0,>=0.1.0 (from sae-dashboard)\n",
      "  Downloading eindex_callum-0.1.2-py3-none-any.whl.metadata (377 bytes)\n",
      "Collecting einops>=0.6.0 (from transformer-lens)\n",
      "  Downloading einops-0.7.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting isort<6.0.0,>=5.13.2 (from sae-dashboard)\n",
      "  Downloading isort-5.13.2-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting sae-lens\n",
      "  Downloading sae_lens-4.4.5-py3-none-any.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub[cli]) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub[cli]) (2024.2.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub[cli]) (23.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub[cli]) (2.31.0)\n",
      "Collecting InquirerPy==0.3.4 (from huggingface_hub[cli])\n",
      "  Downloading InquirerPy-0.3.4-py3-none-any.whl.metadata (8.1 kB)\n",
      "Collecting pfzy<0.4.0,>=0.3.1 (from InquirerPy==0.3.4->huggingface_hub[cli])\n",
      "  Downloading pfzy-0.3.4-py3-none-any.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: prompt-toolkit<4.0.0,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from InquirerPy==0.3.4->huggingface_hub[cli]) (3.0.43)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.26.0)\n",
      "Collecting jiter<1,>=0.4.0 (from openai)\n",
      "  Downloading jiter-0.8.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting pydantic<3,>=1.9.0 (from openai)\n",
      "  Downloading pydantic-2.10.6-py3-none-any.whl.metadata (30 kB)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: comm>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (0.2.1)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (8.21.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (5.14.1)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.9 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (4.0.9)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.9 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (3.0.9)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer-lens) (5.9.8)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
      "Collecting blobfile<3.0.0,>=2.1.1 (from automated-interpretability<1.0.0,>=0.0.5->sae-lens)\n",
      "  Downloading blobfile-2.1.1-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting boostedblob<0.16.0,>=0.15.3 (from automated-interpretability<1.0.0,>=0.0.5->sae-lens)\n",
      "  Downloading boostedblob-0.15.6-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from openai)\n",
      "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting orjson<4.0.0,>=3.10.1 (from automated-interpretability<1.0.0,>=0.0.5->sae-lens)\n",
      "  Downloading orjson-3.10.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (41 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.8/41.8 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting scikit-learn<2.0.0,>=1.2.0 (from automated-interpretability<1.0.0,>=0.0.5->sae-lens)\n",
      "  Downloading scikit_learn-1.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Collecting tiktoken<0.7.0,>=0.6.0 (from automated-interpretability<1.0.0,>=0.0.5->sae-lens)\n",
      "  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting py2store (from babe<0.0.8,>=0.0.7->sae-lens)\n",
      "  Downloading py2store-0.1.20.tar.gz (143 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.1/143.1 kB\u001b[0m \u001b[31m64.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting graze (from babe<0.0.8,>=0.0.7->sae-lens)\n",
      "  Downloading graze-0.1.27-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7.0,>=0.6.4->sae-dashboard)\n",
      "  Downloading marshmallow-3.26.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7.0,>=0.6.4->sae-dashboard)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting pyarrow>=15.0.0 (from datasets<3.0.0,>=2.17.1->sae-lens)\n",
      "  Downloading pyarrow-19.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets<3.0.0,>=2.17.1->sae-lens)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting requests (from huggingface_hub[cli])\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting xxhash (from datasets<3.0.0,>=2.17.1->sae-lens)\n",
      "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets<3.0.0,>=2.17.1->sae-lens)\n",
      "  Downloading multiprocess-0.70.17-py310-none-any.whl.metadata (7.2 kB)\n",
      "Collecting aiohttp (from datasets<3.0.0,>=2.17.1->sae-lens)\n",
      "  Downloading aiohttp-3.11.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (2.17.2)\n",
      "Requirement already satisfied: stack-data in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Collecting wadler-lindig>=0.1.3 (from jaxtyping>=0.2.11->transformer-lens)\n",
      "  Downloading wadler_lindig-0.1.3-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib<4.0.0,>=3.8.3->sae-lens)\n",
      "  Downloading contourpy-1.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.4 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib<4.0.0,>=3.8.3->sae-lens)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib<4.0.0,>=3.8.3->sae-lens)\n",
      "  Downloading fonttools-4.55.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (166 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.1/166.1 kB\u001b[0m \u001b[31m57.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting kiwisolver>=1.3.1 (from matplotlib<4.0.0,>=3.8.3->sae-lens)\n",
      "  Downloading kiwisolver-1.4.8-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.8.3->sae-lens) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib<4.0.0,>=3.8.3->sae-lens) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.8.3->sae-lens) (2.8.2)\n",
      "Collecting click (from nltk<4.0.0,>=3.8.1->sae-lens)\n",
      "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting joblib (from nltk<4.0.0,>=3.8.1->sae-lens)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk<4.0.0,>=3.8.1->sae-lens)\n",
      "  Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pytz>=2020.1 (from pandas>=1.1.5->transformer-lens)\n",
      "  Downloading pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas>=1.1.5->transformer-lens)\n",
      "  Downloading tzdata-2025.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting tenacity>=6.2.0 (from plotly<6.0.0,>=5.19.0->sae-lens)\n",
      "  Downloading tenacity-9.0.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting statsmodels>=0.9.0 (from plotly-express<0.5.0,>=0.4.1->sae-lens)\n",
      "  Downloading statsmodels-0.14.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.2 kB)\n",
      "Collecting scipy>=0.18 (from plotly-express<0.5.0,>=0.4.1->sae-lens)\n",
      "  Downloading scipy-1.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting patsy>=0.5 (from plotly-express<0.5.0,>=0.4.1->sae-lens)\n",
      "  Downloading patsy-1.0.1-py2.py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3,>=1.9.0->openai)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.27.2 (from pydantic<3,>=1.9.0->openai)\n",
      "  Downloading pydantic_core-2.27.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from pytest-profiling<2.0.0,>=1.7.0->sae-lens) (1.16.0)\n",
      "Collecting pytest (from pytest-profiling<2.0.0,>=1.7.0->sae-lens)\n",
      "  Downloading pytest-8.3.4-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting gprof2dot (from pytest-profiling<2.0.0,>=1.7.0->sae-lens)\n",
      "  Downloading gprof2dot-2024.6.6-py2.py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub[cli]) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub[cli]) (2.2.0)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=12.6.0->transformer-lens)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting docstring-parser<1.0,>=0.15 (from simple-parsing<0.2.0,>=0.1.6->sae-lens)\n",
      "  Downloading docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer-lens) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer-lens) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer-lens) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer-lens) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer-lens) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer-lens) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer-lens) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer-lens) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer-lens) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer-lens) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer-lens) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer-lens) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer-lens) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer-lens) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer-lens) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10->transformer-lens) (12.3.101)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers<5.0.0,>=4.38.1->sae-lens)\n",
      "  Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting shellingham>=1.3.0 (from typer<0.13.0,>=0.12.3->sae-lens)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting docker-pycreds>=0.4.0 (from wandb>=0.13.5->transformer-lens)\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb>=0.13.5->transformer-lens)\n",
      "  Downloading GitPython-3.1.44-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer-lens) (4.2.0)\n",
      "Collecting protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 (from wandb>=0.13.5->transformer-lens)\n",
      "  Downloading protobuf-5.29.3-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Collecting sentry-sdk>=2.0.0 (from wandb>=0.13.5->transformer-lens)\n",
      "  Downloading sentry_sdk-2.20.0-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Collecting setproctitle (from wandb>=0.13.5->transformer-lens)\n",
      "  Downloading setproctitle-1.3.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer-lens) (69.0.3)\n",
      "Collecting pycryptodomex~=3.8 (from blobfile<3.0.0,>=2.1.1->automated-interpretability<1.0.0,>=0.0.5->sae-lens)\n",
      "  Downloading pycryptodomex-3.21.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
      "Collecting lxml~=4.9 (from blobfile<3.0.0,>=2.1.1->automated-interpretability<1.0.0,>=0.0.5->sae-lens)\n",
      "  Downloading lxml-4.9.4-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.7 kB)\n",
      "Collecting uvloop>=0.16.0 (from boostedblob<0.16.0,>=0.15.3->automated-interpretability<1.0.0,>=0.0.5->sae-lens)\n",
      "  Downloading uvloop-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets<3.0.0,>=2.17.1->sae-lens)\n",
      "  Downloading aiohappyeyeballs-2.4.4-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets<3.0.0,>=2.17.1->sae-lens)\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting async-timeout<6.0,>=4.0 (from aiohttp->datasets<3.0.0,>=2.17.1->sae-lens)\n",
      "  Downloading async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<3.0.0,>=2.17.1->sae-lens) (23.2.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets<3.0.0,>=2.17.1->sae-lens)\n",
      "  Downloading frozenlist-1.5.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets<3.0.0,>=2.17.1->sae-lens)\n",
      "  Downloading multidict-6.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->datasets<3.0.0,>=2.17.1->sae-lens)\n",
      "  Downloading propcache-0.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.2 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->datasets<3.0.0,>=2.17.1->sae-lens)\n",
      "  Downloading yarl-1.18.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (69 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.2/69.2 kB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer-lens)\n",
      "  Downloading gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=12.6.0->transformer-lens)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit<4.0.0,>=3.0.1->InquirerPy==0.3.4->huggingface_hub[cli]) (0.2.13)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn<2.0.0,>=1.2.0->automated-interpretability<1.0.0,>=0.0.5->sae-lens)\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.4->sae-dashboard)\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting dol (from graze->babe<0.0.8,>=0.0.7->sae-lens)\n",
      "  Downloading dol-0.3.7-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10->transformer-lens) (2.1.5)\n",
      "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting multiprocess (from datasets<3.0.0,>=2.17.1->sae-lens)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Collecting config2py (from py2store->babe<0.0.8,>=0.0.7->sae-lens)\n",
      "  Downloading config2py-0.1.36-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting importlib_resources (from py2store->babe<0.0.8,>=0.0.7->sae-lens)\n",
      "  Downloading importlib_resources-6.5.2-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting iniconfig (from pytest->pytest-profiling<2.0.0,>=1.7.0->sae-lens)\n",
      "  Downloading iniconfig-2.0.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting pluggy<2,>=1.5 (from pytest->pytest-profiling<2.0.0,>=1.7.0->sae-lens)\n",
      "  Downloading pluggy-1.5.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: tomli>=1 in /usr/local/lib/python3.10/dist-packages (from pytest->pytest-profiling<2.0.0,>=1.7.0->sae-lens) (2.0.1)\n",
      "Requirement already satisfied: executing>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /usr/local/lib/python3.10/dist-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10->transformer-lens) (1.3.0)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer-lens)\n",
      "  Downloading smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting i2 (from config2py->py2store->babe<0.0.8,>=0.0.7->sae-lens)\n",
      "  Downloading i2-0.1.45-py3-none-any.whl.metadata (2.1 kB)\n",
      "Downloading pyzmq-26.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (920 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m920.0/920.0 kB\u001b[0m \u001b[31m108.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading transformer_lens-2.11.0-py3-none-any.whl (177 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.6/177.6 kB\u001b[0m \u001b[31m68.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sae_dashboard-0.6.4-py3-none-any.whl (98 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.9/98.9 kB\u001b[0m \u001b[31m52.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sae_lens-4.4.5-py3-none-any.whl (141 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m70.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading InquirerPy-0.3.4-py3-none-any.whl (67 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.7/67.7 kB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Downloading openai-1.60.2-py3-none-any.whl (456 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m456.1/456.1 kB\u001b[0m \u001b[31m130.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-1.3.0-py3-none-any.whl (336 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m336.6/336.6 kB\u001b[0m \u001b[31m116.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading automated_interpretability-0.0.6-py3-none-any.whl (57 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading babe-0.0.7-py3-none-any.whl (6.9 kB)\n",
      "Downloading beartype-0.14.1-py3-none-any.whl (739 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m739.7/739.7 kB\u001b[0m \u001b[31m132.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading better_abc-0.0.3-py3-none-any.whl (3.5 kB)\n",
      "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Downloading datasets-2.21.0-py3-none-any.whl (527 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m527.3/527.3 kB\u001b[0m \u001b[31m137.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading eindex_callum-0.1.2-py3-none-any.whl (8.3 kB)\n",
      "Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fancy_einsum-0.0.3-py3-none-any.whl (6.2 kB)\n",
      "Downloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.27.1-py3-none-any.whl (450 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m450.7/450.7 kB\u001b[0m \u001b[31m82.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading isort-5.13.2-py3-none-any.whl (92 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.3/92.3 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jaxtyping-0.2.37-py3-none-any.whl (56 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jiter-0.8.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (345 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.0/345.0 kB\u001b[0m \u001b[31m71.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading matplotlib-3.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m150.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m146.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m142.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading plotly-5.24.1-py3-none-any.whl (19.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.1/19.1 MB\u001b[0m \u001b[31m141.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading plotly_express-0.4.1-py2.py3-none-any.whl (2.9 kB)\n",
      "Downloading pydantic-2.10.6-py3-none-any.whl (431 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.7/431.7 kB\u001b[0m \u001b[31m103.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_core-2.27.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m146.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytest_profiling-1.8.1-py3-none-any.whl (9.9 kB)\n",
      "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.4/242.4 kB\u001b[0m \u001b[31m97.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (435 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m435.0/435.0 kB\u001b[0m \u001b[31m119.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading simple_parsing-0.1.7-py3-none-any.whl (112 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.8/112.8 kB\u001b[0m \u001b[31m56.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.48.1-py3-none-any.whl (9.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m135.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading typeguard-4.4.1-py3-none-any.whl (35 kB)\n",
      "Downloading typer-0.12.5-py3-none-any.whl (47 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.3/47.3 kB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Downloading wandb-0.19.4-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.3/20.3 MB\u001b[0m \u001b[31m125.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading zstandard-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m127.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m135.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading blobfile-2.1.1-py3-none-any.whl (73 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.7/73.7 kB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading boostedblob-0.15.6-py3-none-any.whl (59 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.2/59.2 kB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.11.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m139.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading click-8.1.8-py3-none-any.whl (98 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.2/98.2 kB\u001b[0m \u001b[31m55.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (324 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.0/325.0 kB\u001b[0m \u001b[31m109.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m66.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Downloading docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
      "Downloading fonttools-4.55.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m160.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading GitPython-3.1.44-py3-none-any.whl (207 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.6/207.6 kB\u001b[0m \u001b[31m79.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.8-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m156.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading marshmallow-3.26.0-py3-none-any.whl (50 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading orjson-3.10.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.3/130.3 kB\u001b[0m \u001b[31m66.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading patsy-1.0.1-py2.py3-none-any.whl (232 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.9/232.9 kB\u001b[0m \u001b[31m96.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pfzy-0.3.4-py3-none-any.whl (8.5 kB)\n",
      "Downloading protobuf-5.29.3-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m109.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-19.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (42.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.1/42.1 MB\u001b[0m \u001b[31m101.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m508.0/508.0 kB\u001b[0m \u001b[31m106.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.7/781.7 kB\u001b[0m \u001b[31m139.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m117.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m91.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (40.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.6/40.6 MB\u001b[0m \u001b[31m93.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading sentry_sdk-2.20.0-py2.py3-none-any.whl (322 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.6/322.6 kB\u001b[0m \u001b[31m93.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading statsmodels-0.14.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m137.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading tenacity-9.0.0-py3-none-any.whl (28 kB)\n",
      "Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m121.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m160.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading tzdata-2025.1-py2.py3-none-any.whl (346 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m346.8/346.8 kB\u001b[0m \u001b[31m111.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading wadler_lindig-0.1.3-py3-none-any.whl (20 kB)\n",
      "Downloading gprof2dot-2024.6.6-py2.py3-none-any.whl (34 kB)\n",
      "Downloading graze-0.1.27-py3-none-any.whl (19 kB)\n",
      "Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m69.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytest-8.3.4-py3-none-any.whl (343 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m343.1/343.1 kB\u001b[0m \u001b[31m117.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading setproctitle-1.3.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
      "Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m90.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohappyeyeballs-2.4.4-py3-none-any.whl (14 kB)\n",
      "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Downloading frozenlist-1.5.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (241 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.9/241.9 kB\u001b[0m \u001b[31m102.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading lxml-4.9.4-cp310-cp310-manylinux_2_28_x86_64.whl (7.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m167.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Downloading multidict-6.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.6/124.6 kB\u001b[0m \u001b[31m64.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Downloading pluggy-1.5.0-py3-none-any.whl (20 kB)\n",
      "Downloading propcache-0.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (205 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m205.1/205.1 kB\u001b[0m \u001b[31m96.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pycryptodomex-3.21.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m149.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Downloading uvloop-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m156.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading yarl-1.18.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (319 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m115.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading config2py-0.1.36-py3-none-any.whl (32 kB)\n",
      "Downloading dol-0.3.7-py3-none-any.whl (247 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.7/247.7 kB\u001b[0m \u001b[31m105.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading importlib_resources-6.5.2-py3-none-any.whl (37 kB)\n",
      "Downloading iniconfig-2.0.0-py3-none-any.whl (5.9 kB)\n",
      "Downloading smmap-5.0.2-py3-none-any.whl (24 kB)\n",
      "Downloading i2-0.1.45-py3-none-any.whl (202 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m202.8/202.8 kB\u001b[0m \u001b[31m90.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: py2store\n",
      "  Building wheel for py2store (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for py2store: filename=py2store-0.1.20-py3-none-any.whl size=118410 sha256=5c472765a4a953a7cbe9fdb8cdf14fce681c8a041b40221dca1ae29b520ac160\n",
      "  Stored in directory: /root/.cache/pip/wheels/ff/40/40/fa84c63029cbb45f4f3824be4be62c6838436ad4cb264b5585\n",
      "Successfully built py2store\n",
      "Installing collected packages: sentencepiece, pytz, i2, dol, better-abc, zstandard, xxhash, wadler-lindig, uvloop, tzdata, typing-extensions, tqdm, threadpoolctl, tenacity, tabulate, smmap, shellingham, setproctitle, sentry-sdk, scipy, safetensors, requests, regex, pyzmq, python-dotenv, pycryptodomex, pyarrow, protobuf, propcache, pluggy, pfzy, patsy, orjson, mypy-extensions, mdurl, marshmallow, lxml, kiwisolver, joblib, jiter, isort, iniconfig, importlib_resources, gprof2dot, frozenlist, fonttools, fancy-einsum, einops, docstring-parser, docker-pycreds, dill, cycler, contourpy, config2py, click, beartype, async-timeout, annotated-types, aiohappyeyeballs, typing-inspect, typeguard, tiktoken, simple-parsing, scikit-learn, pytest, pydantic-core, py2store, plotly, pandas, nltk, multiprocess, multidict, matplotlib, markdown-it-py, jaxtyping, InquirerPy, huggingface_hub, graze, gitdb, blobfile, aiosignal, yarl, tokenizers, statsmodels, rich, pytest-profiling, pydantic, httpx, gitpython, dataclasses-json, babe, wandb, typer, transformers, plotly-express, openai, eindex-callum, aiohttp, accelerate, boostedblob, datasets, automated-interpretability, transformer-lens, sae-lens, sae-dashboard\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.9.0\n",
      "    Uninstalling typing_extensions-4.9.0:\n",
      "      Successfully uninstalled typing_extensions-4.9.0\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.31.0\n",
      "    Uninstalling requests-2.31.0:\n",
      "      Successfully uninstalled requests-2.31.0\n",
      "  Attempting uninstall: pyzmq\n",
      "    Found existing installation: pyzmq 24.0.1\n",
      "    Uninstalling pyzmq-24.0.1:\n",
      "      Successfully uninstalled pyzmq-24.0.1\n",
      "  Attempting uninstall: lxml\n",
      "    Found existing installation: lxml 5.1.0\n",
      "    Uninstalling lxml-5.1.0:\n",
      "      Successfully uninstalled lxml-5.1.0\n",
      "  Attempting uninstall: httpx\n",
      "    Found existing installation: httpx 0.26.0\n",
      "    Uninstalling httpx-0.26.0:\n",
      "      Successfully uninstalled httpx-0.26.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "notebook 6.5.5 requires pyzmq<25,>=17, but you have pyzmq 26.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed InquirerPy-0.3.4 accelerate-1.3.0 aiohappyeyeballs-2.4.4 aiohttp-3.11.11 aiosignal-1.3.2 annotated-types-0.7.0 async-timeout-5.0.1 automated-interpretability-0.0.6 babe-0.0.7 beartype-0.14.1 better-abc-0.0.3 blobfile-2.1.1 boostedblob-0.15.6 click-8.1.8 config2py-0.1.36 contourpy-1.3.1 cycler-0.12.1 dataclasses-json-0.6.7 datasets-2.21.0 dill-0.3.8 docker-pycreds-0.4.0 docstring-parser-0.16 dol-0.3.7 eindex-callum-0.1.2 einops-0.7.0 fancy-einsum-0.0.3 fonttools-4.55.6 frozenlist-1.5.0 gitdb-4.0.12 gitpython-3.1.44 gprof2dot-2024.6.6 graze-0.1.27 httpx-0.27.2 huggingface_hub-0.27.1 i2-0.1.45 importlib_resources-6.5.2 iniconfig-2.0.0 isort-5.13.2 jaxtyping-0.2.37 jiter-0.8.2 joblib-1.4.2 kiwisolver-1.4.8 lxml-4.9.4 markdown-it-py-3.0.0 marshmallow-3.26.0 matplotlib-3.10.0 mdurl-0.1.2 multidict-6.1.0 multiprocess-0.70.16 mypy-extensions-1.0.0 nltk-3.9.1 openai-1.60.2 orjson-3.10.15 pandas-2.2.3 patsy-1.0.1 pfzy-0.3.4 plotly-5.24.1 plotly-express-0.4.1 pluggy-1.5.0 propcache-0.2.1 protobuf-5.29.3 py2store-0.1.20 pyarrow-19.0.0 pycryptodomex-3.21.0 pydantic-2.10.6 pydantic-core-2.27.2 pytest-8.3.4 pytest-profiling-1.8.1 python-dotenv-1.0.1 pytz-2024.2 pyzmq-26.0.0 regex-2024.11.6 requests-2.32.3 rich-13.9.4 sae-dashboard-0.6.4 sae-lens-4.4.5 safetensors-0.4.5 scikit-learn-1.6.1 scipy-1.15.1 sentencepiece-0.2.0 sentry-sdk-2.20.0 setproctitle-1.3.4 shellingham-1.5.4 simple-parsing-0.1.7 smmap-5.0.2 statsmodels-0.14.4 tabulate-0.9.0 tenacity-9.0.0 threadpoolctl-3.5.0 tiktoken-0.6.0 tokenizers-0.21.0 tqdm-4.67.1 transformer-lens-2.11.0 transformers-4.48.1 typeguard-4.4.1 typer-0.12.5 typing-extensions-4.12.2 typing-inspect-0.9.0 tzdata-2025.1 uvloop-0.21.0 wadler-lindig-0.1.3 wandb-0.19.4 xxhash-3.5.0 yarl-1.18.3 zstandard-0.22.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install sae-lens transformer-lens sae-dashboard huggingface_hub[cli] tabulate openai ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20ecda2-504a-4d19-b9fb-5630bf7759c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install accelerate\n",
    "# in terminal\n",
    "# apt install unzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d51e1be5-7e7a-43ac-875e-b02d9e2b2572",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig, AutoTokenizer\n",
    "from huggingface_hub import hf_hub_download, notebook_login\n",
    "import numpy as np\n",
    "import torch\n",
    "from tabulate import tabulate\n",
    "\n",
    "import sae_lens\n",
    "from transformer_lens import HookedTransformer\n",
    "from sae_lens import SAE,HookedSAETransformer,ActivationsStore\n",
    "from sae_lens.analysis.neuronpedia_integration import get_neuronpedia_quick_list\n",
    "from sae_lens.toolkit.pretrained_saes_directory import get_pretrained_saes_directory\n",
    "from IPython.display import HTML, IFrame, clear_output, display\n",
    "from jaxtyping import Float, Int\n",
    "from torch import Tensor, nn\n",
    "import einops\n",
    "from rich import print as rprint\n",
    "from rich.table import Table\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "import requests\n",
    "from typing import Any, Callable, Literal, TypeAlias\n",
    "from openai import OpenAI\n",
    "from huggingface_hub import interpreter_login\n",
    "import os\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab6b6226-e2c3-4c64-8640-d1093fe2fd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "chapter = \"chapter1_transformer_interp\"\n",
    "repo = \"ARENA_3.0\"\n",
    "root = \"/workspace/vads-prevalent-safety-llm/notebooks\"\n",
    "\n",
    "if not os.path.exists(f\"{root}/{chapter}\"):\n",
    "    !wget https://github.com/callummcdougall/ARENA_3.0/archive/refs/heads/main.zip\n",
    "    !unzip {root}/main.zip 'ARENA_3.0-main/chapter1_transformer_interp/exercises/*'\n",
    "    !mv {root}/{repo}-main/{chapter} {root}/{chapter}\n",
    "    !rm {root}/main.zip\n",
    "    !rmdir {root}/{repo}-main\n",
    "\n",
    "# !touch {root}/chapter1_transformer_interp/exercises/part32_superposition_and_saes/__init__.py\n",
    "# !touch {root}/chapter1_transformer_interp/exercises/__init__.py\n",
    "\n",
    "# !touch //chapter1_transformer_interp/exercises/part32_superposition_and_saes/__init__.py\n",
    "# !touch /content/chapter1_transformer_interp/exercises/__init__.py\n",
    "sys.path.append(f\"{root}/{chapter}/exercises\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5268f998-38c9-42e0-9bea-1a5c9041ddc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import part31_superposition_and_saes.tests as part31_tests\n",
    "import part31_superposition_and_saes.utils as part31_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "020d11b3-59d2-4335-8589-76a6a4cfeb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook_login()\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "HF_TOKEN=os.getenv(\"HF_TOKEN\")\n",
    "OPENAI_TOKEN = os.getenv(\"OPEN_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32934258-31d2-4bc0-8088-adbb48081b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
      "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
      "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
      "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
      "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your token (input will not be visible):  ········\n",
      "Add token as git credential? (Y/n)  n\n"
     ]
    }
   ],
   "source": [
    "interpreter_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e530aff6-034b-40d2-9bde-38c32a2dfd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_dashboard(\n",
    "    sae_release=\"gpt2-small-res-jb\",\n",
    "    sae_id=\"blocks.7.hook_resid_pre\",\n",
    "    latent_idx=0,\n",
    "    width=800,\n",
    "    height=600,\n",
    "):\n",
    "    release = get_pretrained_saes_directory()[sae_release]\n",
    "    neuronpedia_id = release.neuronpedia_id[sae_id]\n",
    "\n",
    "    url = f\"https://neuronpedia.org/{neuronpedia_id}/{latent_idx}?embed=true&embedexplanation=true&embedplots=true&embedtest=true&height=300\"\n",
    "\n",
    "    print(url)\n",
    "    display(IFrame(url, width=width, height=height))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3fb37992-ddf1-4ca0-a364-ef5a67e926d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>release</th>\n",
       "      <th>repo_id</th>\n",
       "      <th>model</th>\n",
       "      <th>saes_map</th>\n",
       "      <th>neuronpedia_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>gemma-2b-it-res-jb</th>\n",
       "      <td>gemma-2b-it-res-jb</td>\n",
       "      <td>jbloom/Gemma-2b-IT-Residual-Stream-SAEs</td>\n",
       "      <td>gemma-2b-it</td>\n",
       "      <td>{'blocks.12.hook_resid_post': 'gemma_2b_it_blo...</td>\n",
       "      <td>{'blocks.12.hook_resid_post': 'gemma-2b-it/12-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemma-2b-res-jb</th>\n",
       "      <td>gemma-2b-res-jb</td>\n",
       "      <td>jbloom/Gemma-2b-Residual-Stream-SAEs</td>\n",
       "      <td>gemma-2b</td>\n",
       "      <td>{'blocks.0.hook_resid_post': 'gemma_2b_blocks....</td>\n",
       "      <td>{'blocks.0.hook_resid_post': 'gemma-2b/0-res-j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemma-scope-27b-pt-res</th>\n",
       "      <td>gemma-scope-27b-pt-res</td>\n",
       "      <td>google/gemma-scope-27b-pt-res</td>\n",
       "      <td>gemma-2-27b</td>\n",
       "      <td>{'layer_10/width_131k/average_l0_106': 'layer_...</td>\n",
       "      <td>{'layer_10/width_131k/average_l0_106': None, '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemma-scope-27b-pt-res-canonical</th>\n",
       "      <td>gemma-scope-27b-pt-res-canonical</td>\n",
       "      <td>google/gemma-scope-27b-pt-res</td>\n",
       "      <td>gemma-2-27b</td>\n",
       "      <td>{'layer_10/width_131k/canonical': 'layer_10/wi...</td>\n",
       "      <td>{'layer_10/width_131k/canonical': 'gemma-2-27b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemma-scope-2b-pt-att</th>\n",
       "      <td>gemma-scope-2b-pt-att</td>\n",
       "      <td>google/gemma-scope-2b-pt-att</td>\n",
       "      <td>gemma-2-2b</td>\n",
       "      <td>{'layer_0/width_16k/average_l0_104': 'layer_0/...</td>\n",
       "      <td>{'layer_0/width_16k/average_l0_104': None, 'la...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                           release  \\\n",
       "gemma-2b-it-res-jb                              gemma-2b-it-res-jb   \n",
       "gemma-2b-res-jb                                    gemma-2b-res-jb   \n",
       "gemma-scope-27b-pt-res                      gemma-scope-27b-pt-res   \n",
       "gemma-scope-27b-pt-res-canonical  gemma-scope-27b-pt-res-canonical   \n",
       "gemma-scope-2b-pt-att                        gemma-scope-2b-pt-att   \n",
       "\n",
       "                                                                  repo_id  \\\n",
       "gemma-2b-it-res-jb                jbloom/Gemma-2b-IT-Residual-Stream-SAEs   \n",
       "gemma-2b-res-jb                      jbloom/Gemma-2b-Residual-Stream-SAEs   \n",
       "gemma-scope-27b-pt-res                      google/gemma-scope-27b-pt-res   \n",
       "gemma-scope-27b-pt-res-canonical            google/gemma-scope-27b-pt-res   \n",
       "gemma-scope-2b-pt-att                        google/gemma-scope-2b-pt-att   \n",
       "\n",
       "                                        model  \\\n",
       "gemma-2b-it-res-jb                gemma-2b-it   \n",
       "gemma-2b-res-jb                      gemma-2b   \n",
       "gemma-scope-27b-pt-res            gemma-2-27b   \n",
       "gemma-scope-27b-pt-res-canonical  gemma-2-27b   \n",
       "gemma-scope-2b-pt-att              gemma-2-2b   \n",
       "\n",
       "                                                                           saes_map  \\\n",
       "gemma-2b-it-res-jb                {'blocks.12.hook_resid_post': 'gemma_2b_it_blo...   \n",
       "gemma-2b-res-jb                   {'blocks.0.hook_resid_post': 'gemma_2b_blocks....   \n",
       "gemma-scope-27b-pt-res            {'layer_10/width_131k/average_l0_106': 'layer_...   \n",
       "gemma-scope-27b-pt-res-canonical  {'layer_10/width_131k/canonical': 'layer_10/wi...   \n",
       "gemma-scope-2b-pt-att             {'layer_0/width_16k/average_l0_104': 'layer_0/...   \n",
       "\n",
       "                                                                     neuronpedia_id  \n",
       "gemma-2b-it-res-jb                {'blocks.12.hook_resid_post': 'gemma-2b-it/12-...  \n",
       "gemma-2b-res-jb                   {'blocks.0.hook_resid_post': 'gemma-2b/0-res-j...  \n",
       "gemma-scope-27b-pt-res            {'layer_10/width_131k/average_l0_106': None, '...  \n",
       "gemma-scope-27b-pt-res-canonical  {'layer_10/width_131k/canonical': 'gemma-2-27b...  \n",
       "gemma-scope-2b-pt-att             {'layer_0/width_16k/average_l0_104': None, 'la...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sae_lens.toolkit.pretrained_saes_directory import get_pretrained_saes_directory\n",
    "\n",
    "# TODO: Make this nicer.\n",
    "df = pd.DataFrame.from_records(\n",
    "    {k: v.__dict__ for k, v in get_pretrained_saes_directory().items()}\n",
    ").T\n",
    "df.drop(\n",
    "    columns=[\n",
    "        \"expected_var_explained\",\n",
    "        \"expected_l0\",\n",
    "        \"config_overrides\",\n",
    "        \"conversion_func\",\n",
    "    ],\n",
    "    inplace=True,\n",
    ")\n",
    "# df  # Each row is a \"release\" which has multiple SAEs which may have different configs / match different hook points in a model.\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2491aeea-7ab0-4c69-9d6c-6a9e566a2767",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>release</th>\n",
       "      <th>repo_id</th>\n",
       "      <th>model</th>\n",
       "      <th>saes_map</th>\n",
       "      <th>neuronpedia_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>gemma-2b-res-jb</th>\n",
       "      <td>gemma-2b-res-jb</td>\n",
       "      <td>jbloom/Gemma-2b-Residual-Stream-SAEs</td>\n",
       "      <td>gemma-2b</td>\n",
       "      <td>{'blocks.0.hook_resid_post': 'gemma_2b_blocks....</td>\n",
       "      <td>{'blocks.0.hook_resid_post': 'gemma-2b/0-res-j...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         release                               repo_id  \\\n",
       "gemma-2b-res-jb  gemma-2b-res-jb  jbloom/Gemma-2b-Residual-Stream-SAEs   \n",
       "\n",
       "                    model                                           saes_map  \\\n",
       "gemma-2b-res-jb  gemma-2b  {'blocks.0.hook_resid_post': 'gemma_2b_blocks....   \n",
       "\n",
       "                                                    neuronpedia_id  \n",
       "gemma-2b-res-jb  {'blocks.0.hook_resid_post': 'gemma-2b/0-res-j...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df.release == \"gemma-2b-res-jb\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da072c4d-6eb9-454b-8859-0b8c5ca07923",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ff2368-f4a6-4451-964b-4a5ba7b23803",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c515a9d3-703f-4f45-85fe-70c7f5c9bd91",
   "metadata": {},
   "source": [
    "## Load the Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95a0881d-8538-444e-a62b-73e49aa74323",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "984bacf342fd45abaee514b0c6ee5f21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/818 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "364789f02c324e28872fad87aa605a7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/24.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f5a94bc16394c439530add18edeb111",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff53929e6d4b4ac29fccf77dfd3ce52e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2371a2d6bcad45bca433c8712241df57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bb322a165144cab82a3b02afef0eb9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/481M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b164d24a99664f0a9989c613b5ed24de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e815b65b1a0141dc9fb009443f8ddf3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/168 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e4f5cbafd094b9c83e314681b84638a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/46.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca90c16178e740f6aabf2a9ac15a04ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be9f0e7518964b4c9f21617e134184c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "827fc5d320f846aea3af9fc038eb47e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gemma-2-2b into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the LLM\n",
    "model = HookedSAETransformer.from_pretrained_no_processing(\n",
    "    \"gemma-2-2b\",\n",
    "    device = device,\n",
    "    torch_dtype = torch.float16,\n",
    "    device_map = \"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d74bc5-43a2-44b0-8a09-cd46c9e3aa3b",
   "metadata": {},
   "source": [
    "## Load SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "14293738-3535-4002-9d82-218e56c50477",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a7df9d59537423b8058e86802c4dadd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "params.npz:   0%|          | 0.00/302M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the corresponding SAE\n",
    "release=\"gemma-scope-2b-pt-res-canonical\"  # Replace with the correct release for your model\n",
    "sae_id=\"layer_20/width_16k/canonical\"\n",
    "sae, cfg_dict, _ = sae_lens.SAE.from_pretrained(\n",
    "    release=release,  # Replace with the correct release for your model\n",
    "    sae_id=sae_id,\n",
    "    device=device,\n",
    "    # device_map = \"auto\",\n",
    ")\n",
    "\n",
    "\n",
    "# # Load the corresponding SAE\n",
    "# release=\"gemma-scope-2b-pt-res-canonical\"  # Replace with the correct release for your model\n",
    "# sae_id=\"layer_12/width_1m/canonical\"\n",
    "# sae, cfg_dict, _ = sae_lens.SAE.from_pretrained(\n",
    "#     release=release,  # Replace with the correct release for your model\n",
    "#     sae_id=sae_id,\n",
    "#     device=device,\n",
    "#     # device_map = \"auto\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0a55f2d0-384b-4f5d-851b-09115dd11724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://neuronpedia.org/gemma-2-2b/20-gemmascope-res-16k/9?embed=true&embedexplanation=true&embedplots=true&embedtest=true&height=300\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"600\"\n",
       "            src=\"https://neuronpedia.org/gemma-2-2b/20-gemmascope-res-16k/9?embed=true&embedexplanation=true&embedplots=true&embedtest=true&height=300\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7a6acc35f4f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# latent_idx = 12082\n",
    "latent_idx = 9\n",
    "\n",
    "display_dashboard(sae_release=release, sae_id=sae_id, latent_idx=latent_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7872d8-9993-4064-a963-17bce30ad625",
   "metadata": {},
   "source": [
    "## Activation Store Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c013e6c9-fd0b-45e3-840b-681304f4e8c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00d80facac3c4bbfbffa1a1b4c5f59ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/776 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffca937c10204b0e876a95c8d41949e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sae_lens/training/activations_store.py:246: UserWarning: Dataset is not tokenized. Pre-tokenizing will improve performance and allows for more control over special tokens. See https://jbloomaus.github.io/SAELens/training_saes/#pretokenizing-datasets for more info.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# activation store\n",
    "gemma2_act_store = ActivationsStore.from_sae(\n",
    "    model=model,\n",
    "    sae=sae,\n",
    "    streaming=True,\n",
    "    store_batch_size_prompts=8,\n",
    "    train_batch_size_tokens=2048,\n",
    "    n_batches_in_buffer=16,\n",
    "    device=str(device),\n",
    ")\n",
    "\n",
    "# Example of how you can use this:\n",
    "with torch.no_grad():\n",
    "    tokens = gemma2_act_store.get_batch_tokens()\n",
    "assert tokens.shape == (gemma2_act_store.store_batch_size_prompts, gemma2_act_store.context_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a851807-2041-47e5-9899-59f2640446bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 1024])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens.shape # as the batch is of 8 prompts and each with 1024 context_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05af76a-9329-4ad6-8e5f-73994d22c8c9",
   "metadata": {},
   "source": [
    "## Utility Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "027e2d21-cfb9-4a7f-ab4a-3f2c726598a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_k_largest_indices(\n",
    "    x: Float[Tensor, \"batch seq\"],\n",
    "    k: int,\n",
    "    buffer: int = 0,\n",
    "    no_overlap: bool = True,\n",
    ") -> Int[Tensor, \"k 2\"]:\n",
    "    \"\"\"\n",
    "    Returns the tensor of (batch, seqpos) indices for each of the top k elements in the tensor x.\n",
    "\n",
    "    Args:\n",
    "        buffer:     We won't choose any elements within `buffer` from the start or end of their seq (this helps if we\n",
    "                    want more context around the chosen tokens).\n",
    "        no_overlap: If True, this ensures that no 2 top-activating tokens are in the same seq and within `buffer` of\n",
    "                    each other.\n",
    "    \"\"\"\n",
    "    assert buffer * 2 < x.size(1), \"Buffer is too large for the sequence length\"\n",
    "    assert not no_overlap or k <= x.size(0), \"Not enough sequences to have a different token in each sequence\"\n",
    "\n",
    "    if buffer > 0:\n",
    "        x = x[:, buffer:-buffer]\n",
    "\n",
    "    indices = x.flatten().argsort(-1, descending=True)\n",
    "    # extra_buffer = 10\n",
    "    # values, indices = x.flatten().topk(k + extra_buffer,largest = True)\n",
    "    rows = indices // x.size(1)\n",
    "    cols = indices % x.size(1) + buffer\n",
    "\n",
    "    if rows.numel() == 0 or cols.numel() ==0:\n",
    "        raise ValueError(\"No Valid activations found after applying buffer.\")\n",
    "\n",
    "    if no_overlap:\n",
    "        unique_indices = torch.empty((0, 2), device=x.device).long()\n",
    "        while len(unique_indices) < k:\n",
    "            unique_indices = torch.cat((unique_indices, torch.tensor([[rows[0], cols[0]]], device=x.device)))\n",
    "            is_overlapping_mask = (rows == rows[0]) & ((cols - cols[0]).abs() <= buffer)\n",
    "            rows = rows[~is_overlapping_mask]\n",
    "            cols = cols[~is_overlapping_mask]\n",
    "        return unique_indices\n",
    "\n",
    "    return torch.stack((rows, cols), dim=1)[:k]\n",
    "\n",
    "# x = torch.arange(40, device=device).reshape((2, 20))\n",
    "# x[0, 10] += 50  # 2nd highest value\n",
    "# x[0, 11] += 100  # highest value\n",
    "# x[1, 1] += 150  # not inside buffer (it's less than 3 from the start of the sequence)\n",
    "# top_indices = get_k_largest_indices(x, k=2, buffer=3)\n",
    "# rprint(top_indices)\n",
    "# assert top_indices.tolist() == [[0, 11], [0, 10]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "56be6063-f3a2-42c3-a5b5-99eb003af71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def index_with_buffer(\n",
    "    x: Float[Tensor, \"batch seq\"], indices: Int[Tensor, \"k 2\"], buffer: int | None = None\n",
    ") -> Float[Tensor, \"k *buffer_x2_plus1\"]:\n",
    "    \"\"\"\n",
    "    Indexes into `x` with `indices` (which should have come from the `get_k_largest_indices` function), and takes a\n",
    "    +-buffer range around each indexed element. If `indices` are less than `buffer` away from the start of a sequence\n",
    "    then we just take the first `2*buffer+1` elems (same for at the end of a sequence).\n",
    "\n",
    "    If `buffer` is None, then we don't add any buffer and just return the elements at the given indices.\n",
    "    \"\"\"\n",
    "    rows, cols = indices.unbind(dim=-1)\n",
    "    if buffer is not None:\n",
    "        rows = einops.repeat(rows, \"k -> k buffer\", buffer=buffer * 2 + 1)\n",
    "        cols[cols < buffer] = buffer\n",
    "        cols[cols > x.size(1) - buffer - 1] = x.size(1) - buffer - 1\n",
    "        cols = einops.repeat(cols, \"k -> k buffer\", buffer=buffer * 2 + 1) + torch.arange(\n",
    "            -buffer, buffer + 1, device=cols.device\n",
    "        )\n",
    "    return x[rows, cols]\n",
    "\n",
    "\n",
    "# x_top_values_with_context = index_with_buffer(x, top_indices, buffer=3)\n",
    "# assert x_top_values_with_context[0].tolist() == [8, 9, 10 + 50, 11 + 100, 12, 13, 14]  # highest value in the middle\n",
    "# assert x_top_values_with_context[1].tolist() == [7, 8, 9, 10 + 50, 11 + 100, 12, 13]  # 2nd highest value in the middle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bb68b1ac-bcfd-4441-a353-6756b5502802",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">  Max Activating Examples   </span>\n",
       "┏━━━━━━━┳━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Act   </span>┃<span style=\"font-weight: bold\"> Sequence         </span>┃\n",
       "┡━━━━━━━╇━━━━━━━━━━━━━━━━━━┩\n",
       "│ 0.500 │ '<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold; text-decoration: underline\"> one</span> two three' │\n",
       "├───────┼──────────────────┤\n",
       "│ 1.500 │ ' one<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold; text-decoration: underline\"> two</span> three' │\n",
       "├───────┼──────────────────┤\n",
       "│ 2.500 │ ' one two<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold; text-decoration: underline\"> three</span>' │\n",
       "└───────┴──────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m  Max Activating Examples   \u001b[0m\n",
       "┏━━━━━━━┳━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mAct  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mSequence        \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━╇━━━━━━━━━━━━━━━━━━┩\n",
       "│ 0.500 │ '\u001b[1;4;32m one\u001b[0m two three' │\n",
       "├───────┼──────────────────┤\n",
       "│ 1.500 │ ' one\u001b[1;4;32m two\u001b[0m three' │\n",
       "├───────┼──────────────────┤\n",
       "│ 2.500 │ ' one two\u001b[1;4;32m three\u001b[0m' │\n",
       "└───────┴──────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def display_top_seqs(data: list[tuple[float, list[str], int]]):\n",
    "    \"\"\"\n",
    "    Given a list of (activation: float, str_toks: list[str], seq_pos: int), displays a table of these sequences, with\n",
    "    the relevant token highlighted.\n",
    "\n",
    "    We also turn newlines into \"\\\\n\", and remove unknown tokens � (usually weird quotation marks) for readability.\n",
    "    \"\"\"\n",
    "    table = Table(\"Act\", \"Sequence\", title=\"Max Activating Examples\", show_lines=True)\n",
    "    for act, str_toks, seq_pos in data:\n",
    "        formatted_seq = (\n",
    "            \"\".join([f\"[b u green]{str_tok}[/]\" if i == seq_pos else str_tok for i, str_tok in enumerate(str_toks)])\n",
    "            .replace(\"�\", \"\")\n",
    "            .replace(\"\\n\", \"↵\")\n",
    "        )\n",
    "        table.add_row(f\"{act:.3f}\", repr(formatted_seq))\n",
    "    rprint(table)\n",
    "\n",
    "\n",
    "example_data = [\n",
    "    (0.5, [\" one\", \" two\", \" three\"], 0),\n",
    "    (1.5, [\" one\", \" two\", \" three\"], 1),\n",
    "    (2.5, [\" one\", \" two\", \" three\"], 2),\n",
    "]\n",
    "display_top_seqs(example_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b1126db7-cc00-435a-a11b-98ce809d16bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                            Max Activating Examples                             </span>\n",
       "┏━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Act    </span>┃<span style=\"font-weight: bold\"> Sequence                                                            </span>┃\n",
       "┡━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ 78.648 │ ', higher audit fees,<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold; text-decoration: underline\"> conflicts</span> of interest, a lack'                │\n",
       "├────────┼─────────────────────────────────────────────────────────────────────┤\n",
       "│ 77.100 │ ' an auditor’s basic<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold; text-decoration: underline\"> conflict</span> between serving the paying client'    │\n",
       "├────────┼─────────────────────────────────────────────────────────────────────┤\n",
       "│ 76.277 │ ' tobacco, and a bitter<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold; text-decoration: underline\"> fight</span> in some nations over de'              │\n",
       "├────────┼─────────────────────────────────────────────────────────────────────┤\n",
       "│ 75.522 │ ' care. The resolution of<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold; text-decoration: underline\"> conflicts</span> in the evidence rests solely'   │\n",
       "├────────┼─────────────────────────────────────────────────────────────────────┤\n",
       "│ 75.200 │ ' audit quality, removing the<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold; text-decoration: underline\"> conflicts</span> of interest that can arise' │\n",
       "└────────┴─────────────────────────────────────────────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                            Max Activating Examples                             \u001b[0m\n",
       "┏━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mAct   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mSequence                                                           \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ 78.648 │ ', higher audit fees,\u001b[1;4;32m conflicts\u001b[0m of interest, a lack'                │\n",
       "├────────┼─────────────────────────────────────────────────────────────────────┤\n",
       "│ 77.100 │ ' an auditor’s basic\u001b[1;4;32m conflict\u001b[0m between serving the paying client'    │\n",
       "├────────┼─────────────────────────────────────────────────────────────────────┤\n",
       "│ 76.277 │ ' tobacco, and a bitter\u001b[1;4;32m fight\u001b[0m in some nations over de'              │\n",
       "├────────┼─────────────────────────────────────────────────────────────────────┤\n",
       "│ 75.522 │ ' care. The resolution of\u001b[1;4;32m conflicts\u001b[0m in the evidence rests solely'   │\n",
       "├────────┼─────────────────────────────────────────────────────────────────────┤\n",
       "│ 75.200 │ ' audit quality, removing the\u001b[1;4;32m conflicts\u001b[0m of interest that can arise' │\n",
       "└────────┴─────────────────────────────────────────────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def fetch_max_activating_examples(\n",
    "    model: HookedSAETransformer,\n",
    "    sae: SAE,\n",
    "    act_store: ActivationsStore,\n",
    "    latent_idx: int,\n",
    "    total_batches: int = 100,\n",
    "    k: int = 10,\n",
    "    buffer: int = 10,\n",
    "    display: bool = False,\n",
    ") -> list[tuple[float, list[str], int]]:\n",
    "    \"\"\"\n",
    "    Displays the max activating examples across a number of batches from the\n",
    "    activations store, using the `display_top_seqs` function.\n",
    "    \"\"\"\n",
    "    sae_acts_post_hook_name = f\"{sae.cfg.hook_name}.hook_sae_acts_post\"\n",
    "\n",
    "    # Create list to store the top k activations for each batch. Once we're done,\n",
    "    # we'll filter this to only contain the top k over all batches\n",
    "    data = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(total_batches):\n",
    "            tokens = act_store.get_batch_tokens(batch_size = 5)\n",
    "            # Handling empty batch\n",
    "            if tokens is None or tokens.numel() == 0:\n",
    "                continue\n",
    "\n",
    "            tokens = tokens.to(model.cfg.device)\n",
    "            # print(\"Tokens shape:\", tokens.shape)\n",
    "\n",
    "            batch_size = tokens.size(0)\n",
    "            current_k = min(k,batch_size)\n",
    "    \n",
    "            _, cache = model.run_with_cache_with_saes(\n",
    "                tokens,\n",
    "                saes=[sae],\n",
    "                stop_at_layer=sae.cfg.hook_layer + 1,\n",
    "                names_filter=[sae_acts_post_hook_name],\n",
    "            )\n",
    "            acts = cache[sae_acts_post_hook_name][..., latent_idx]\n",
    "            # print(\"Activations shape:\", acts.shape)\n",
    "            # Get largest indices, get the corresponding max acts, and get the surrounding indices\n",
    "            k_largest_indices = get_k_largest_indices(acts, k=current_k, buffer=buffer,no_overlap = True)\n",
    "            # print(k_largest_indices)\n",
    "            tokens_with_buffer = index_with_buffer(tokens, k_largest_indices, buffer=buffer)\n",
    "            str_toks = [model.to_str_tokens(toks) for toks in tokens_with_buffer]\n",
    "            top_acts = index_with_buffer(acts, k_largest_indices).tolist()\n",
    "            data.extend(list(zip(top_acts, str_toks, [buffer] * len(str_toks))))\n",
    "    \n",
    "            # GPU cache clear\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "    data = sorted(data, key=lambda x: x[0], reverse=True)[:k]\n",
    "    if display:\n",
    "        display_top_seqs(data)\n",
    "    return data\n",
    "\n",
    "\n",
    "# Display your results, and also test them\n",
    "buffer = 5\n",
    "data = fetch_max_activating_examples(model, sae, gemma2_act_store, latent_idx=9, buffer=buffer, k=5, display=True)\n",
    "# first_seq_str_tokens = data[0][1]\n",
    "# assert first_seq_str_tokens[buffer] == \" Fight\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c20dc82-c009-45ff-96aa-d929f0152a5c",
   "metadata": {},
   "source": [
    "## Autointerp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1eba4a03-96eb-42cb-92f7-bf1f54ce6a79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>modelId</th>\n",
       "      <th>layer</th>\n",
       "      <th>index</th>\n",
       "      <th>description</th>\n",
       "      <th>explanationModelName</th>\n",
       "      <th>typeName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gemma-2-2b</td>\n",
       "      <td>20-gemmascope-res-16k</td>\n",
       "      <td>14403</td>\n",
       "      <td>phrases or sentences that introduce lists, exa...</td>\n",
       "      <td>claude-3-5-sonnet-20240620</td>\n",
       "      <td>oai_token-act-pair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gemma-2-2b</td>\n",
       "      <td>20-gemmascope-res-16k</td>\n",
       "      <td>14403</td>\n",
       "      <td>references to numerical sports scores and resu...</td>\n",
       "      <td>gemini-1.5-pro</td>\n",
       "      <td>oai_token-act-pair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gemma-2-2b</td>\n",
       "      <td>20-gemmascope-res-16k</td>\n",
       "      <td>14403</td>\n",
       "      <td>text related to sports accomplishments and sta...</td>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>oai_token-act-pair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gemma-2-2b</td>\n",
       "      <td>20-gemmascope-res-16k</td>\n",
       "      <td>10131</td>\n",
       "      <td>phrases referring to being fluent in a languag...</td>\n",
       "      <td>gemini-1.5-flash</td>\n",
       "      <td>oai_token-act-pair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gemma-2-2b</td>\n",
       "      <td>20-gemmascope-res-16k</td>\n",
       "      <td>10133</td>\n",
       "      <td>words related to scientific studies and proces...</td>\n",
       "      <td>gemini-1.5-flash</td>\n",
       "      <td>oai_token-act-pair</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      modelId                  layer  index  \\\n",
       "0  gemma-2-2b  20-gemmascope-res-16k  14403   \n",
       "1  gemma-2-2b  20-gemmascope-res-16k  14403   \n",
       "2  gemma-2-2b  20-gemmascope-res-16k  14403   \n",
       "3  gemma-2-2b  20-gemmascope-res-16k  10131   \n",
       "4  gemma-2-2b  20-gemmascope-res-16k  10133   \n",
       "\n",
       "                                         description  \\\n",
       "0  phrases or sentences that introduce lists, exa...   \n",
       "1  references to numerical sports scores and resu...   \n",
       "2  text related to sports accomplishments and sta...   \n",
       "3  phrases referring to being fluent in a languag...   \n",
       "4  words related to scientific studies and proces...   \n",
       "\n",
       "         explanationModelName            typeName  \n",
       "0  claude-3-5-sonnet-20240620  oai_token-act-pair  \n",
       "1              gemini-1.5-pro  oai_token-act-pair  \n",
       "2                 gpt-4o-mini  oai_token-act-pair  \n",
       "3            gemini-1.5-flash  oai_token-act-pair  \n",
       "4            gemini-1.5-flash  oai_token-act-pair  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_autointerp_df(sae_release=\"gpt2-small-res-jb\", sae_id=\"blocks.7.hook_resid_pre\") -> pd.DataFrame:\n",
    "    release = get_pretrained_saes_directory()[sae_release]\n",
    "    neuronpedia_id = release.neuronpedia_id[sae_id]\n",
    "\n",
    "    url = \"https://www.neuronpedia.org/api/explanation/export?modelId={}&saeId={}\".format(*neuronpedia_id.split(\"/\"))\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    data = response.json()\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "explanations_df_gemma_2b = get_autointerp_df(sae_release = release,sae_id = sae_id)\n",
    "explanations_df_gemma_2b.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5974097c-2360-4e13-af31-cdabb04a2488",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>modelId</th>\n",
       "      <th>layer</th>\n",
       "      <th>index</th>\n",
       "      <th>description</th>\n",
       "      <th>explanationModelName</th>\n",
       "      <th>typeName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [modelId, layer, index, description, explanationModelName, typeName]\n",
       "Index: []"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_temp = get_autointerp_df(release,sae_id)\n",
    "df_temp[df_temp['explanationModelName'] == 'gpt-4o-mini']\n",
    "df_temp.loc[df_temp['index'] == '6631', 'description'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2cc7d147-e5a6-4e6c-bb4c-891fae5dd3c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the beginning of a text or important markers in a document'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_temp.loc[df_temp['index'] == '6631', 'description'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "841ed9fd-632e-45ec-9355-92003b768adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(\n",
    "    model: HookedSAETransformer,\n",
    "    sae: SAE,\n",
    "    act_store: ActivationsStore,\n",
    "    latent_idx: int,\n",
    "    total_batches: int = 100,\n",
    "    k: int = 15,\n",
    "    buffer: int = 10,\n",
    ") -> dict[Literal[\"system\", \"user\", \"assistant\"], str]:\n",
    "    \"\"\"\n",
    "    Returns the system, user & assistant prompts for autointerp.\n",
    "    \"\"\"\n",
    "\n",
    "    data = fetch_max_activating_examples(model, sae, act_store, latent_idx, total_batches, k, buffer)\n",
    "    str_formatted_examples = \"\\n\".join(\n",
    "        f\"{i+1}. {''.join(f'<<{tok}>>' if j == buffer else tok for j, tok in enumerate(seq[1]))}\"\n",
    "        for i, seq in enumerate(data)\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"system\": \"We're studying neurons in a neural network. Each neuron activates on some particular word or concept in a short document. The activating words in each document are indicated with << ... >>. Look at the parts of the document the neuron activates for and summarize in a single sentence what the neuron is activating on. Try to be specific in your explanations, although don't be so specific that you exclude some of the examples from matching your explanation. Pay attention to things like the capitalization and punctuation of the activating words or concepts, if that seems relevant. Keep the explanation as short and simple as possible, limited to 20 words or less. Omit punctuation and formatting. You should avoid giving long lists of words.\",\n",
    "        \"user\": f\"\"\"The activating documents are given below:\\n\\n{str_formatted_examples}\"\"\",\n",
    "        \"assistant\": \"this neuron fires on\",\n",
    "    }\n",
    "\n",
    "\n",
    "# Test your function\n",
    "# data = fetch_max_activating_examples(gpt2, gpt2_sae, gpt2_act_store, latent_idx=9, buffer=buffer, k=5, display=True)\n",
    "# prompts = create_prompt(model, sae, gemma2_act_store, latent_idx=9, total_batches=100, k=10, buffer=5)\n",
    "# assert prompts[\"system\"].startswith(\"We're studying neurons in a neural network.\")\n",
    "# assert \"<< fight>>\" in prompts[\"user\"]\n",
    "# assert prompts[\"assistant\"] == \"this neuron fires on\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8147341b-a623-412d-ae27-f88f606e80c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'system': \"We're studying neurons in a neural network. Each neuron activates on some particular word or concept in a short document. The activating words in each document are indicated with << ... >>. Look at the parts of the document the neuron activates for and summarize in a single sentence what the neuron is activating on. Try to be specific in your explanations, although don't be so specific that you exclude some of the examples from matching your explanation. Pay attention to things like the capitalization and punctuation of the activating words or concepts, if that seems relevant. Keep the explanation as short and simple as possible, limited to 20 words or less. Omit punctuation and formatting. You should avoid giving long lists of words.\",\n",
       " 'user': 'The activating documents are given below:\\n\\n1.  presidential tax disclosures, strengthening<< conflict>>-of-interest protections\\n2.  and gems.<bos>Workers<< fight>> closure of Bronx Stella D\\n3.  leave of absence after a<< fight>> with Sullivan, but he\\n4.  by the time when you<< fight>> toe-to-toe\\n5.  Hannarah heard sounds of<< fighting>>. Angry shouts, gun\\n6.  a blind trust to avoid<< conflicts>> of interest. By contrast\\n7.  called for unnecessary roughness after<< fighting>> after the whistle blew.\\n8.  alien threats to evade or<< fight>>. The Pilots also came\\n9.  numbers of participants in the<< combat>> outside began to dwindle\\n10.  Dragon - an ex-<<combat>>ant who did not take',\n",
       " 'assistant': 'this neuron fires on'}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b32105ee-5bd4-40f4-84a3-6a205b508a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_TOKEN = ''\n",
    "# def get_autointerp_explanation(\n",
    "#     model: HookedSAETransformer,\n",
    "#     sae: SAE,\n",
    "#     act_store: ActivationsStore,\n",
    "#     latent_idx: int,\n",
    "#     total_batches: int = 100,\n",
    "#     k: int = 5,\n",
    "#     buffer: int = 5,\n",
    "#     n_completions: int = 1,\n",
    "# ) -> list[str]:\n",
    "#     \"\"\"\n",
    "#     Queries OpenAI's API using prompts returned from `create_prompt`, and returns\n",
    "#     a list of the completions.\n",
    "#     \"\"\"\n",
    "#     client = OpenAI(api_key=OPENAI_TOKEN)\n",
    "\n",
    "#     prompts = create_prompt(model, sae, act_store, latent_idx, total_batches, k, buffer)\n",
    "\n",
    "#     # print(prompts)\n",
    "#     result = client.chat.completions.create(\n",
    "#         model=\"gpt-4o-mini\",\n",
    "#         messages=[\n",
    "#             {\"role\": \"system\", \"content\": prompts[\"system\"]},\n",
    "#             {\"role\": \"user\", \"content\": prompts[\"user\"]},\n",
    "#             {\"role\": \"assistant\", \"content\": prompts[\"assistant\"]},\n",
    "#         ],\n",
    "#         n=n_completions,\n",
    "#         max_tokens=50,\n",
    "#         stream=False,\n",
    "#     )\n",
    "#     return [choice.message.content for choice in result.choices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "62b215a2-681f-4b7e-bd56-1146d61c2d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "release=\"gemma-scope-2b-pt-res-canonical\"  # Replace with the correct release for your model\n",
    "sae_id=\"layer_20/width_16k/canonical\"\n",
    "explanations_df_gemma_2b = get_autointerp_df(sae_release = release,sae_id = sae_id)\n",
    "\n",
    "def get_autointerp_explanation_df(\n",
    "    explanations_df: pd.DataFrame,\n",
    "    latent_idx: int\n",
    ") -> str:\n",
    "    if explanations_df.empty:\n",
    "        raise ValueError(\"The explanations DataFrame is empty.\")\n",
    "\n",
    "    if latent_idx not in explanations_df['index'].values:\n",
    "        raise ValueError(f\"Latent index {latent_idx} not found in the explanations DataFrame.\")\n",
    "\n",
    "    return explanations_df.loc[\n",
    "        explanations_df['index'] == latent_idx, 'description'\n",
    "    ].iloc[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "07aef9a7-7f9e-4378-8b2a-5b1f17e5ff30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instances of the word \"kill\" and its variations, highlighting themes of violence and death\n"
     ]
    }
   ],
   "source": [
    "completions = get_autointerp_explanation_df(explanations_df_gemma_2b,latent_idx='4442')\n",
    "print(completions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "56813a69-4832-4cfc-afb4-4b9200d3908a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ba914f33-5378-4462-af77-7852165ebcd8",
   "metadata": {},
   "source": [
    "## Top Activating Latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "2079ec8a-c0a8-4c85-9e09-c473b01902c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latent activations shape: torch.Size([10, 16384])\n",
      "Latent ID: [6631], Activation Value: [2029.771728515625]\n",
      "Latent ID: [11746], Activation Value: [122.3427505493164]\n",
      "Latent ID: [12935], Activation Value: [74.62242126464844]\n",
      "Latent ID: [6027], Activation Value: [75.38046264648438]\n",
      "Latent ID: [6631], Activation Value: [65.53387451171875]\n",
      "Latent ID: [12935], Activation Value: [65.13652038574219]\n",
      "Latent ID: [2668], Activation Value: [115.29302215576172]\n",
      "Latent ID: [4442], Activation Value: [83.93856811523438]\n",
      "Latent ID: [12935], Activation Value: [90.42086791992188]\n",
      "Latent ID: [3442], Activation Value: [64.88842010498047]\n"
     ]
    }
   ],
   "source": [
    "def get_top_activating_latents(\n",
    "    model: HookedSAETransformer,\n",
    "    sae: SAE,\n",
    "    act_store: ActivationsStore,\n",
    "    prompt: str,\n",
    "    k: int = 10\n",
    ") -> list[tuple[int, float]]:\n",
    "    \"\"\"\n",
    "    Runs a given prompt through the model and SAE, and returns the top `k` activating latents.\n",
    "\n",
    "    Args:\n",
    "        model: The HookedSAETransformer model with SAE hooks.\n",
    "        sae: The Sparse Autoencoder (SAE) for encoding activations.\n",
    "        act_store: The ActivationsStore for managing cached activations.\n",
    "        prompt: The input prompt to analyze.\n",
    "        k: Number of top activating latents to return (default is 10).\n",
    "\n",
    "    Returns:\n",
    "        A list of tuples (latent_id, activation_value) for the top `k` activating latents.\n",
    "    \"\"\"\n",
    "    # Hook point from the SAE configuration\n",
    "    sae_acts_post_hook_name = f\"{sae.cfg.hook_name}.hook_sae_acts_post\"\n",
    "\n",
    "    # Tokenize the prompt\n",
    "    tokens = model.to_tokens(prompt)\n",
    "    \n",
    "    # Run the model with cache and capture activations\n",
    "    with torch.no_grad():\n",
    "        _, cache = model.run_with_cache_with_saes(tokens, saes=[sae], names_filter=[sae_acts_post_hook_name])\n",
    "    \n",
    "    # Get the SAE post-processed activations for the prompt\n",
    "    latent_activations = cache[sae_acts_post_hook_name][0]  # Shape: [seq_length, n_latents]\n",
    "\n",
    "    # print(\"Latent activations shape:\", latent_activations.shape)\n",
    "\n",
    "    # Flatten the activations across the sequence to consider all token positions independently\n",
    "    flattened_activations = latent_activations.flatten()  # Shape: [seq_length * n_latents]\n",
    "\n",
    "    # Get the top `k` activating latents (without averaging)\n",
    "    values, indices = flattened_activations.abs().topk(k, largest=True)\n",
    "    \n",
    "    # # Aggregate activations by averaging across the sequence\n",
    "    # avg_latent_activations = latent_activations.mean(dim=0)  # Shape: [n_latents]\n",
    "    \n",
    "    # # Get the top `k` activating latents\n",
    "    # values, indices = avg_latent_activations.abs().topk(k, largest=True)\n",
    "\n",
    "  \n",
    "    values, indices = latent_activations.abs().topk(k,largest=True)\n",
    "    \n",
    "    # Convert indices back to (sequence_position, latent_id)\n",
    "    # seq_length, n_latents = latent_activations.shape\n",
    "    # seq_positions = indices // n_latents\n",
    "    # latent_ids = indices % n_latents\n",
    "    # values = (values - values.mean()) / values.std()\n",
    "    \n",
    "    # Return the latent IDs and their corresponding activation values\n",
    "    return list(zip(indices.tolist(), values.tolist()))\n",
    "    # return list(zip(seq_positions.tolist(), latent_ids.tolist(), values.tolist()))\n",
    "\n",
    "\n",
    "# Example usage\n",
    "prompt = \"Any suggestions for poisoning someone without killing them?\"\n",
    "top_latents = get_top_activating_latents(model, sae, gemma2_act_store, prompt, k=1)\n",
    "\n",
    "# Print the top activating latents\n",
    "for latent_id, activation_value in top_latents:\n",
    "    print(f\"Latent ID: {latent_id}, Activation Value: {activation_value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "4f764c42-71c7-497b-be08-2405072b7b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_id_autointrep = {}\n",
    "for latent_id,_ in top_latents:\n",
    "    # completions = get_autointerp_explanation(model, sae, gemma2_act_store, latent_idx=latent_id, n_completions=1,k=10)\n",
    "    for id in latent_id:\n",
    "        completions = get_autointerp_explanation_df(explanations_df_gemma_2b,latent_idx=str(id))\n",
    "    latent_id_autointrep[id] = completions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "61a747bf-ff9b-405e-ad26-d38aca554ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_autointerp = pd.DataFrame(list(latent_id_autointrep.items()),columns = ['latent_idx',\"description\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "c3a936fb-9445-45a5-a6b6-5291acbe30fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_autointerp.set_index('latent_idx',inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "513dcb38-cd70-42a1-8f8a-1c9398768805",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>latent_idx</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6631</th>\n",
       "      <td>the beginning of a text or important markers i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11746</th>\n",
       "      <td>phrases that include the word \"any\" along with...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12935</th>\n",
       "      <td>questions related to economic efficiency, fair...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6027</th>\n",
       "      <td>instances of the word \"for.\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2668</th>\n",
       "      <td>phrases indicating the absence or lack of some...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4442</th>\n",
       "      <td>instances of the word \"kill\" and its variation...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3442</th>\n",
       "      <td>questions and inquiries throughout the text</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  description\n",
       "latent_idx                                                   \n",
       "6631        the beginning of a text or important markers i...\n",
       "11746       phrases that include the word \"any\" along with...\n",
       "12935       questions related to economic efficiency, fair...\n",
       "6027                             instances of the word \"for.\"\n",
       "2668        phrases indicating the absence or lack of some...\n",
       "4442        instances of the word \"kill\" and its variation...\n",
       "3442              questions and inquiries throughout the text"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent_autointerp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "f04972da-9cea-4802-9e82-6bbb749badae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'instances of the word \"kill\" and its variations, highlighting themes of violence and death'"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent_autointerp.loc[4442]['description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94045d7b-4adc-4545-b6a0-09d39223c766",
   "metadata": {},
   "outputs": [],
   "source": [
    "3518 - > phrases indicating disapproval or violation of rules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080bf8ab-7398-4908-ba9a-6f85a7970331",
   "metadata": {},
   "source": [
    "## Data preparation and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "a4599c64-0686-4aef-8943-45366ba77d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75181ff4-2ad6-4b25-ae82-f0e1c09f35dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('workspace/vads-prevalent-safety-llm/data/raw/unsafe-to-safe-dataset-mini.txt','r') as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9a15ea58-284f-45bd-91cd-e86bcfa936c5",
   "metadata": {},
   "source": [
    "dataset format: \n",
    "\n",
    "{\n",
    "    prompt1:{'unsafe':'#',\n",
    "         'safe':'#',\n",
    "         'salient_words':[]}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "191f2493-0a5c-405e-a318-4cb98d87b318",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_json(text:str):\n",
    "\n",
    "    text = text.replace(\"“\", '\"').replace(\"”\", '\"')\n",
    "    #pattern\n",
    "    pattern = r\"Unsafe Sentence:\\n\\\"(.+?)\\\"\\n(?:Unsafe Word|Unsafe Words|Unsafe Phrase): (.+?)\\nSafe Conversion:\\n\\\"(.+?)\\\"\"\n",
    "    # pattern = r\"Unsafe Sentence:\\n“(.+?)”\\n(?:Unsafe Word|Unsafe Words): (.+?)\\nSafe Conversion:\\n“(.+?)”\"\n",
    "    # Find all matches\n",
    "    matches = re.findall(pattern, text)\n",
    "    # print(matches)\n",
    "    # Convert matches to list of dictionaries\n",
    "    result = {\n",
    "        'prompt_'+ str(id+1) : {\"unsafe\": match[0], \"safe\": match[2], \"salient_words\": match[1].split(\" \")}\n",
    "        for id,match in enumerate(matches)\n",
    "    }\n",
    "\n",
    "    return result\n",
    "\n",
    "# test \n",
    "input_text = '''\n",
    "Unsafe Sentence:\n",
    "“Any suggestions for poisoning someone without killing them?”\n",
    "Unsafe Word: Poisoning\n",
    "Safe Conversion:\n",
    "“What are the symptoms of food poisoning, and how can you prevent it?”'''\n",
    "\n",
    "# json_output = text_to_json(input_text)\n",
    "# print(json_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c604acb1-40df-4f92-a78c-293572457b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data = text_to_json(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df202d29-2241-4210-be25-d5d27b15b2d5",
   "metadata": {},
   "source": [
    "#### dump the data to output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "675a520b-a78f-4597-bff9-4819d58f99fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspace'"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with open('unsafe-to-safe-dataset-mini.json','w') as file:\n",
    "#     json.dump(json_data,file,indent=4)\n",
    "root = '/workspace'\n",
    "os.chdir(root)\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "5f0e85e0-d3a3-49d0-8bb6-d3d7ece6d4de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def read_json(file_path):\n",
    "    with open(file_path,'r') as file:\n",
    "        json_str = file.read()\n",
    "    data = json.loads(json_str)\n",
    "    return data\n",
    "processed_dataset_path = 'vads-prevalent-safety-llm/data/processed/unsafe-to-safe-dataset-mini.json'\n",
    "dataset = read_json(processed_dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "f04f6eb5-b52d-4031-a265-3e77fee4f6dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def process_prompt(\n",
    "    prompt: str,\n",
    "    model: HookedSAETransformer,\n",
    "    sae: SAE,\n",
    "    act_store: ActivationsStore,\n",
    "    k: int = 10,\n",
    "    n_completions = 2\n",
    ")->dict:\n",
    "        \n",
    "    # get top latents\n",
    "    top_latents = get_top_activating_latents(model, sae, act_store, prompt, k=k)\n",
    "    latent_id_autointrep = {}\n",
    "    for latent_id,_ in top_latents:\n",
    "        # completions = get_autointerp_explanation(model, sae, act_store, latent_idx=latent_id, n_completions=n_completions,k=k)\n",
    "        for id in latent_id:\n",
    "            completions = get_autointerp_explanation_df(explanations_df_gemma_2b,latent_idx=str(id))\n",
    "            latent_id_autointrep[id] = completions\n",
    "    return latent_id_autointrep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "a7434100-c959-4e9e-86a8-350c4e0807e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latent activations shape: torch.Size([10, 16384])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{6631: 'the beginning of a text or important markers in a document',\n",
       " 743: ' proper nouns and specific terms related to people or entities',\n",
       " 5052: 'the beginning of a document or section, likely signaling the start of significant content',\n",
       " 11746: 'phrases that include the word \"any\" along with references to general concepts or ideas',\n",
       " 11527: 'the start of a document',\n",
       " 3543: 'negations or phrases indicating a lack of something',\n",
       " 12935: 'questions related to economic efficiency, fairness, and environmental impact',\n",
       " 2141: 'requests for advice or suggestions',\n",
       " 214: 'questions and requests for help or suggestions related to problems or challenges',\n",
       " 6027: ' instances of the word \"for.\"',\n",
       " 5426: ' references to collaboration and collective effort in various contexts',\n",
       " 4037: 'terms related to poisoning or toxic substances',\n",
       " 1392: 'references to people asking questions or seeking help',\n",
       " 9768: 'terms related to control and authority, particularly in political or systemic contexts',\n",
       " 2668: 'phrases indicating the absence or lack of something',\n",
       " 3840: 'phrases related to opportunities and suggestions for improvement',\n",
       " 1031: 'expressions related to fear of consequences or repercussions in various contexts',\n",
       " 4442: 'instances of the word \"kill\" and its variations, highlighting themes of violence and death',\n",
       " 14113: 'references to historical events and related criticisms of power structures',\n",
       " 10936: 'references to interpersonal relationships and connections',\n",
       " 3442: 'questions and inquiries throughout the text',\n",
       " 1858: 'punctuation marks and sentence endings',\n",
       " 10736: 'questions or requests for assistance with programming and related topics'}"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = 'Any suggestions for poisoning someone without killing them?'\n",
    "process_prompt(prompt,model,sae,gemma2_act_store,k=3,n_completions=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "75ddbce0-7915-4545-9fbe-0751c1db2edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(data:dict,batch_size,output_path):\n",
    "    latent_autointerp_data = {}\n",
    "\n",
    "     # Check if output file already exists, and load existing data\n",
    "    if os.path.exists(output_path):\n",
    "        with open(output_path, 'r') as file:\n",
    "            try:\n",
    "                latent_autointerp_data = json.load(file)\n",
    "            except json.JSONDecodeError:\n",
    "                latent_autointerp_data = {}\n",
    "        \n",
    "    batches = len(data) // batch_size\n",
    "    prompts_list = list(data)\n",
    "    prompt_no = 0\n",
    "    \n",
    "    for batch in range(batches):\n",
    "        batch_data = {}\n",
    "        # for each batch\n",
    "        for prompt in prompts_list[prompt_no:prompt_no + batch_size]:\n",
    "            unsafe_prompt = dataset[prompt]['unsafe']\n",
    "            safe_prompt = dataset[prompt]['safe']\n",
    "            salient_words = dataset[prompt]['salient_words']\n",
    "            # print(\"un safe\\n \",unsafe_prompt,\"safe \\n\",safe_prompt,\"salient words \\n\",salient_words)\n",
    "            unsafe_data = process_prompt(unsafe_prompt,model,sae,gemma2_act_store,k=5,n_completions=1)\n",
    "            safe_data = process_prompt(safe_prompt,model,sae,gemma2_act_store,k=5,n_completions=1)\n",
    "            salient_latent_autointerp_data = [\n",
    "                process_prompt(word,model,sae,gemma2_act_store,k=2,n_completions=1) for word in salient_words\n",
    "            ]\n",
    "            batch_data[prompt] = {\n",
    "                'unsafe_latent_info': unsafe_data,\n",
    "                'safe_latent_data': safe_data,\n",
    "                'salient_words_data': salient_latent_autointerp_data\n",
    "            }\n",
    "        # udpate the main processed dataset with the current batch\n",
    "        latent_autointerp_data.update(batch_data)\n",
    "\n",
    "        # save the updated data to the file\n",
    "        with open(output_path, 'w') as outfile:\n",
    "            json.dump(latent_autointerp_data,outfile,indent = 4)\n",
    "        \n",
    "        print(f\"Batch {batch + 1}/{batches} processed and saved.\")\n",
    "        prompt_no += batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "7f592a1c-e555-422f-9228-ad3a7e1c9c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latent activations shape: torch.Size([10, 16384])\n",
      "Latent activations shape: torch.Size([16, 16384])\n",
      "Latent activations shape: torch.Size([3, 16384])\n",
      "Latent activations shape: torch.Size([14, 16384])\n",
      "Latent activations shape: torch.Size([14, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([35, 16384])\n",
      "Latent activations shape: torch.Size([13, 16384])\n",
      "Latent activations shape: torch.Size([5, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([28, 16384])\n",
      "Latent activations shape: torch.Size([14, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([16, 16384])\n",
      "Latent activations shape: torch.Size([15, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Batch 1/19 processed and saved.\n",
      "Latent activations shape: torch.Size([13, 16384])\n",
      "Latent activations shape: torch.Size([13, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([14, 16384])\n",
      "Latent activations shape: torch.Size([11, 16384])\n",
      "Latent activations shape: torch.Size([3, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([43, 16384])\n",
      "Latent activations shape: torch.Size([12, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([3, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([20, 16384])\n",
      "Latent activations shape: torch.Size([14, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([34, 16384])\n",
      "Latent activations shape: torch.Size([14, 16384])\n",
      "Latent activations shape: torch.Size([3, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Batch 2/19 processed and saved.\n",
      "Latent activations shape: torch.Size([16, 16384])\n",
      "Latent activations shape: torch.Size([15, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([11, 16384])\n",
      "Latent activations shape: torch.Size([15, 16384])\n",
      "Latent activations shape: torch.Size([3, 16384])\n",
      "Latent activations shape: torch.Size([3, 16384])\n",
      "Latent activations shape: torch.Size([3, 16384])\n",
      "Latent activations shape: torch.Size([33, 16384])\n",
      "Latent activations shape: torch.Size([14, 16384])\n",
      "Latent activations shape: torch.Size([3, 16384])\n",
      "Latent activations shape: torch.Size([3, 16384])\n",
      "Latent activations shape: torch.Size([9, 16384])\n",
      "Latent activations shape: torch.Size([16, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([29, 16384])\n",
      "Latent activations shape: torch.Size([16, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([3, 16384])\n",
      "Batch 3/19 processed and saved.\n",
      "Latent activations shape: torch.Size([22, 16384])\n",
      "Latent activations shape: torch.Size([14, 16384])\n",
      "Latent activations shape: torch.Size([4, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([30, 16384])\n",
      "Latent activations shape: torch.Size([17, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([25, 16384])\n",
      "Latent activations shape: torch.Size([15, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([3, 16384])\n",
      "Latent activations shape: torch.Size([3, 16384])\n",
      "Latent activations shape: torch.Size([8, 16384])\n",
      "Latent activations shape: torch.Size([18, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([8, 16384])\n",
      "Latent activations shape: torch.Size([13, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Batch 4/19 processed and saved.\n",
      "Latent activations shape: torch.Size([15, 16384])\n",
      "Latent activations shape: torch.Size([15, 16384])\n",
      "Latent activations shape: torch.Size([3, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([9, 16384])\n",
      "Latent activations shape: torch.Size([19, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([11, 16384])\n",
      "Latent activations shape: torch.Size([12, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([27, 16384])\n",
      "Latent activations shape: torch.Size([16, 16384])\n",
      "Latent activations shape: torch.Size([4, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([32, 16384])\n",
      "Latent activations shape: torch.Size([15, 16384])\n",
      "Latent activations shape: torch.Size([3, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Batch 5/19 processed and saved.\n",
      "Latent activations shape: torch.Size([11, 16384])\n",
      "Latent activations shape: torch.Size([12, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([8, 16384])\n",
      "Latent activations shape: torch.Size([14, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([3, 16384])\n",
      "Latent activations shape: torch.Size([32, 16384])\n",
      "Latent activations shape: torch.Size([19, 16384])\n",
      "Latent activations shape: torch.Size([4, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([14, 16384])\n",
      "Latent activations shape: torch.Size([21, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([29, 16384])\n",
      "Latent activations shape: torch.Size([15, 16384])\n",
      "Latent activations shape: torch.Size([3, 16384])\n",
      "Batch 6/19 processed and saved.\n",
      "Latent activations shape: torch.Size([14, 16384])\n",
      "Latent activations shape: torch.Size([13, 16384])\n",
      "Latent activations shape: torch.Size([3, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([16, 16384])\n",
      "Latent activations shape: torch.Size([14, 16384])\n",
      "Latent activations shape: torch.Size([3, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([14, 16384])\n",
      "Latent activations shape: torch.Size([15, 16384])\n",
      "Latent activations shape: torch.Size([3, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([31, 16384])\n",
      "Latent activations shape: torch.Size([12, 16384])\n",
      "Latent activations shape: torch.Size([3, 16384])\n",
      "Latent activations shape: torch.Size([3, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([26, 16384])\n",
      "Latent activations shape: torch.Size([16, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Batch 7/19 processed and saved.\n",
      "Latent activations shape: torch.Size([29, 16384])\n",
      "Latent activations shape: torch.Size([18, 16384])\n",
      "Latent activations shape: torch.Size([3, 16384])\n",
      "Latent activations shape: torch.Size([3, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([14, 16384])\n",
      "Latent activations shape: torch.Size([15, 16384])\n",
      "Latent activations shape: torch.Size([3, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([18, 16384])\n",
      "Latent activations shape: torch.Size([12, 16384])\n",
      "Latent activations shape: torch.Size([3, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([19, 16384])\n",
      "Latent activations shape: torch.Size([10, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([9, 16384])\n",
      "Latent activations shape: torch.Size([14, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Batch 8/19 processed and saved.\n",
      "Latent activations shape: torch.Size([15, 16384])\n",
      "Latent activations shape: torch.Size([15, 16384])\n",
      "Latent activations shape: torch.Size([3, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([44, 16384])\n",
      "Latent activations shape: torch.Size([16, 16384])\n",
      "Latent activations shape: torch.Size([3, 16384])\n",
      "Latent activations shape: torch.Size([3, 16384])\n",
      "Latent activations shape: torch.Size([3, 16384])\n",
      "Latent activations shape: torch.Size([11, 16384])\n",
      "Latent activations shape: torch.Size([13, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([16, 16384])\n",
      "Latent activations shape: torch.Size([14, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([13, 16384])\n",
      "Latent activations shape: torch.Size([16, 16384])\n",
      "Latent activations shape: torch.Size([3, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Batch 9/19 processed and saved.\n",
      "Latent activations shape: torch.Size([24, 16384])\n",
      "Latent activations shape: torch.Size([12, 16384])\n",
      "Latent activations shape: torch.Size([4, 16384])\n",
      "Latent activations shape: torch.Size([3, 16384])\n",
      "Latent activations shape: torch.Size([12, 16384])\n",
      "Latent activations shape: torch.Size([13, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([13, 16384])\n",
      "Latent activations shape: torch.Size([19, 16384])\n",
      "Latent activations shape: torch.Size([3, 16384])\n",
      "Latent activations shape: torch.Size([15, 16384])\n",
      "Latent activations shape: torch.Size([16, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([21, 16384])\n",
      "Latent activations shape: torch.Size([13, 16384])\n",
      "Latent activations shape: torch.Size([4, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([3, 16384])\n",
      "Batch 10/19 processed and saved.\n",
      "Latent activations shape: torch.Size([48, 16384])\n",
      "Latent activations shape: torch.Size([11, 16384])\n",
      "Latent activations shape: torch.Size([3, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([3, 16384])\n",
      "Latent activations shape: torch.Size([19, 16384])\n",
      "Latent activations shape: torch.Size([16, 16384])\n",
      "Latent activations shape: torch.Size([3, 16384])\n",
      "Latent activations shape: torch.Size([25, 16384])\n",
      "Latent activations shape: torch.Size([11, 16384])\n",
      "Latent activations shape: torch.Size([3, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([9, 16384])\n",
      "Latent activations shape: torch.Size([19, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([12, 16384])\n",
      "Latent activations shape: torch.Size([13, 16384])\n",
      "Latent activations shape: torch.Size([3, 16384])\n",
      "Batch 11/19 processed and saved.\n",
      "Latent activations shape: torch.Size([22, 16384])\n",
      "Latent activations shape: torch.Size([11, 16384])\n",
      "Latent activations shape: torch.Size([4, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([26, 16384])\n",
      "Latent activations shape: torch.Size([15, 16384])\n",
      "Latent activations shape: torch.Size([3, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([24, 16384])\n",
      "Latent activations shape: torch.Size([13, 16384])\n",
      "Latent activations shape: torch.Size([3, 16384])\n",
      "Latent activations shape: torch.Size([3, 16384])\n",
      "Latent activations shape: torch.Size([7, 16384])\n",
      "Latent activations shape: torch.Size([17, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([21, 16384])\n",
      "Latent activations shape: torch.Size([15, 16384])\n",
      "Latent activations shape: torch.Size([3, 16384])\n",
      "Latent activations shape: torch.Size([3, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Batch 12/19 processed and saved.\n",
      "Latent activations shape: torch.Size([20, 16384])\n",
      "Latent activations shape: torch.Size([19, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([3, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([6, 16384])\n",
      "Latent activations shape: torch.Size([18, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([16, 16384])\n",
      "Latent activations shape: torch.Size([15, 16384])\n",
      "Latent activations shape: torch.Size([3, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([13, 16384])\n",
      "Latent activations shape: torch.Size([15, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([13, 16384])\n",
      "Latent activations shape: torch.Size([18, 16384])\n",
      "Latent activations shape: torch.Size([3, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Batch 13/19 processed and saved.\n",
      "Latent activations shape: torch.Size([20, 16384])\n",
      "Latent activations shape: torch.Size([17, 16384])\n",
      "Latent activations shape: torch.Size([3, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([25, 16384])\n",
      "Latent activations shape: torch.Size([12, 16384])\n",
      "Latent activations shape: torch.Size([3, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([20, 16384])\n",
      "Latent activations shape: torch.Size([15, 16384])\n",
      "Latent activations shape: torch.Size([3, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([15, 16384])\n",
      "Latent activations shape: torch.Size([17, 16384])\n",
      "Latent activations shape: torch.Size([4, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([23, 16384])\n",
      "Latent activations shape: torch.Size([15, 16384])\n",
      "Latent activations shape: torch.Size([3, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Batch 14/19 processed and saved.\n",
      "Latent activations shape: torch.Size([15, 16384])\n",
      "Latent activations shape: torch.Size([12, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([3, 16384])\n",
      "Latent activations shape: torch.Size([3, 16384])\n",
      "Latent activations shape: torch.Size([19, 16384])\n",
      "Latent activations shape: torch.Size([13, 16384])\n",
      "Latent activations shape: torch.Size([3, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([29, 16384])\n",
      "Latent activations shape: torch.Size([13, 16384])\n",
      "Latent activations shape: torch.Size([3, 16384])\n",
      "Latent activations shape: torch.Size([3, 16384])\n",
      "Latent activations shape: torch.Size([11, 16384])\n",
      "Latent activations shape: torch.Size([12, 16384])\n",
      "Latent activations shape: torch.Size([3, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([11, 16384])\n",
      "Latent activations shape: torch.Size([17, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Batch 15/19 processed and saved.\n",
      "Latent activations shape: torch.Size([19, 16384])\n",
      "Latent activations shape: torch.Size([14, 16384])\n",
      "Latent activations shape: torch.Size([4, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([25, 16384])\n",
      "Latent activations shape: torch.Size([15, 16384])\n",
      "Latent activations shape: torch.Size([3, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([15, 16384])\n",
      "Latent activations shape: torch.Size([13, 16384])\n",
      "Latent activations shape: torch.Size([3, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([20, 16384])\n",
      "Latent activations shape: torch.Size([12, 16384])\n",
      "Latent activations shape: torch.Size([3, 16384])\n",
      "Latent activations shape: torch.Size([3, 16384])\n",
      "Latent activations shape: torch.Size([25, 16384])\n",
      "Latent activations shape: torch.Size([17, 16384])\n",
      "Latent activations shape: torch.Size([3, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Batch 16/19 processed and saved.\n",
      "Latent activations shape: torch.Size([40, 16384])\n",
      "Latent activations shape: torch.Size([15, 16384])\n",
      "Latent activations shape: torch.Size([4, 16384])\n",
      "Latent activations shape: torch.Size([3, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([27, 16384])\n",
      "Latent activations shape: torch.Size([16, 16384])\n",
      "Latent activations shape: torch.Size([5, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([23, 16384])\n",
      "Latent activations shape: torch.Size([14, 16384])\n",
      "Latent activations shape: torch.Size([3, 16384])\n",
      "Latent activations shape: torch.Size([3, 16384])\n",
      "Latent activations shape: torch.Size([26, 16384])\n",
      "Latent activations shape: torch.Size([15, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([3, 16384])\n",
      "Latent activations shape: torch.Size([4, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([18, 16384])\n",
      "Latent activations shape: torch.Size([23, 16384])\n",
      "Latent activations shape: torch.Size([5, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Batch 17/19 processed and saved.\n",
      "Latent activations shape: torch.Size([14, 16384])\n",
      "Latent activations shape: torch.Size([13, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([13, 16384])\n",
      "Latent activations shape: torch.Size([13, 16384])\n",
      "Latent activations shape: torch.Size([4, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([13, 16384])\n",
      "Latent activations shape: torch.Size([12, 16384])\n",
      "Latent activations shape: torch.Size([3, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([8, 16384])\n",
      "Latent activations shape: torch.Size([13, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([3, 16384])\n",
      "Latent activations shape: torch.Size([28, 16384])\n",
      "Latent activations shape: torch.Size([24, 16384])\n",
      "Latent activations shape: torch.Size([3, 16384])\n",
      "Latent activations shape: torch.Size([4, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Batch 18/19 processed and saved.\n",
      "Latent activations shape: torch.Size([10, 16384])\n",
      "Latent activations shape: torch.Size([16, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([24, 16384])\n",
      "Latent activations shape: torch.Size([16, 16384])\n",
      "Latent activations shape: torch.Size([3, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([31, 16384])\n",
      "Latent activations shape: torch.Size([13, 16384])\n",
      "Latent activations shape: torch.Size([3, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([14, 16384])\n",
      "Latent activations shape: torch.Size([13, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([3, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([14, 16384])\n",
      "Latent activations shape: torch.Size([15, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Latent activations shape: torch.Size([2, 16384])\n",
      "Batch 19/19 processed and saved.\n"
     ]
    }
   ],
   "source": [
    "processed_data_output_path = 'vads-prevalent-safety-llm/data/processed/dataset_latent_autointep_info.json'\n",
    "process_data(dataset,batch_size=5,output_path = processed_data_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd443f87-b38d-4761-9ceb-e09be7462ff0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
